{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "incremental.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Z02x1UlmNfWK",
        "yLbmVHm5Nwsk",
        "_vwt2ztUXFaH",
        "FhabEzELXMFR"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tsadoq/iCarl/blob/master/incremental.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6vORZRxNJYy",
        "colab_type": "text"
      },
      "source": [
        "# **Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbHg8CdIoYRN",
        "colab_type": "text"
      },
      "source": [
        "**CHECK FOR DECENT GPU** (not K80)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CO92U6fAmzk9",
        "colab_type": "code",
        "outputId": "3243055b-f116-4b7f-e52e-1b1c55d0a3c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Jun  8 22:31:58 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjmzHT-Q3RR4",
        "colab_type": "text"
      },
      "source": [
        "**Install python packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uneVv4y2YcwF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip3 install 'torch==1.4.0'\n",
        "# !pip3 install 'torchvision==0.5.0'\n",
        "# !pip3 install 'Pillow-SIMD'\n",
        "# !pip3 install 'tqdm'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPJT2xmJ3Xnm",
        "colab_type": "text"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBUh1xw7Yr6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import logging\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "import dill"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKMon4lO3aYE",
        "colab_type": "text"
      },
      "source": [
        "**Download dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XABs0cQ7ak2r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if not os.path.isdir('./cifar-100-python'):\n",
        "#   !wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "#   !tar xfz cifar-100-python.tar.gz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Gob5Y8_9SoR",
        "colab_type": "text"
      },
      "source": [
        "**MACROS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXGAM0a43mCg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEVICE = 'cuda'\n",
        "\n",
        "NUM_EPOCHS = 70\n",
        "BATCH_SIZE = 128\n",
        "LR = 2.0\n",
        "GAMMA = 0.2\n",
        "K = 2000\n",
        "FIRST_STEP = 49\n",
        "SECOND_STEP = 63"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISfQL4G03rIN",
        "colab_type": "text"
      },
      "source": [
        "**Load datasets**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da1N6YJuEz-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define transforms for training phase\n",
        "train_transform = transforms.Compose([\n",
        "                                      transforms.ToPILImage(),\n",
        "                                      transforms.RandomCrop(32, padding=4),\n",
        "                                      transforms.RandomHorizontalFlip(0.5),\n",
        "                                      transforms.ToTensor(), # Turn PIL Image to torch.Tensor\n",
        "                                      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)) # Normalizes tensor with mean and standard deviation\n",
        "])\n",
        "# Define transforms for the evaluation phase\n",
        "eval_transform = transforms.Compose([\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))                                    \n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWcKmQ86ZCES",
        "colab_type": "code",
        "outputId": "6c08a136-0779-4957-943e-92ed3b64aff6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_dataset = torchvision.datasets.CIFAR100('./', train=True, transform=train_transform, target_transform=None, download=True)\n",
        "test_dataset = torchvision.datasets.CIFAR100('./', train=False, transform=eval_transform, target_transform=None, download=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z02x1UlmNfWK",
        "colab_type": "text"
      },
      "source": [
        "# **Split Dataset into batches of 10 classes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFQMAnDpqSZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = train_dataset.classes.copy()\n",
        "random.seed(666)\n",
        "random.shuffle(labels)\n",
        "incremental_mapping = {v: k for k, v in enumerate(labels)}\n",
        "partitions = []\n",
        "for i in range(10):\n",
        "  partitions.append(labels[i*10:(i+1)*10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxSqAzdFeinM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BatchDataset:\n",
        "  def __init__(self, dataset, labels, split='train', transform=None, target_transform=None, mapping = incremental_mapping):\n",
        "    self.data_per_label=[]\n",
        "    self.split = split\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "    self.data = []\n",
        "    self.labels = labels\n",
        "    self.labels_to_int = [incremental_mapping[label] for label in labels]\n",
        "    \n",
        "    for label in labels: \n",
        "      data_per_label=[(dataset.data[x], int(self.labels_to_int[labels.index(label)])) for x in [index for index, element in enumerate(dataset.targets) if element == dataset.class_to_idx[label]]]\n",
        "      self.data+=data_per_label\n",
        "      self.data_per_label.append(data_per_label)\n",
        "  def __getitem__(self, index):\n",
        "    image, label = self.data[index]\n",
        "    if self.transform is not None:\n",
        "      image = self.transform(image)\n",
        "    return index, image, label\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpCwcPv3b-WF",
        "colab_type": "text"
      },
      "source": [
        "**Create splits**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHk5_g6Pj_sL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_datasets = []\n",
        "# val_datasets = []\n",
        "test_datasets = []\n",
        "for i, partition in enumerate(partitions):\n",
        "  batch_test=BatchDataset(test_dataset,partition,transform=eval_transform, mapping = incremental_mapping)\n",
        "  test_datasets.append(batch_test)\n",
        "  batch = BatchDataset(train_dataset, partition,transform=train_transform, mapping = incremental_mapping)\n",
        "  train_datasets.append(batch)\n",
        "  # sss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2)\n",
        "  # for train_indexes, val_indexes in sss.split(range(len(batch)), [x[1] for x in batch.data]):\n",
        "    # batch_val_dataset = Subset(batch, val_indexes)\n",
        "    # batch_train_dataset = Subset(batch, train_indexes)\n",
        "    # train_datasets.append(batch_train_dataset)\n",
        "    # val_datasets.append(batch_val_dataset)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLbmVHm5Nwsk",
        "colab_type": "text"
      },
      "source": [
        "# **ResNet32**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Exbyc4iv_Cpe",
        "colab_type": "text"
      },
      "source": [
        "**ResNets for CIFAR100**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQkjYKsDBaGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "\n",
        "\"\"\"\n",
        "Credits to @hshustc\n",
        "Taken from https://github.com/hshustc/CVPR19_Incremental_Learning/tree/master/cifar100-class-incremental\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \"\"\"3x3 convolution with padding\"\"\"\n",
        "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
        "                     padding=1, bias=False)\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n",
        "                               padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers):\n",
        "        self.inplanes = 16\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(8, stride=1)\n",
        "\n",
        "        self.out_dim = 64 * block.expansion\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        return x\n",
        "\n",
        "def resnet20(pretrained=False, **kwargs):\n",
        "    n = 3\n",
        "    model = ResNet(BasicBlock, [n, n, n], **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnet32(pretrained=False, **kwargs):\n",
        "    n = 5\n",
        "    model = ResNet(BasicBlock, [n, n, n], **kwargs)\n",
        "    return model\n",
        "\n",
        "def resnet56(pretrained=False, **kwargs):\n",
        "    n = 9\n",
        "    model = ResNet(Bottleneck, [n, n, n], **kwargs)\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZxOzjlMXB8v",
        "colab_type": "text"
      },
      "source": [
        "# **iCaRL model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9686ugaH5XYt",
        "colab_type": "text"
      },
      "source": [
        "ICARL NET "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyh9W7MM5RUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class iCarlNet(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    self.total_num_classes = n_classes\n",
        "    self.known_classes = 0\n",
        "    self.list_known_classes=[]\n",
        "    self.exemplar_sets = []\n",
        "    self.flag_mean = True\n",
        "    self.exemplar_means = []\n",
        "    self.exemplars_list_knn=[]\n",
        "    self.labels_knn=[]\n",
        "    # We take a standard ResNet and Extend it\n",
        "    super(iCarlNet, self).__init__()\n",
        "    self.extractor = resnet32()\n",
        "    self.fully_connected = nn.Linear(self.extractor.out_dim, 0, bias=True)\n",
        "    torch.nn.init.xavier_uniform_(self.fully_connected.weight)\n",
        "\n",
        "    self.fully_connected.bias.data.fill_(0.01)\n",
        "    self.loss=nn.BCEWithLogitsLoss()\n",
        "        \n",
        "  def forward(self, x):\n",
        "      # X: input data\n",
        "\n",
        "      self.extractor.to(DEVICE)\n",
        "      self.fully_connected.to(DEVICE)\n",
        "\n",
        "      x = self.extractor(x)\n",
        "      x = self.fully_connected(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "  # def classify(self, input_image_batch, transform):\n",
        "  #   # input_image_batch: batch of up to 10 shuffled classes that we use for training and validation\n",
        "  #   # transform: transformation to be applied to raw exemplar images\n",
        "  #   batch_size = input_image_batch.size(0)\n",
        "  #   self.eval()\n",
        "\n",
        "  #   # compute exemplar means when calling right after update_representation\n",
        "  #   with torch.no_grad():\n",
        "  #     if self.flag_mean:\n",
        "  #       print(f\"num exemplar sets: {len(self.exemplar_sets)}\")\n",
        "  #       exemplar_means = []\n",
        "  #       for exemplars in self.exemplar_sets:\n",
        "  #         features = []\n",
        "  #         for exemplar in exemplars:\n",
        "  #           feature = self.extractor(transform(exemplar).unsqueeze(0).cuda())\n",
        "  #           feature.data = feature.data / feature.data.norm()\n",
        "  #           features.append(feature)\n",
        "  #         features = torch.stack(features)\n",
        "  #         exemplar_mean = torch.mean(features, 0)\n",
        "  #         exemplar_means.append(exemplar_mean)\n",
        "  #       print(f\"Exemplar means len: {len(exemplar_means)}\")\n",
        "  #       self.exemplar_means = exemplar_means\n",
        "  #       self.flag_mean = False\n",
        "\n",
        "  #     # mean: tensor of dimensions known_classes x features (64)\n",
        "  #     mean = torch.stack(self.exemplar_means).squeeze(1)    \n",
        "  #     # extract the features of the whole batch\n",
        "  #     features = self.extractor(input_image_batch)          \n",
        "  #     predictions = []\n",
        "  #     # normalize and predict by nearest exemplar mean\n",
        "  #     for n, feature in enumerate(features):\n",
        "  #       feature.data = feature.data / feature.data.norm() \n",
        "  #       distances = torch.sum((feature - mean)**2, 1)\n",
        "  #       predictions.append(torch.argmin(distances, 0))\n",
        "  #     return predictions\n",
        "\n",
        "  # def classify_without_mean(self,input_image_batch):\n",
        "  #   # input image batch: batch of 128 images to compute predictions on\n",
        "\n",
        "  #   self.eval()\n",
        "  #   with torch.no_grad():\n",
        "  #     # mean: tensor of dimension: known_classes x features (64)\n",
        "  #     mean = torch.stack(self.exemplar_means).squeeze(1).cuda()\n",
        "  #     # extract the features of the whole batch \n",
        "  #     self.extractor.to(DEVICE)   \n",
        "  #     features = self.extractor(input_image_batch)        \n",
        "  #     predictions = []\n",
        "\n",
        "  #     # normalize and predict by nearest exemplar mean\n",
        "  #     for n, feature in enumerate(features):\n",
        "  #       feature.data = feature.data / feature.data.norm()   \n",
        "  #       distances = torch.sum((feature - mean)**2, 1)\n",
        "  #       predictions.append(torch.argmin(distances, 0))\n",
        "  #     return predictions\n",
        "  \n",
        "  def classify_without_mean_cosine_similarity(self,input_image_batch):\n",
        "    # input image batch: batch of 128 images to compute predictions on\n",
        "\n",
        "    self.eval()\n",
        "    with torch.no_grad():\n",
        "      # mean: tensor of dimension: known_classes x features (64)\n",
        "      mean = torch.stack(self.exemplar_means).squeeze(1).cuda()\n",
        "      # extract the features of the whole batch \n",
        "      self.extractor.to(DEVICE)   \n",
        "      features = self.extractor(input_image_batch)        \n",
        "      predictions = []\n",
        "\n",
        "      # normalize and predict by nearest exemplar mean\n",
        "      for n, feature in enumerate(features):\n",
        "        feature.data = feature.data / feature.data.norm() \n",
        "        feature=feature.expand_as(mean)\n",
        "        cosine_similarity = nn.CosineSimilarity(dim=1, eps=1e-08)\n",
        "        distances=cosine_similarity(feature,mean)\n",
        "        predictions.append(torch.argmax(distances, 0))\n",
        "      return predictions\n",
        "\n",
        "  def compute_exemplar_list_knn(self,transform):\n",
        "    self.eval()\n",
        "    with torch.no_grad():\n",
        "      exemplar_features = []\n",
        "      labels=[]\n",
        "      for i,exemplars in enumerate(self.exemplar_sets):\n",
        "        for exemplar in exemplars:\n",
        "          feature = self.extractor(transform(exemplar).unsqueeze(0).cuda())\n",
        "          feature.data = feature.data / feature.data.norm()\n",
        "          exemplar_features.append(feature)\n",
        "          labels.append(i)\n",
        "      \n",
        "      self.exemplars_list_knn=exemplar_features\n",
        "      self.labels_knn=labels\n",
        "\n",
        "  def KNN_classify(self,input_image_batch,k):\n",
        "    exemplar_features=torch.stack(self.exemplars_list_knn).squeeze(1)\n",
        "    self.eval()\n",
        "    with torch.no_grad():\n",
        "      # mean: tensor of dimension: known_classes x features (64)\n",
        "      # mean = torch.stack(self.exemplar_means).squeeze(1).cuda()\n",
        "      # extract the features of the whole batch \n",
        "      self.extractor.to(DEVICE)   \n",
        "      features = self.extractor(input_image_batch)        \n",
        "      predictions = []\n",
        "\n",
        "      # normalize and predict by nearest exemplar mean\n",
        "      for n, feature in enumerate(features):\n",
        "        labels_knn=[]\n",
        "        feature.data = feature.data / feature.data.norm() \n",
        "        feature=feature.expand_as(exemplar_features)\n",
        "        l2_distances=torch.sum((feature - exemplar_features)**2, 1)\n",
        "        knn=l2_distances.topk(k, largest=False)\n",
        "        for index in knn.indices.tolist():\n",
        "          labels_knn.append(self.labels_knn[index])\n",
        "        predictions.append(torch.tensor(max(set(labels_knn),key=labels_knn.count)))\n",
        "      return predictions\n",
        "\n",
        "  # def compute_means(self,transform):\n",
        "  #   # transform: transformation to be applied to raw exemplar images\n",
        "  #   self.eval()\n",
        "  #   with torch.no_grad():\n",
        "  #     print(f\"num exemplar sets: {len(self.exemplar_sets)}\")\n",
        "  #     exemplar_means = []\n",
        "  #     for exemplars in self.exemplar_sets:\n",
        "  #       features = []\n",
        "  #       for exemplar in exemplars:\n",
        "  #         feature = self.extractor(transform(exemplar).unsqueeze(0).cuda())\n",
        "  #         feature.data = feature.data / feature.data.norm()\n",
        "  #         features.append(feature)\n",
        "  #       features = torch.stack(features)\n",
        "  #       exemplar_mean = torch.mean(features, 0)\n",
        "  #       exemplar_means.append(exemplar_mean)\n",
        "  #     print(f\"Exemplar means len: {len(exemplar_means)}\")\n",
        "  #     self.exemplar_means = exemplar_means\n",
        "  #     self.flag_mean = False\n",
        "\n",
        "  # def construct_exemplar_set_with_class_mean(self, images, exemplars_per_class, transform):\n",
        "  #   # images: batch of a single class, used to construct exemplar sets\n",
        "  #   # exemplars per class: m = K/known_classes\n",
        "  #   # transform: transformation to be applied to raw exemplar images\n",
        "\n",
        "  #   self.eval()\n",
        "  #   with torch.no_grad():\n",
        "  #     # to be computed only one time, after the update representation\n",
        "  #     if self.flag_mean:\n",
        "  #       print(f\"num exemplar sets: {len(self.exemplar_sets)}\")\n",
        "  #       exemplar_means = []\n",
        "  #       for exemplars in self.exemplar_sets:\n",
        "  #         features = []\n",
        "  #         for exemplar in exemplars:\n",
        "  #           # extract features and normalize\n",
        "  #           feature = self.extractor(transform(exemplar).unsqueeze(0).cuda())\n",
        "  #           feature.data = feature.data / feature.data.norm()\n",
        "  #           features.append(feature)\n",
        "  #         # features is a tensor m x 64, where m is the number of exemplars in a set\n",
        "  #         features = torch.stack(features)\n",
        "  #         exemplar_mean = torch.mean(features, 0)\n",
        "  #         exemplar_means.append(exemplar_mean)\n",
        "  #       print(f\"Exemplar means len: {len(exemplar_means)}\")\n",
        "  #       self.exemplar_means = exemplar_means\n",
        "  #       self.flag_mean = False\n",
        "  \n",
        "  #     images_features = []\n",
        "  #     exemplar_set = []\n",
        "  #     exemplar_features = []\n",
        "\n",
        "  #     # compute the features of all images\n",
        "  #     for image in images:\n",
        "  #       image = transform(image[0]).to(DEVICE).unsqueeze(0)\n",
        "  #       feature = self.extractor(image)\n",
        "  #       feature.data = feature.data / feature.data.norm()\n",
        "  #       images_features.append(feature)\n",
        "  #     # images_features is a tensor n x 64, where n is the number of images of a class\n",
        "  #     images_features = torch.stack(images_features, dim=0)\n",
        "  #     class_mean = torch.mean(images_features, 0)\n",
        "  #     class_mean = class_mean / class_mean.norm()\n",
        "  #     # for the \"new classes\" use the mean computed on all data of the class instead\n",
        "  #     # of using a limited number of exemplars\n",
        "  #     self.exemplar_means.append(class_mean)\n",
        "\n",
        "  #     # choose exemplars based on distance from class mean\n",
        "  #     for i in range(exemplars_per_class):\n",
        "  #       mask = [True]*len(images)\n",
        "  #       total = np.sum(exemplar_features, axis=0)\n",
        "  #       extracted_features = images_features\n",
        "  #       mean = class_mean\n",
        "  #       average_feature_vector = (float(1)/(i+1))*(extracted_features + total).squeeze(1)\n",
        "  #       average_feature_vector = average_feature_vector / average_feature_vector.norm()\n",
        "  #       j = torch.argmin(torch.sqrt(\n",
        "  #           torch.sum(mean-average_feature_vector, 1)**2))\n",
        "  #       exemplar_set.append(images[j][0])\n",
        "  #       exemplar_features.append(extracted_features[j].squeeze(0))\n",
        "  #       mask[j] = False\n",
        "  #       images_features = images_features[mask]\n",
        "  #       images = np.array(images)[mask].tolist()\n",
        "\n",
        "  #     self.exemplar_sets.append(exemplar_set)\n",
        "  #     print(f\"Created exemplar set for class {images[0][1]} of len {len(self.exemplar_sets[0])}\")\n",
        "\n",
        "  # def construct_examplar_set_random(self, images, exemplars_per_class, transform):\n",
        "  #   # images: batch of a single class, used to construct exemplar sets\n",
        "  #   # exemplars per class: m = K/known_classes\n",
        "  #   # transform: transformation to be applied to raw exemplar images\n",
        "\n",
        "  #   self.eval()\n",
        "  #   with torch.no_grad():\n",
        "  #     images_features = []\n",
        "  #     exemplar_set = []\n",
        "  #     exemplar_features = []\n",
        "\n",
        "  #     for image in images:\n",
        "  #       image = transform(image[0]).to(DEVICE).unsqueeze(0)\n",
        "  #       feature = self.extractor(image)\n",
        "  #       feature.data = feature.data / feature.data.norm()\n",
        "  #       images_features.append(feature)\n",
        "\n",
        "  #     # images_features is a tensor n x 64, where n is the number of images of a class\n",
        "  #     images_features = torch.stack(images_features, dim=0)\n",
        "  #     class_mean = torch.mean(images_features, 0)\n",
        "  #     class_mean = class_mean / class_mean.norm()\n",
        "  #     # for the \"new classes\" use the mean computed on all data of the class instead\n",
        "  #     # of using a limited number of exemplars\n",
        "  #     self.exemplar_means.append(class_mean)\n",
        "\n",
        "  #     # choose exemplars at random (avoids overfitting when considering few exemplars)\n",
        "  #     for i in range(exemplars_per_class):                \n",
        "  #       mask = [True]*len(images)\n",
        "  #       j = random.choice(range(len(images)))\n",
        "  #       exemplar_set.append(images[j][0])\n",
        "  #       mask[j] = False\n",
        "  #       images_features = images_features[mask]\n",
        "  #       images = np.array(images)[mask].tolist()\n",
        "\n",
        "  #     self.exemplar_sets.append(exemplar_set)\n",
        "  #     print(f\"Created exemplar set for class {images[0][1]} of len {len(self.exemplar_sets[0])}\")\n",
        "\n",
        "  # def construct_exemplar_set(self, images, exemplars_per_class, transform):\n",
        "  #   # images: batch of a single class, used to construct exemplar sets\n",
        "  #   # exemplars per class: m = K/known_classes\n",
        "  #   # transform: transformation to be applied to raw exemplar images\n",
        "\n",
        "  #   self.eval()\n",
        "  #   with torch.no_grad():\n",
        "  #     images_features = []\n",
        "  #     exemplar_set = []\n",
        "  #     exemplar_features = []\n",
        "\n",
        "  #     # compute the features of all images\n",
        "  #     for image in images:\n",
        "  #       image = transform(image[0]).to(DEVICE).unsqueeze(0)\n",
        "  #       feature = self.extractor(image)\n",
        "  #       feature.data = feature.data / feature.data.norm()\n",
        "  #       images_features.append(feature)\n",
        "  #     # images_features is a tensor n x 64, where n is the number of images of a class\n",
        "  #     images_features = torch.stack(images_features, dim=0)\n",
        "  #     class_mean = torch.mean(images_features, 0)\n",
        "  #     class_mean = class_mean / class_mean.norm()\n",
        "  #     # for the \"new classes\" use the mean computed on all data of the class instead\n",
        "  #     # of using a limited number of exemplars\n",
        "  #     self.exemplar_means.append(class_mean)\n",
        "\n",
        "  #     # choose exemplars based on distance from class mean\n",
        "  #     for i in range(exemplars_per_class):\n",
        "  #       mask = [True]*len(images)\n",
        "  #       total = np.sum(exemplar_features, axis=0)\n",
        "  #       extracted_features = images_features\n",
        "  #       mean = class_mean\n",
        "  #       average_feature_vector = (float(1)/(i+1))*(extracted_features + total).squeeze(1)\n",
        "  #       average_feature_vector = average_feature_vector / average_feature_vector.norm()\n",
        "  #       j = torch.argmin(torch.sqrt(\n",
        "  #           torch.sum(mean-average_feature_vector, 1)**2))\n",
        "  #       exemplar_set.append(images[j][0])\n",
        "  #       exemplar_features.append(extracted_features[j].squeeze(0))\n",
        "  #       mask[j] = False\n",
        "  #       images_features = images_features[mask]\n",
        "  #       images = np.array(images)[mask].tolist()\n",
        "\n",
        "  #     self.exemplar_sets.append(exemplar_set)\n",
        "  #     print(f\"Created exemplar set for class {images[0][1]} of len {len(self.exemplar_sets[0])}\")\n",
        "\n",
        "  # def increment_classes(self, classes_to_add):\n",
        "  #   # increments the number of classes we are using\n",
        "  #   # classes_to_add: list of new classes          \n",
        "  #   n_classes_to_add = len(classes_to_add)\n",
        "  #   self.list_known_classes+=classes_to_add\n",
        "  #   print(f\"Known classes {self.list_known_classes}\")\n",
        "  #   weight = self.fully_connected.weight.data\n",
        "  #   feature_size = self.fully_connected.in_features\n",
        "  #   old_num_classes = self.fully_connected.out_features\n",
        "  #   # edit last classification layer to accomodate new classes, keep previuos weights for old classes\n",
        "  #   self.fully_connected = nn.Linear(\n",
        "  #       feature_size, old_num_classes+n_classes_to_add, bias = True)\n",
        "  #   self.fully_connected.weight.data[:old_num_classes] = weight\n",
        "\n",
        "  # def reduce_exemplar_sets(self, m):\n",
        "  #   # m: new size for exemplar sets                    \n",
        "  #   for i in range(len(self.exemplar_sets)):\n",
        "  #     self.exemplar_sets[i] = self.exemplar_sets[i][:m]\n",
        "  #     print(f\"Reducing exemplars of class {i} to {len(self.exemplar_sets[i])}\")\n",
        "\n",
        "  # def combine_dataset_with_exemplars(self, dataset):\n",
        "  #   # dataset: training data to be used in update_representation, \n",
        "  #   #          append exemplars\n",
        "  #   list_exemplars = []\n",
        "  #   for label, exemplars in enumerate(self.exemplar_sets):\n",
        "  #     list_exemplars += [(image, label) for image in exemplars]\n",
        "  #     dataset.labels_to_int.append(label)\n",
        "  #   dataset.data = list_exemplars + dataset.data\n",
        "\n",
        "  # def update_representation(self, dataset):\n",
        "  #   # dataset: training data belonging to 10 new classes\n",
        "\n",
        "  #   ########################################\n",
        "  #   optimizer = optim.SGD(self.parameters(), lr=2.0,weight_decay=0.00001, momentum = 0.9)\n",
        "  #   scheduler = MultiStepLR(optimizer,[47,63],gamma=GAMMA)\n",
        "  #   ########################################\n",
        "\n",
        "  #   self.combine_dataset_with_exemplars(dataset)\n",
        "  #   self.flag_mean = True\n",
        "  #   classes_to_idx = dataset.labels_to_int\n",
        "  #   new_classes = [cls for cls in classes_to_idx if cls not in self.list_known_classes]\n",
        "\n",
        "  #   train_data_loader = DataLoader(\n",
        "  #       dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "    \n",
        "  #   # if some classes have been previously observed, compute the\n",
        "  #   # outputs of the new data on the old weights\n",
        "  #   if self.known_classes > 0:\n",
        "  #     logits_old_net = torch.zeros(len(dataset.data), self.known_classes).cuda()\n",
        "  #     self.eval()\n",
        "  #     with torch.no_grad():\n",
        "  #       for indices, images, labels in train_data_loader:\n",
        "  #         images = images.to(DEVICE)\n",
        "  #         g = self.forward(images)\n",
        "  #         sigmoid = torch.nn.Sigmoid()\n",
        "  #         logits_old_net[indices] = sigmoid(g)\n",
        "  #     torch.cuda.empty_cache()\n",
        "\n",
        "  #   self.increment_classes(new_classes)\n",
        "    \n",
        "  #   # start training on the new batch of 10 classes + exemplars\n",
        "  #   self.train()\n",
        "  #   eye = torch.eye(self.known_classes+len(new_classes))\n",
        "\n",
        "  #   for epoch in range(NUM_EPOCHS):\n",
        "  #     print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], LR: {scheduler.get_last_lr()}\")\n",
        "  #     for indices, images, labels in train_data_loader:\n",
        "  #       images = images.to(DEVICE)\n",
        "  #       labels = labels.to(DEVICE)\n",
        "  #       indices = indices.to(DEVICE)\n",
        "\n",
        "  #       optimizer.zero_grad()\n",
        "        \n",
        "  #       output = self.forward(images)\n",
        "\n",
        "  #       # for the first batch: calculate loss using as target the one hot encoded labels\n",
        "  #       # no knowledge of previous classes, classification loss only\n",
        "  #       if self.known_classes==0:\n",
        "  #         # one hot encode new classes\n",
        "\n",
        "  #         labels_one_hot_new_classes = []\n",
        "  #         for label in labels:\n",
        "  #           labels_one_hot_new_classes.append(eye[label])\n",
        "  #         labels_one_hot_new_classes = torch.stack(labels_one_hot_new_classes).cuda()\n",
        "\n",
        "  #         loss=self.loss(output,labels_one_hot_new_classes)\n",
        "  #         loss.backward()\n",
        "\n",
        "  #       # calculate loss taking into account knowledge of previous classes and performance on\n",
        "  #       # the previous net's weights, clf + dst\n",
        "  #       if self.known_classes > 0:\n",
        "  #         labels_one_hot_new_classes = eye[:, self.known_classes:]\n",
        "  #         labels_one_hot = []\n",
        "  #         for label in labels:\n",
        "  #           labels_one_hot.append(labels_one_hot_new_classes[label])\n",
        "  #         labels_one_hot = torch.stack(labels_one_hot).cuda()\n",
        "  #         logits = logits_old_net[indices].cuda()\n",
        "\n",
        "  #         # concatenate old network's logits and new labels one hot encoded (0 for old classes)\n",
        "  #         labels_concatenate=torch.cat((logits,labels_one_hot),dim=1)\n",
        "  #         loss=self.loss(output,labels_concatenate)\n",
        "  #         loss.backward()\n",
        "\n",
        "  #       optimizer.step()\n",
        "  #     torch.cuda.empty_cache()\n",
        "  #     print(f\"Loss: {loss.item()}\")\n",
        "  #     scheduler.step() \n",
        "  #   # increment number of known classes to accomodate newly learned 10 classes\n",
        "  #   self.known_classes += len(new_classes)\n",
        "\n",
        "  def update_representation_using_cross_entropy_and_kl(self, dataset):\n",
        "    # dataset: training data belonging to 10 new classes\n",
        "\n",
        "    ########################################\n",
        "    loss1=nn.CrossEntropyLoss(ignore_index=-1)\n",
        "    loss2=nn.KLDivLoss()\n",
        "    sigmoid = nn.Sigmoid()\n",
        "    log_sigmoid = nn.LogSigmoid()\n",
        "    softmax = nn.Softmax()\n",
        "    log_softmax = nn.LogSoftmax()\n",
        "    optimizer = optim.SGD(self.parameters(), lr=2,weight_decay=0.00001, momentum = 0.9)\n",
        "    scheduler = MultiStepLR(optimizer,[47,63],gamma=GAMMA)\n",
        "    ########################################\n",
        "\n",
        "    self.combine_dataset_with_exemplars(dataset)\n",
        "    self.flag_mean = True\n",
        "    classes_to_idx = dataset.labels_to_int\n",
        "    new_classes = [cls for cls in classes_to_idx if cls not in self.list_known_classes]\n",
        "\n",
        "    train_data_loader = DataLoader(\n",
        "        dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "    \n",
        "    # if some classes have been previously observed, compute the\n",
        "    # outputs of the new data on the old weights\n",
        "\n",
        "    if self.known_classes > 0:\n",
        "      self.eval()\n",
        "      logits_old_net = torch.zeros(len(dataset.data), self.known_classes).cuda()\n",
        "\n",
        "      with torch.no_grad():\n",
        "        for indices, images, labels in train_data_loader:\n",
        "          images = images.to(DEVICE)\n",
        "          g = self.forward(images)\n",
        "          logits_old_net[indices] = sigmoid(g)\n",
        "\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    self.increment_classes(new_classes)\n",
        "    \n",
        "    # start training on the new batch of 10 classes + exemplars\n",
        "    self.train()\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "      print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], LR: {scheduler.get_last_lr()}\")\n",
        "      i=0\n",
        "      for indices, images, labels in train_data_loader:\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        indices = indices.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = self.forward(images)\n",
        "\n",
        "        # classification loss only for the first batch of 10 classes\n",
        "        if self.known_classes==0:\n",
        "          #labels_one_hot_new_classes = []\n",
        "          #for label in labels:\n",
        "          #  labels_one_hot_new_classes.append(eye[label])\n",
        "          #labels_one_hot_new_classes = torch.stack(labels_one_hot_new_classes).cuda()\n",
        "          #loss=self.loss(output,labels_one_hot_new_classes)\n",
        "          loss=loss1(output,labels)\n",
        "          loss.backward()\n",
        "        # clf + dst loss\n",
        "        if self.known_classes > 0:\n",
        "          #labels_one_hot_new_classes = eye[:, self.known_classes:]\n",
        "          # print(labels_one_hot_new_classes.size())\n",
        "          #labels_one_hot = []\n",
        "          #for label in labels:\n",
        "          #  labels_one_hot.append(labels_one_hot_new_classes[label])\n",
        "          #labels_one_hot = torch.stack(labels_one_hot).cuda()\n",
        "          logits = logits_old_net[indices].cuda()\n",
        "          # print(f\"logits old net: {logits.size()}\")\n",
        "          # print(f\"new classes onehot encoded: {labels_one_hot.size()}\")\n",
        "          #labels_concatenate = torch.zeros(logits_old_net.size()\n",
        "          # print(f\"{logits.size()}-{labels_one_hot.size()}\")\n",
        "          #labels_concatenate=torch.cat((logits,labels_one_hot),dim=1)\n",
        "          loss_classification=loss1(output,labels)\n",
        "          loss_distillation=loss2(log_sigmoid(output[:,:self.known_classes]),logits)\n",
        "          loss=loss_classification+loss_distillation\n",
        "          loss.backward()\n",
        "        optimizer.step()\n",
        "        i+=1\n",
        "      torch.cuda.empty_cache()\n",
        "      print(f\"Loss: {loss.item()}\")\n",
        "      scheduler.step() \n",
        "    self.known_classes += len(new_classes)\n",
        "\n",
        "\n",
        "\n",
        "# ## OLD METHODS\n",
        "\n",
        "\n",
        "  def update_representation(self, dataset):             \n",
        "    self.combine_dataset_with_exemplars(dataset)\n",
        "    self.flag_mean = True\n",
        "    classes_to_idx = dataset.labels_to_int\n",
        "    new_classes = [cls for cls in classes_to_idx if cls not in self.list_known_classes] #gets indexes of new classes only\n",
        "\n",
        "    train_data_loader = DataLoader(\n",
        "        dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "    \n",
        "    preds_old_net = []\n",
        "    #preds_old_net = torch.zeros(len(dataset), self.num_classes).cuda()\n",
        "    \n",
        "    if self.known_classes > 0:\n",
        "\n",
        "      self.eval()\n",
        "      logits_old_net = torch.zeros(len(dataset.data), self.known_classes).cuda()\n",
        "      with torch.no_grad():\n",
        "        for indices, images, labels in train_data_loader:\n",
        "          images = images.to(DEVICE)\n",
        "          g = self.forward(images)\n",
        "          sigmoid = torch.nn.Sigmoid()\n",
        "          logits_old_net[indices] = sigmoid(g)\n",
        "        # preds_old_net.append(g.data)\n",
        "        # preds_old_net[indices] = g.data\n",
        "      \n",
        "      # print(f\"Labels old net size: {preds_old_net.size()}\")\n",
        "      # print(f\"Labels old net: {preds_old_net}\")\n",
        "      # preds_old_net = preds_old_net.to(DEVICE)\n",
        "      # logits_old_net = torch.cat(logits_old_net)         \n",
        "      # print(logits_old_net.size())\n",
        "      torch.cuda.empty_cache()\n",
        "\n",
        "    self.increment_classes(new_classes)\n",
        "    \n",
        "    optimizer = optim.SGD(self.parameters(), lr=2.0,weight_decay=0.00001, momentum = 0.9)\n",
        "    scheduler = MultiStepLR(optimizer,[47,63],gamma=GAMMA)\n",
        "    self.train()\n",
        "\n",
        "    eye = torch.eye(self.known_classes+len(new_classes))\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "      print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], LR: {scheduler.get_last_lr()}\")\n",
        "      i=0\n",
        "      for indices, images, labels in train_data_loader:\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        indices = indices.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        output = self.forward(images)\n",
        "        # print(f\"Output size: {output.size()}\")\n",
        "        # print(f\"Labels size: {labels.size()}\")\n",
        "\n",
        "        if self.known_classes==0:\n",
        "          labels_one_hot_new_classes = []\n",
        "          for label in labels:\n",
        "            labels_one_hot_new_classes.append(eye[label])\n",
        "          labels_one_hot_new_classes = torch.stack(labels_one_hot_new_classes).cuda()\n",
        "          loss=self.loss(output,labels_one_hot_new_classes)\n",
        "          loss.backward()\n",
        "        if self.known_classes > 0:\n",
        "          labels_one_hot_new_classes = eye[:, self.known_classes:]\n",
        "          # print(labels_one_hot_new_classes.size())\n",
        "          labels_one_hot = []\n",
        "          for label in labels:\n",
        "            labels_one_hot.append(labels_one_hot_new_classes[label])\n",
        "          labels_one_hot = torch.stack(labels_one_hot).cuda()\n",
        "          logits = logits_old_net[indices].cuda()\n",
        "          # print(f\"logits old net: {logits.size()}\")\n",
        "          # print(f\"new classes onehot encoded: {labels_one_hot.size()}\")\n",
        "          #labels_concatenate = torch.zeros(logits_old_net.size()\n",
        "          # print(f\"{logits.size()}-{labels_one_hot.size()}\")\n",
        "          labels_concatenate=torch.cat((logits,labels_one_hot),dim=1)\n",
        "          loss=self.loss(output,labels_concatenate)\n",
        "          loss.backward()\n",
        "        optimizer.step()\n",
        "        i+=1\n",
        "      torch.cuda.empty_cache()\n",
        "      print(f\"Loss: {loss.item()}\")\n",
        "      scheduler.step() \n",
        "    self.known_classes += len(new_classes)\n",
        "\n",
        "  def classify(self, input_image_batch, transform):\n",
        "      # input_image_batch: batch of up to 10 shuffled classes that we use for training and validation\n",
        "      # transform: transformation to be applied to raw exemplar images\n",
        "      batch_size = input_image_batch.size(0)\n",
        "      self.eval()\n",
        "      with torch.no_grad():\n",
        "        if self.flag_mean:\n",
        "          print(f\"num exemplar sets: {len(self.exemplar_sets)}\")\n",
        "          exemplar_means = []\n",
        "          for exemplars in self.exemplar_sets:\n",
        "            features = []\n",
        "            for exemplar in exemplars:\n",
        "              feature = self.extractor(transform(exemplar).unsqueeze(0).cuda())\n",
        "              feature.data = feature.data / feature.data.norm()\n",
        "              features.append(feature)\n",
        "            features = torch.stack(features)\n",
        "            exemplar_mean = torch.mean(features, 0)\n",
        "            exemplar_means.append(exemplar_mean)\n",
        "          print(f\"Exemplar means len: {len(exemplar_means)}\")\n",
        "          self.exemplar_means = exemplar_means\n",
        "          self.flag_mean = False\n",
        "\n",
        "\n",
        "        mean = torch.stack(self.exemplar_means).squeeze(1)    # tensor of dimension: known_classes x features (64)\n",
        "        # print(mean.size())\n",
        "        features = self.extractor(input_image_batch)          # extracts the features of the batch\n",
        "        predictions = []\n",
        "        for n, feature in enumerate(features):\n",
        "          feature.data = feature.data / feature.data.norm()   # normalize and returns the distance of the nearest one\n",
        "        # Predict label by nearest exemplar mean\n",
        "          distances = torch.sum((feature - mean)**2, 1)\n",
        "          predictions.append(torch.argmin(distances, 0))\n",
        "        return predictions\n",
        "\n",
        "  def classify_without_mean(self,input_image_batch):\n",
        "    # input_image_batch: batch of up to 10 shuffled classes that we use for training and validation\n",
        "\n",
        "    self.eval()\n",
        "    with torch.no_grad():\n",
        "      # mean: tensor of dimensions known_classes x features (64)\n",
        "      mean = torch.stack(self.exemplar_means).squeeze(1)    \n",
        "\n",
        "      features = self.extractor(input_image_batch)          # extracts the features of the batch\n",
        "      predictions = []\n",
        "      for n, feature in enumerate(features):\n",
        "        feature.data = feature.data / feature.data.norm()   # normalize and returns the distance of the nearest one\n",
        "        # Predict label by nearest exemplar mean\n",
        "        distances = torch.sum((feature - mean)**2, 1)\n",
        "        predictions.append(torch.argmin(distances, 0))\n",
        "      return predictions\n",
        "\n",
        "  def construct_with_class_mean(self, images, exemplars_per_class, transform):\n",
        "    self.eval()\n",
        "    with torch.no_grad():\n",
        "      if self.flag_mean:\n",
        "        print(f\"num exemplar sets: {len(self.exemplar_sets)}\")\n",
        "        exemplar_means = []\n",
        "        for exemplars in self.exemplar_sets:\n",
        "          features = []\n",
        "          for exemplar in exemplars:\n",
        "            feature = self.extractor(transform(exemplar).unsqueeze(0).cuda())\n",
        "            feature.data = feature.data / feature.data.norm()\n",
        "            features.append(feature)\n",
        "          features = torch.stack(features)\n",
        "          exemplar_mean = torch.mean(features, 0)\n",
        "          exemplar_means.append(exemplar_mean)\n",
        "        print(f\"Exemplar means len: {len(exemplar_means)}\")\n",
        "        self.exemplar_means = exemplar_means\n",
        "        self.flag_mean = False\n",
        "  \n",
        "      images_features = []\n",
        "      exemplar_set = []\n",
        "      exemplar_features = []\n",
        "      for image in images:\n",
        "        image = transform(image[0]).to(DEVICE).unsqueeze(0)\n",
        "        feature = self.extractor(image)\n",
        "        feature.data = feature.data / feature.data.norm()\n",
        "        images_features.append(feature)\n",
        "      images_features = torch.stack(images_features, dim=0)\n",
        "      # should do the mean on the dimension of the append\n",
        "      class_mean = torch.mean(images_features, 0)\n",
        "      class_mean = class_mean / class_mean.norm()\n",
        "      self.exemplar_means.append(class_mean)\n",
        "      for i in range(exemplars_per_class):                #gets the nearest image to the mean and returns it\n",
        "        mask = [True]*len(images)\n",
        "        total = np.sum(exemplar_features, axis=0)\n",
        "        extracted_features = images_features\n",
        "        mean = class_mean\n",
        "        average_feature_vector = (float(1)/(i+1))*(extracted_features + total).squeeze(1)\n",
        "        average_feature_vector = average_feature_vector / average_feature_vector.norm()\n",
        "        j = torch.argmin(torch.sqrt(\n",
        "            torch.sum(mean-average_feature_vector, 1)**2))\n",
        "        exemplar_set.append(images[j][0])\n",
        "        exemplar_features.append(extracted_features[j].squeeze(0))\n",
        "        mask[j] = False\n",
        "        images_features = images_features[mask]\n",
        "        images = np.array(images)[mask].tolist()\n",
        "      self.exemplar_sets.append(exemplar_set)\n",
        "      print(f\"Created exemplar set for class {images[0][1]} of len {len(self.exemplar_sets[0])}\")\n",
        "\n",
        "  def construct_exemplar_set(self, images, exemplars_per_class, transform):     # images: images of one class of the training set, exemplars_per_class is m, transform is the transformation\n",
        "      self.eval()\n",
        "      with torch.no_grad():\n",
        "        images_features = []\n",
        "        exemplar_set = []\n",
        "        exemplar_features = []\n",
        "        for image in images:\n",
        "          image = transform(image[0]).to(DEVICE).unsqueeze(0)\n",
        "          feature = self.extractor(image)\n",
        "          feature.data = feature.data / feature.data.norm()\n",
        "          images_features.append(feature)\n",
        "        images_features = torch.stack(images_features, dim=0)\n",
        "        # should do the mean on the dimension of the append\n",
        "        class_mean = torch.mean(images_features, 0)\n",
        "        class_mean = class_mean / class_mean.norm()         #computes the mean of the class and normalizes it, it creates the features of the average image\n",
        "\n",
        "        for i in range(exemplars_per_class):                #gets the nearest image to the mean and returns it\n",
        "          mask = [True]*len(images)\n",
        "          total = np.sum(exemplar_features, axis=0)\n",
        "          extracted_features = images_features\n",
        "          mean = class_mean\n",
        "          average_feature_vector = (float(1)/(i+1))*(extracted_features + total).squeeze(1)\n",
        "          average_feature_vector = average_feature_vector / average_feature_vector.norm()\n",
        "          j = torch.argmin(torch.sqrt(\n",
        "              torch.sum(mean-average_feature_vector, 1)**2))\n",
        "          exemplar_set.append(images[j][0])\n",
        "          exemplar_features.append(extracted_features[j].squeeze(0))\n",
        "          mask[j] = False\n",
        "          images_features = images_features[mask]\n",
        "          images = np.array(images)[mask].tolist()\n",
        "        self.exemplar_sets.append(exemplar_set)\n",
        "        print(f\"Created exemplar set for class {images[0][1]} of len {len(self.exemplar_sets[0])}\")\n",
        "\n",
        "  def construct_examplar_set_random(self, images, exemplars_per_class, transform):\n",
        "    self.eval()\n",
        "    with torch.no_grad():\n",
        "  \n",
        "      images_features = []\n",
        "      exemplar_set = []\n",
        "      exemplar_features = []\n",
        "      for image in images:\n",
        "        image = transform(image[0]).to(DEVICE).unsqueeze(0)\n",
        "        feature = self.extractor(image)\n",
        "        feature.data = feature.data / feature.data.norm()\n",
        "        images_features.append(feature)\n",
        "      images_features = torch.stack(images_features, dim=0)\n",
        "      # should do the mean on the dimension of the append\n",
        "      class_mean = torch.mean(images_features, 0)\n",
        "      class_mean = class_mean / class_mean.norm()\n",
        "      self.exemplar_means.append(class_mean)\n",
        "      for i in range(exemplars_per_class):                #gets the nearest image to the mean and returns it\n",
        "        mask = [True]*len(images)\n",
        "        j = random.choice(range(len(images)))\n",
        "        exemplar_set.append(images[j][0])\n",
        "        mask[j] = False\n",
        "        images_features = images_features[mask]\n",
        "        images = np.array(images)[mask].tolist()\n",
        "      self.exemplar_sets.append(exemplar_set)\n",
        "      print(f\"Created exemplar set for class {images[0][1]} of len {len(self.exemplar_sets[0])}\")\n",
        "\n",
        "  def compute_means(self,transform):\n",
        "    self.eval()\n",
        "    with torch.no_grad():\n",
        "      print(f\"num exemplar sets: {len(self.exemplar_sets)}\")\n",
        "      exemplar_means = []\n",
        "      for exemplars in self.exemplar_sets:\n",
        "        features = []\n",
        "        for exemplar in exemplars:\n",
        "          feature = self.extractor(transform(exemplar).unsqueeze(0).cuda())\n",
        "          feature.data = feature.data / feature.data.norm()\n",
        "          features.append(feature)\n",
        "        features = torch.stack(features)\n",
        "        exemplar_mean = torch.mean(features, 0)\n",
        "        exemplar_means.append(exemplar_mean)\n",
        "      print(f\"Exemplar means len: {len(exemplar_means)}\")\n",
        "      self.exemplar_means = exemplar_means\n",
        "      self.flag_mean = False\n",
        "\n",
        "  def combine_dataset_with_exemplars(self, dataset):    #combines them\n",
        "    list_exemplars = []\n",
        "    for label, exemplars in enumerate(self.exemplar_sets):\n",
        "      list_exemplars += [(image, label) for image in exemplars]\n",
        "      dataset.labels_to_int.append(label)\n",
        "    dataset.data = list_exemplars + dataset.data\n",
        "\n",
        "  def increment_classes(self, classes_to_add):          # increments the number of classes we are using\n",
        "    n_classes_to_add = len(classes_to_add)\n",
        "    self.list_known_classes+=classes_to_add             #add the new classes\n",
        "    print(f\"Known classes {self.list_known_classes}\")\n",
        "    weight = self.fully_connected.weight.data\n",
        "    feature_size = self.fully_connected.in_features\n",
        "    old_num_classes = self.fully_connected.out_features\n",
        "    self.fully_connected = nn.Linear(\n",
        "        feature_size, old_num_classes+n_classes_to_add, bias = True)\n",
        "    self.fully_connected.weight.data[:old_num_classes] = weight\n",
        "\n",
        "  def reduce_exemplar_sets(self, m):                    # reduces exemplar set\n",
        "      for i in range(len(self.exemplar_sets)):\n",
        "        self.exemplar_sets[i] = self.exemplar_sets[i][:m]\n",
        "        print(f\"Reducing exemplars of class {i} to {len(self.exemplar_sets[i])}\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkq_p07mdhDp",
        "colab_type": "text"
      },
      "source": [
        "TRAINING AND VALIDATE ICARL \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-3lYRZhdliE",
        "colab_type": "code",
        "outputId": "3cd69a79-e099-457e-ab60-f1ef95b0013b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "net=iCarlNet(n_classes=100)\n",
        "net.to(DEVICE)\n",
        "accuracies = []\n",
        "for i, train_dataset in enumerate(train_datasets):\n",
        "  torch.cuda.empty_cache()\n",
        "  print(f\"BATCH [{i}]\")\n",
        "  print(f\"Training on {train_dataset.labels} -> {train_dataset.labels_to_int}\")\n",
        "  net.train()\n",
        "  net.update_representation(copy.deepcopy(train_dataset))\n",
        "  #net.update_representation_using_cross_entropy_and_kl(copy.deepcopy(train_dataset))\n",
        "\n",
        "  m = int(K/((i+1)*10))\n",
        "  net.compute_means(train_dataset.transform)\n",
        "  net.reduce_exemplar_sets(m)\n",
        "  for data_per_label in train_dataset.data_per_label:\n",
        "    print(f\"Constructing exemplar for class [{data_per_label[0][1]}]\")\n",
        "    #net.construct_exemplar_set(data_per_label,m,train_dataset.transform)\n",
        "    #net.construct_exemplar_set_with_class_mean(data_per_label,m,train_dataset.transform)\n",
        "    net.construct_examplar_set_random(data_per_label,m,train_dataset.transform)\n",
        "  # net.compute_exemplar_list_knn(train_dataset.transform)\n",
        "  net.eval()\n",
        "  corrects = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for num in range(i+1):\n",
        "      print(f\"Validating classes {test_datasets[num].labels} -> {test_datasets[num].labels_to_int}\")\n",
        "      test_dataloader = DataLoader(test_datasets[num], batch_size=BATCH_SIZE, num_workers=4, shuffle = True)\n",
        "      for _, images, labels in test_dataloader:\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        # classify using NME\n",
        "        # preds = torch.stack(net.KNN_classify(images,100)).cuda()\n",
        "        preds = torch.stack(net.classify_without_mean(images)).cuda()\n",
        "        # Update Corrects\n",
        "        corrects += torch.sum(preds == labels.data).data.item()\n",
        "        total += len(images)\n",
        "    torch.cuda.empty_cache()\n",
        "    # Calculate Accuracy\n",
        "    accuracy = corrects / float(total)\n",
        "    accuracies.append(accuracy)\n",
        "    print(f\"Accuracy: {accuracy}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BATCH [0]\n",
            "Training on ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Known classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Epoch [1/70], LR: [2.0]\n",
            "Loss: 0.3305477797985077\n",
            "Epoch [2/70], LR: [2.0]\n",
            "Loss: 0.3301875591278076\n",
            "Epoch [3/70], LR: [2.0]\n",
            "Loss: 0.3216121196746826\n",
            "Epoch [4/70], LR: [2.0]\n",
            "Loss: 0.313019722700119\n",
            "Epoch [5/70], LR: [2.0]\n",
            "Loss: 0.3104317784309387\n",
            "Epoch [6/70], LR: [2.0]\n",
            "Loss: 0.3262775242328644\n",
            "Epoch [7/70], LR: [2.0]\n",
            "Loss: 0.33550313115119934\n",
            "Epoch [8/70], LR: [2.0]\n",
            "Loss: 0.29964563250541687\n",
            "Epoch [9/70], LR: [2.0]\n",
            "Loss: 0.2753533720970154\n",
            "Epoch [10/70], LR: [2.0]\n",
            "Loss: 0.22768452763557434\n",
            "Epoch [11/70], LR: [2.0]\n",
            "Loss: 0.27266407012939453\n",
            "Epoch [12/70], LR: [2.0]\n",
            "Loss: 0.3421652317047119\n",
            "Epoch [13/70], LR: [2.0]\n",
            "Loss: 0.3417690396308899\n",
            "Epoch [14/70], LR: [2.0]\n",
            "Loss: 0.3070462644100189\n",
            "Epoch [15/70], LR: [2.0]\n",
            "Loss: 0.2794950604438782\n",
            "Epoch [16/70], LR: [2.0]\n",
            "Loss: 0.34839722514152527\n",
            "Epoch [17/70], LR: [2.0]\n",
            "Loss: 0.2566055953502655\n",
            "Epoch [18/70], LR: [2.0]\n",
            "Loss: 0.23067179322242737\n",
            "Epoch [19/70], LR: [2.0]\n",
            "Loss: 0.3204760253429413\n",
            "Epoch [20/70], LR: [2.0]\n",
            "Loss: 0.15756751596927643\n",
            "Epoch [21/70], LR: [2.0]\n",
            "Loss: 0.2491595596075058\n",
            "Epoch [22/70], LR: [2.0]\n",
            "Loss: 0.33351239562034607\n",
            "Epoch [23/70], LR: [2.0]\n",
            "Loss: 0.38354966044425964\n",
            "Epoch [24/70], LR: [2.0]\n",
            "Loss: 0.34994199872016907\n",
            "Epoch [25/70], LR: [2.0]\n",
            "Loss: 0.2291155606508255\n",
            "Epoch [26/70], LR: [2.0]\n",
            "Loss: 0.26991987228393555\n",
            "Epoch [27/70], LR: [2.0]\n",
            "Loss: 0.20888900756835938\n",
            "Epoch [28/70], LR: [2.0]\n",
            "Loss: 0.19489596784114838\n",
            "Epoch [29/70], LR: [2.0]\n",
            "Loss: 0.34941646456718445\n",
            "Epoch [30/70], LR: [2.0]\n",
            "Loss: 0.1416367143392563\n",
            "Epoch [31/70], LR: [2.0]\n",
            "Loss: 0.16158609092235565\n",
            "Epoch [32/70], LR: [2.0]\n",
            "Loss: 0.297073096036911\n",
            "Epoch [33/70], LR: [2.0]\n",
            "Loss: 0.17505215108394623\n",
            "Epoch [34/70], LR: [2.0]\n",
            "Loss: 0.16539715230464935\n",
            "Epoch [35/70], LR: [2.0]\n",
            "Loss: 0.16649793088436127\n",
            "Epoch [36/70], LR: [2.0]\n",
            "Loss: 0.23109667003154755\n",
            "Epoch [37/70], LR: [2.0]\n",
            "Loss: 0.19664375483989716\n",
            "Epoch [38/70], LR: [2.0]\n",
            "Loss: 0.15062008798122406\n",
            "Epoch [39/70], LR: [2.0]\n",
            "Loss: 0.3113827705383301\n",
            "Epoch [40/70], LR: [2.0]\n",
            "Loss: 0.2552858889102936\n",
            "Epoch [41/70], LR: [2.0]\n",
            "Loss: 0.1149306520819664\n",
            "Epoch [42/70], LR: [2.0]\n",
            "Loss: 0.1527094691991806\n",
            "Epoch [43/70], LR: [2.0]\n",
            "Loss: 0.16324816644191742\n",
            "Epoch [44/70], LR: [2.0]\n",
            "Loss: 0.14486506581306458\n",
            "Epoch [45/70], LR: [2.0]\n",
            "Loss: 0.1573277711868286\n",
            "Epoch [46/70], LR: [2.0]\n",
            "Loss: 0.14614008367061615\n",
            "Epoch [47/70], LR: [2.0]\n",
            "Loss: 0.22035078704357147\n",
            "Epoch [48/70], LR: [0.4]\n",
            "Loss: 0.059211622923612595\n",
            "Epoch [49/70], LR: [0.4]\n",
            "Loss: 0.11352675408124924\n",
            "Epoch [50/70], LR: [0.4]\n",
            "Loss: 0.1479172259569168\n",
            "Epoch [51/70], LR: [0.4]\n",
            "Loss: 0.22980499267578125\n",
            "Epoch [52/70], LR: [0.4]\n",
            "Loss: 0.20320935547351837\n",
            "Epoch [53/70], LR: [0.4]\n",
            "Loss: 0.11448638886213303\n",
            "Epoch [54/70], LR: [0.4]\n",
            "Loss: 0.06454136967658997\n",
            "Epoch [55/70], LR: [0.4]\n",
            "Loss: 0.14578992128372192\n",
            "Epoch [56/70], LR: [0.4]\n",
            "Loss: 0.07862581312656403\n",
            "Epoch [57/70], LR: [0.4]\n",
            "Loss: 0.1391923874616623\n",
            "Epoch [58/70], LR: [0.4]\n",
            "Loss: 0.273342102766037\n",
            "Epoch [59/70], LR: [0.4]\n",
            "Loss: 0.2323044091463089\n",
            "Epoch [60/70], LR: [0.4]\n",
            "Loss: 0.0957607552409172\n",
            "Epoch [61/70], LR: [0.4]\n",
            "Loss: 0.16652871668338776\n",
            "Epoch [62/70], LR: [0.4]\n",
            "Loss: 0.23461496829986572\n",
            "Epoch [63/70], LR: [0.4]\n",
            "Loss: 0.1671200394630432\n",
            "Epoch [64/70], LR: [0.08000000000000002]\n",
            "Loss: 0.19441214203834534\n",
            "Epoch [65/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08733274042606354\n",
            "Epoch [66/70], LR: [0.08000000000000002]\n",
            "Loss: 0.19022271037101746\n",
            "Epoch [67/70], LR: [0.08000000000000002]\n",
            "Loss: 0.06822977215051651\n",
            "Epoch [68/70], LR: [0.08000000000000002]\n",
            "Loss: 0.10840952396392822\n",
            "Epoch [69/70], LR: [0.08000000000000002]\n",
            "Loss: 0.23752717673778534\n",
            "Epoch [70/70], LR: [0.08000000000000002]\n",
            "Loss: 0.10670311748981476\n",
            "num exemplar sets: 0\n",
            "Exemplar means len: 0\n",
            "Constructing exemplar for class [0]\n",
            "Created exemplar set for class 0 of len 200\n",
            "Constructing exemplar for class [1]\n",
            "Created exemplar set for class 1 of len 200\n",
            "Constructing exemplar for class [2]\n",
            "Created exemplar set for class 2 of len 200\n",
            "Constructing exemplar for class [3]\n",
            "Created exemplar set for class 3 of len 200\n",
            "Constructing exemplar for class [4]\n",
            "Created exemplar set for class 4 of len 200\n",
            "Constructing exemplar for class [5]\n",
            "Created exemplar set for class 5 of len 200\n",
            "Constructing exemplar for class [6]\n",
            "Created exemplar set for class 6 of len 200\n",
            "Constructing exemplar for class [7]\n",
            "Created exemplar set for class 7 of len 200\n",
            "Constructing exemplar for class [8]\n",
            "Created exemplar set for class 8 of len 200\n",
            "Constructing exemplar for class [9]\n",
            "Created exemplar set for class 9 of len 200\n",
            "Validating classes ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Accuracy: 0.841\n",
            "BATCH [1]\n",
            "Training on ['bicycle', 'plain', 'couch', 'lobster', 'lion', 'chair', 'tulip', 'television', 'skunk', 'girl'] -> [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Known classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Epoch [1/70], LR: [2.0]\n",
            "Loss: 0.17740218341350555\n",
            "Epoch [2/70], LR: [2.0]\n",
            "Loss: 0.141989067196846\n",
            "Epoch [3/70], LR: [2.0]\n",
            "Loss: 0.14567972719669342\n",
            "Epoch [4/70], LR: [2.0]\n",
            "Loss: 0.12970511615276337\n",
            "Epoch [5/70], LR: [2.0]\n",
            "Loss: 0.14165529608726501\n",
            "Epoch [6/70], LR: [2.0]\n",
            "Loss: 0.12368269264698029\n",
            "Epoch [7/70], LR: [2.0]\n",
            "Loss: 0.13598686456680298\n",
            "Epoch [8/70], LR: [2.0]\n",
            "Loss: 0.13791701197624207\n",
            "Epoch [9/70], LR: [2.0]\n",
            "Loss: 0.10929394513368607\n",
            "Epoch [10/70], LR: [2.0]\n",
            "Loss: 0.11585437506437302\n",
            "Epoch [11/70], LR: [2.0]\n",
            "Loss: 0.11925037950277328\n",
            "Epoch [12/70], LR: [2.0]\n",
            "Loss: 0.117918960750103\n",
            "Epoch [13/70], LR: [2.0]\n",
            "Loss: 0.11318057030439377\n",
            "Epoch [14/70], LR: [2.0]\n",
            "Loss: 0.11889722943305969\n",
            "Epoch [15/70], LR: [2.0]\n",
            "Loss: 0.12550610303878784\n",
            "Epoch [16/70], LR: [2.0]\n",
            "Loss: 0.10414216667413712\n",
            "Epoch [17/70], LR: [2.0]\n",
            "Loss: 0.12045671045780182\n",
            "Epoch [18/70], LR: [2.0]\n",
            "Loss: 0.10126098245382309\n",
            "Epoch [19/70], LR: [2.0]\n",
            "Loss: 0.11452559381723404\n",
            "Epoch [20/70], LR: [2.0]\n",
            "Loss: 0.10111687332391739\n",
            "Epoch [21/70], LR: [2.0]\n",
            "Loss: 0.1071099191904068\n",
            "Epoch [22/70], LR: [2.0]\n",
            "Loss: 0.10067974030971527\n",
            "Epoch [23/70], LR: [2.0]\n",
            "Loss: 0.11302118003368378\n",
            "Epoch [24/70], LR: [2.0]\n",
            "Loss: 0.09745366126298904\n",
            "Epoch [25/70], LR: [2.0]\n",
            "Loss: 0.1003648117184639\n",
            "Epoch [26/70], LR: [2.0]\n",
            "Loss: 0.108453668653965\n",
            "Epoch [27/70], LR: [2.0]\n",
            "Loss: 0.09356638044118881\n",
            "Epoch [28/70], LR: [2.0]\n",
            "Loss: 0.09328161925077438\n",
            "Epoch [29/70], LR: [2.0]\n",
            "Loss: 0.10904916375875473\n",
            "Epoch [30/70], LR: [2.0]\n",
            "Loss: 0.09725752472877502\n",
            "Epoch [31/70], LR: [2.0]\n",
            "Loss: 0.09782383590936661\n",
            "Epoch [32/70], LR: [2.0]\n",
            "Loss: 0.0835675448179245\n",
            "Epoch [33/70], LR: [2.0]\n",
            "Loss: 0.10013004392385483\n",
            "Epoch [34/70], LR: [2.0]\n",
            "Loss: 0.09148682653903961\n",
            "Epoch [35/70], LR: [2.0]\n",
            "Loss: 0.08725304901599884\n",
            "Epoch [36/70], LR: [2.0]\n",
            "Loss: 0.10418687015771866\n",
            "Epoch [37/70], LR: [2.0]\n",
            "Loss: 0.09859437495470047\n",
            "Epoch [38/70], LR: [2.0]\n",
            "Loss: 0.10114111006259918\n",
            "Epoch [39/70], LR: [2.0]\n",
            "Loss: 0.08428452908992767\n",
            "Epoch [40/70], LR: [2.0]\n",
            "Loss: 0.09379805624485016\n",
            "Epoch [41/70], LR: [2.0]\n",
            "Loss: 0.08500120043754578\n",
            "Epoch [42/70], LR: [2.0]\n",
            "Loss: 0.07343403249979019\n",
            "Epoch [43/70], LR: [2.0]\n",
            "Loss: 0.09750495105981827\n",
            "Epoch [44/70], LR: [2.0]\n",
            "Loss: 0.08355048298835754\n",
            "Epoch [45/70], LR: [2.0]\n",
            "Loss: 0.08225391805171967\n",
            "Epoch [46/70], LR: [2.0]\n",
            "Loss: 0.08734538406133652\n",
            "Epoch [47/70], LR: [2.0]\n",
            "Loss: 0.08695128560066223\n",
            "Epoch [48/70], LR: [0.4]\n",
            "Loss: 0.0810956209897995\n",
            "Epoch [49/70], LR: [0.4]\n",
            "Loss: 0.06372573971748352\n",
            "Epoch [50/70], LR: [0.4]\n",
            "Loss: 0.07122840732336044\n",
            "Epoch [51/70], LR: [0.4]\n",
            "Loss: 0.08006308972835541\n",
            "Epoch [52/70], LR: [0.4]\n",
            "Loss: 0.07940429449081421\n",
            "Epoch [53/70], LR: [0.4]\n",
            "Loss: 0.07539136707782745\n",
            "Epoch [54/70], LR: [0.4]\n",
            "Loss: 0.07119079679250717\n",
            "Epoch [55/70], LR: [0.4]\n",
            "Loss: 0.07437427341938019\n",
            "Epoch [56/70], LR: [0.4]\n",
            "Loss: 0.07332821190357208\n",
            "Epoch [57/70], LR: [0.4]\n",
            "Loss: 0.07442980259656906\n",
            "Epoch [58/70], LR: [0.4]\n",
            "Loss: 0.06772645562887192\n",
            "Epoch [59/70], LR: [0.4]\n",
            "Loss: 0.06732603907585144\n",
            "Epoch [60/70], LR: [0.4]\n",
            "Loss: 0.06745602935552597\n",
            "Epoch [61/70], LR: [0.4]\n",
            "Loss: 0.07718516141176224\n",
            "Epoch [62/70], LR: [0.4]\n",
            "Loss: 0.06829401105642319\n",
            "Epoch [63/70], LR: [0.4]\n",
            "Loss: 0.07196798175573349\n",
            "Epoch [64/70], LR: [0.08000000000000002]\n",
            "Loss: 0.0869617685675621\n",
            "Epoch [65/70], LR: [0.08000000000000002]\n",
            "Loss: 0.06505861133337021\n",
            "Epoch [66/70], LR: [0.08000000000000002]\n",
            "Loss: 0.06812425702810287\n",
            "Epoch [67/70], LR: [0.08000000000000002]\n",
            "Loss: 0.07281377911567688\n",
            "Epoch [68/70], LR: [0.08000000000000002]\n",
            "Loss: 0.07633688300848007\n",
            "Epoch [69/70], LR: [0.08000000000000002]\n",
            "Loss: 0.0703505203127861\n",
            "Epoch [70/70], LR: [0.08000000000000002]\n",
            "Loss: 0.06110367551445961\n",
            "num exemplar sets: 10\n",
            "Exemplar means len: 10\n",
            "Reducing exemplars of class 0 to 100\n",
            "Reducing exemplars of class 1 to 100\n",
            "Reducing exemplars of class 2 to 100\n",
            "Reducing exemplars of class 3 to 100\n",
            "Reducing exemplars of class 4 to 100\n",
            "Reducing exemplars of class 5 to 100\n",
            "Reducing exemplars of class 6 to 100\n",
            "Reducing exemplars of class 7 to 100\n",
            "Reducing exemplars of class 8 to 100\n",
            "Reducing exemplars of class 9 to 100\n",
            "Constructing exemplar for class [10]\n",
            "Created exemplar set for class 10 of len 100\n",
            "Constructing exemplar for class [11]\n",
            "Created exemplar set for class 11 of len 100\n",
            "Constructing exemplar for class [12]\n",
            "Created exemplar set for class 12 of len 100\n",
            "Constructing exemplar for class [13]\n",
            "Created exemplar set for class 13 of len 100\n",
            "Constructing exemplar for class [14]\n",
            "Created exemplar set for class 14 of len 100\n",
            "Constructing exemplar for class [15]\n",
            "Created exemplar set for class 15 of len 100\n",
            "Constructing exemplar for class [16]\n",
            "Created exemplar set for class 16 of len 100\n",
            "Constructing exemplar for class [17]\n",
            "Created exemplar set for class 17 of len 100\n",
            "Constructing exemplar for class [18]\n",
            "Created exemplar set for class 18 of len 100\n",
            "Constructing exemplar for class [19]\n",
            "Created exemplar set for class 19 of len 100\n",
            "Validating classes ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Validating classes ['bicycle', 'plain', 'couch', 'lobster', 'lion', 'chair', 'tulip', 'television', 'skunk', 'girl'] -> [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Accuracy: 0.782\n",
            "BATCH [2]\n",
            "Training on ['bowl', 'tank', 'lawn_mower', 'snake', 'ray', 'oak_tree', 'poppy', 'castle', 'telephone', 'clock'] -> [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "Known classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "Epoch [1/70], LR: [2.0]\n",
            "Loss: 0.14337070286273956\n",
            "Epoch [2/70], LR: [2.0]\n",
            "Loss: 0.13139650225639343\n",
            "Epoch [3/70], LR: [2.0]\n",
            "Loss: 0.11699701845645905\n",
            "Epoch [4/70], LR: [2.0]\n",
            "Loss: 0.11980602145195007\n",
            "Epoch [5/70], LR: [2.0]\n",
            "Loss: 0.09762774407863617\n",
            "Epoch [6/70], LR: [2.0]\n",
            "Loss: 0.10307851433753967\n",
            "Epoch [7/70], LR: [2.0]\n",
            "Loss: 0.10668924450874329\n",
            "Epoch [8/70], LR: [2.0]\n",
            "Loss: 0.12078921496868134\n",
            "Epoch [9/70], LR: [2.0]\n",
            "Loss: 0.11833322048187256\n",
            "Epoch [10/70], LR: [2.0]\n",
            "Loss: 0.09673624485731125\n",
            "Epoch [11/70], LR: [2.0]\n",
            "Loss: 0.09858567267656326\n",
            "Epoch [12/70], LR: [2.0]\n",
            "Loss: 0.08733056485652924\n",
            "Epoch [13/70], LR: [2.0]\n",
            "Loss: 0.09776551276445389\n",
            "Epoch [14/70], LR: [2.0]\n",
            "Loss: 0.10731790214776993\n",
            "Epoch [15/70], LR: [2.0]\n",
            "Loss: 0.09887082129716873\n",
            "Epoch [16/70], LR: [2.0]\n",
            "Loss: 0.09218407422304153\n",
            "Epoch [17/70], LR: [2.0]\n",
            "Loss: 0.09603939205408096\n",
            "Epoch [18/70], LR: [2.0]\n",
            "Loss: 0.0932917594909668\n",
            "Epoch [19/70], LR: [2.0]\n",
            "Loss: 0.09669188410043716\n",
            "Epoch [20/70], LR: [2.0]\n",
            "Loss: 0.09653160721063614\n",
            "Epoch [21/70], LR: [2.0]\n",
            "Loss: 0.11056691408157349\n",
            "Epoch [22/70], LR: [2.0]\n",
            "Loss: 0.10190203040838242\n",
            "Epoch [23/70], LR: [2.0]\n",
            "Loss: 0.09205128997564316\n",
            "Epoch [24/70], LR: [2.0]\n",
            "Loss: 0.08964583277702332\n",
            "Epoch [25/70], LR: [2.0]\n",
            "Loss: 0.09323201328516006\n",
            "Epoch [26/70], LR: [2.0]\n",
            "Loss: 0.0954807698726654\n",
            "Epoch [27/70], LR: [2.0]\n",
            "Loss: 0.09299791604280472\n",
            "Epoch [28/70], LR: [2.0]\n",
            "Loss: 0.09656038880348206\n",
            "Epoch [29/70], LR: [2.0]\n",
            "Loss: 0.08508942276239395\n",
            "Epoch [30/70], LR: [2.0]\n",
            "Loss: 0.09414413571357727\n",
            "Epoch [31/70], LR: [2.0]\n",
            "Loss: 0.08955374360084534\n",
            "Epoch [32/70], LR: [2.0]\n",
            "Loss: 0.09335833787918091\n",
            "Epoch [33/70], LR: [2.0]\n",
            "Loss: 0.0860610231757164\n",
            "Epoch [34/70], LR: [2.0]\n",
            "Loss: 0.0959458276629448\n",
            "Epoch [35/70], LR: [2.0]\n",
            "Loss: 0.09702470898628235\n",
            "Epoch [36/70], LR: [2.0]\n",
            "Loss: 0.08362208306789398\n",
            "Epoch [37/70], LR: [2.0]\n",
            "Loss: 0.0962049663066864\n",
            "Epoch [38/70], LR: [2.0]\n",
            "Loss: 0.08697168529033661\n",
            "Epoch [39/70], LR: [2.0]\n",
            "Loss: 0.08679431676864624\n",
            "Epoch [40/70], LR: [2.0]\n",
            "Loss: 0.09025924652814865\n",
            "Epoch [41/70], LR: [2.0]\n",
            "Loss: 0.08838019520044327\n",
            "Epoch [42/70], LR: [2.0]\n",
            "Loss: 0.08524736016988754\n",
            "Epoch [43/70], LR: [2.0]\n",
            "Loss: 0.09139260649681091\n",
            "Epoch [44/70], LR: [2.0]\n",
            "Loss: 0.08729980885982513\n",
            "Epoch [45/70], LR: [2.0]\n",
            "Loss: 0.08013419806957245\n",
            "Epoch [46/70], LR: [2.0]\n",
            "Loss: 0.08274383097887039\n",
            "Epoch [47/70], LR: [2.0]\n",
            "Loss: 0.09808003902435303\n",
            "Epoch [48/70], LR: [0.4]\n",
            "Loss: 0.07945989817380905\n",
            "Epoch [49/70], LR: [0.4]\n",
            "Loss: 0.07816877216100693\n",
            "Epoch [50/70], LR: [0.4]\n",
            "Loss: 0.07561302185058594\n",
            "Epoch [51/70], LR: [0.4]\n",
            "Loss: 0.07520756125450134\n",
            "Epoch [52/70], LR: [0.4]\n",
            "Loss: 0.07409359514713287\n",
            "Epoch [53/70], LR: [0.4]\n",
            "Loss: 0.07783794403076172\n",
            "Epoch [54/70], LR: [0.4]\n",
            "Loss: 0.07553745061159134\n",
            "Epoch [55/70], LR: [0.4]\n",
            "Loss: 0.0726860761642456\n",
            "Epoch [56/70], LR: [0.4]\n",
            "Loss: 0.0779130607843399\n",
            "Epoch [57/70], LR: [0.4]\n",
            "Loss: 0.06614495068788528\n",
            "Epoch [58/70], LR: [0.4]\n",
            "Loss: 0.07874221354722977\n",
            "Epoch [59/70], LR: [0.4]\n",
            "Loss: 0.07716742902994156\n",
            "Epoch [60/70], LR: [0.4]\n",
            "Loss: 0.06994958966970444\n",
            "Epoch [61/70], LR: [0.4]\n",
            "Loss: 0.0693533793091774\n",
            "Epoch [62/70], LR: [0.4]\n",
            "Loss: 0.07207398861646652\n",
            "Epoch [63/70], LR: [0.4]\n",
            "Loss: 0.07627630978822708\n",
            "Epoch [64/70], LR: [0.08000000000000002]\n",
            "Loss: 0.0736074149608612\n",
            "Epoch [65/70], LR: [0.08000000000000002]\n",
            "Loss: 0.06997708976268768\n",
            "Epoch [66/70], LR: [0.08000000000000002]\n",
            "Loss: 0.0699492022395134\n",
            "Epoch [67/70], LR: [0.08000000000000002]\n",
            "Loss: 0.06890454888343811\n",
            "Epoch [68/70], LR: [0.08000000000000002]\n",
            "Loss: 0.06825880706310272\n",
            "Epoch [69/70], LR: [0.08000000000000002]\n",
            "Loss: 0.07272133231163025\n",
            "Epoch [70/70], LR: [0.08000000000000002]\n",
            "Loss: 0.07330852001905441\n",
            "num exemplar sets: 20\n",
            "Exemplar means len: 20\n",
            "Reducing exemplars of class 0 to 66\n",
            "Reducing exemplars of class 1 to 66\n",
            "Reducing exemplars of class 2 to 66\n",
            "Reducing exemplars of class 3 to 66\n",
            "Reducing exemplars of class 4 to 66\n",
            "Reducing exemplars of class 5 to 66\n",
            "Reducing exemplars of class 6 to 66\n",
            "Reducing exemplars of class 7 to 66\n",
            "Reducing exemplars of class 8 to 66\n",
            "Reducing exemplars of class 9 to 66\n",
            "Reducing exemplars of class 10 to 66\n",
            "Reducing exemplars of class 11 to 66\n",
            "Reducing exemplars of class 12 to 66\n",
            "Reducing exemplars of class 13 to 66\n",
            "Reducing exemplars of class 14 to 66\n",
            "Reducing exemplars of class 15 to 66\n",
            "Reducing exemplars of class 16 to 66\n",
            "Reducing exemplars of class 17 to 66\n",
            "Reducing exemplars of class 18 to 66\n",
            "Reducing exemplars of class 19 to 66\n",
            "Constructing exemplar for class [20]\n",
            "Created exemplar set for class 20 of len 66\n",
            "Constructing exemplar for class [21]\n",
            "Created exemplar set for class 21 of len 66\n",
            "Constructing exemplar for class [22]\n",
            "Created exemplar set for class 22 of len 66\n",
            "Constructing exemplar for class [23]\n",
            "Created exemplar set for class 23 of len 66\n",
            "Constructing exemplar for class [24]\n",
            "Created exemplar set for class 24 of len 66\n",
            "Constructing exemplar for class [25]\n",
            "Created exemplar set for class 25 of len 66\n",
            "Constructing exemplar for class [26]\n",
            "Created exemplar set for class 26 of len 66\n",
            "Constructing exemplar for class [27]\n",
            "Created exemplar set for class 27 of len 66\n",
            "Constructing exemplar for class [28]\n",
            "Created exemplar set for class 28 of len 66\n",
            "Constructing exemplar for class [29]\n",
            "Created exemplar set for class 29 of len 66\n",
            "Validating classes ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Validating classes ['bicycle', 'plain', 'couch', 'lobster', 'lion', 'chair', 'tulip', 'television', 'skunk', 'girl'] -> [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Validating classes ['bowl', 'tank', 'lawn_mower', 'snake', 'ray', 'oak_tree', 'poppy', 'castle', 'telephone', 'clock'] -> [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "Accuracy: 0.7256666666666667\n",
            "BATCH [3]\n",
            "Training on ['worm', 'rabbit', 'tractor', 'cockroach', 'house', 'lamp', 'sweet_pepper', 'crab', 'beetle', 'dolphin'] -> [30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
            "Known classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
            "Epoch [1/70], LR: [2.0]\n",
            "Loss: 0.12736713886260986\n",
            "Epoch [2/70], LR: [2.0]\n",
            "Loss: 0.12690280377864838\n",
            "Epoch [3/70], LR: [2.0]\n",
            "Loss: 0.11315307766199112\n",
            "Epoch [4/70], LR: [2.0]\n",
            "Loss: 0.11222018301486969\n",
            "Epoch [5/70], LR: [2.0]\n",
            "Loss: 0.11139605194330215\n",
            "Epoch [6/70], LR: [2.0]\n",
            "Loss: 0.09404032677412033\n",
            "Epoch [7/70], LR: [2.0]\n",
            "Loss: 0.11140549927949905\n",
            "Epoch [8/70], LR: [2.0]\n",
            "Loss: 0.10626938939094543\n",
            "Epoch [9/70], LR: [2.0]\n",
            "Loss: 0.1068531945347786\n",
            "Epoch [10/70], LR: [2.0]\n",
            "Loss: 0.10494344681501389\n",
            "Epoch [11/70], LR: [2.0]\n",
            "Loss: 0.10217378288507462\n",
            "Epoch [12/70], LR: [2.0]\n",
            "Loss: 0.109779492020607\n",
            "Epoch [13/70], LR: [2.0]\n",
            "Loss: 0.10265012830495834\n",
            "Epoch [14/70], LR: [2.0]\n",
            "Loss: 0.09217020869255066\n",
            "Epoch [15/70], LR: [2.0]\n",
            "Loss: 0.10444667190313339\n",
            "Epoch [16/70], LR: [2.0]\n",
            "Loss: 0.09708009660243988\n",
            "Epoch [17/70], LR: [2.0]\n",
            "Loss: 0.10422547161579132\n",
            "Epoch [18/70], LR: [2.0]\n",
            "Loss: 0.10332080721855164\n",
            "Epoch [19/70], LR: [2.0]\n",
            "Loss: 0.09849622845649719\n",
            "Epoch [20/70], LR: [2.0]\n",
            "Loss: 0.09831561148166656\n",
            "Epoch [21/70], LR: [2.0]\n",
            "Loss: 0.10596422851085663\n",
            "Epoch [22/70], LR: [2.0]\n",
            "Loss: 0.09728917479515076\n",
            "Epoch [23/70], LR: [2.0]\n",
            "Loss: 0.09222543239593506\n",
            "Epoch [24/70], LR: [2.0]\n",
            "Loss: 0.09858503937721252\n",
            "Epoch [25/70], LR: [2.0]\n",
            "Loss: 0.08643485605716705\n",
            "Epoch [26/70], LR: [2.0]\n",
            "Loss: 0.09875121712684631\n",
            "Epoch [27/70], LR: [2.0]\n",
            "Loss: 0.08656658232212067\n",
            "Epoch [28/70], LR: [2.0]\n",
            "Loss: 0.0992908701300621\n",
            "Epoch [29/70], LR: [2.0]\n",
            "Loss: 0.09545591473579407\n",
            "Epoch [30/70], LR: [2.0]\n",
            "Loss: 0.09409838914871216\n",
            "Epoch [31/70], LR: [2.0]\n",
            "Loss: 0.0904150903224945\n",
            "Epoch [32/70], LR: [2.0]\n",
            "Loss: 0.08841158449649811\n",
            "Epoch [33/70], LR: [2.0]\n",
            "Loss: 0.09474579244852066\n",
            "Epoch [34/70], LR: [2.0]\n",
            "Loss: 0.09201355278491974\n",
            "Epoch [35/70], LR: [2.0]\n",
            "Loss: 0.09632152318954468\n",
            "Epoch [36/70], LR: [2.0]\n",
            "Loss: 0.09476587921380997\n",
            "Epoch [37/70], LR: [2.0]\n",
            "Loss: 0.0908476784825325\n",
            "Epoch [38/70], LR: [2.0]\n",
            "Loss: 0.0943455621600151\n",
            "Epoch [39/70], LR: [2.0]\n",
            "Loss: 0.09519678354263306\n",
            "Epoch [40/70], LR: [2.0]\n",
            "Loss: 0.09194699674844742\n",
            "Epoch [41/70], LR: [2.0]\n",
            "Loss: 0.09324607253074646\n",
            "Epoch [42/70], LR: [2.0]\n",
            "Loss: 0.0861094519495964\n",
            "Epoch [43/70], LR: [2.0]\n",
            "Loss: 0.09418992698192596\n",
            "Epoch [44/70], LR: [2.0]\n",
            "Loss: 0.0936499536037445\n",
            "Epoch [45/70], LR: [2.0]\n",
            "Loss: 0.09891945868730545\n",
            "Epoch [46/70], LR: [2.0]\n",
            "Loss: 0.10207149386405945\n",
            "Epoch [47/70], LR: [2.0]\n",
            "Loss: 0.09418360143899918\n",
            "Epoch [48/70], LR: [0.4]\n",
            "Loss: 0.0798628181219101\n",
            "Epoch [49/70], LR: [0.4]\n",
            "Loss: 0.0790584608912468\n",
            "Epoch [50/70], LR: [0.4]\n",
            "Loss: 0.09176091849803925\n",
            "Epoch [51/70], LR: [0.4]\n",
            "Loss: 0.08584120124578476\n",
            "Epoch [52/70], LR: [0.4]\n",
            "Loss: 0.08058615773916245\n",
            "Epoch [53/70], LR: [0.4]\n",
            "Loss: 0.0778157189488411\n",
            "Epoch [54/70], LR: [0.4]\n",
            "Loss: 0.07450906187295914\n",
            "Epoch [55/70], LR: [0.4]\n",
            "Loss: 0.07592243701219559\n",
            "Epoch [56/70], LR: [0.4]\n",
            "Loss: 0.0855390876531601\n",
            "Epoch [57/70], LR: [0.4]\n",
            "Loss: 0.07925738394260406\n",
            "Epoch [58/70], LR: [0.4]\n",
            "Loss: 0.0785759687423706\n",
            "Epoch [59/70], LR: [0.4]\n",
            "Loss: 0.08170348405838013\n",
            "Epoch [60/70], LR: [0.4]\n",
            "Loss: 0.0880206823348999\n",
            "Epoch [61/70], LR: [0.4]\n",
            "Loss: 0.07268152385950089\n",
            "Epoch [62/70], LR: [0.4]\n",
            "Loss: 0.07487989217042923\n",
            "Epoch [63/70], LR: [0.4]\n",
            "Loss: 0.06924998760223389\n",
            "Epoch [64/70], LR: [0.08000000000000002]\n",
            "Loss: 0.07640864700078964\n",
            "Epoch [65/70], LR: [0.08000000000000002]\n",
            "Loss: 0.07824624329805374\n",
            "Epoch [66/70], LR: [0.08000000000000002]\n",
            "Loss: 0.07754047960042953\n",
            "Epoch [67/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08135712891817093\n",
            "Epoch [68/70], LR: [0.08000000000000002]\n",
            "Loss: 0.07596848160028458\n",
            "Epoch [69/70], LR: [0.08000000000000002]\n",
            "Loss: 0.0771913230419159\n",
            "Epoch [70/70], LR: [0.08000000000000002]\n",
            "Loss: 0.07910270243883133\n",
            "num exemplar sets: 30\n",
            "Exemplar means len: 30\n",
            "Reducing exemplars of class 0 to 50\n",
            "Reducing exemplars of class 1 to 50\n",
            "Reducing exemplars of class 2 to 50\n",
            "Reducing exemplars of class 3 to 50\n",
            "Reducing exemplars of class 4 to 50\n",
            "Reducing exemplars of class 5 to 50\n",
            "Reducing exemplars of class 6 to 50\n",
            "Reducing exemplars of class 7 to 50\n",
            "Reducing exemplars of class 8 to 50\n",
            "Reducing exemplars of class 9 to 50\n",
            "Reducing exemplars of class 10 to 50\n",
            "Reducing exemplars of class 11 to 50\n",
            "Reducing exemplars of class 12 to 50\n",
            "Reducing exemplars of class 13 to 50\n",
            "Reducing exemplars of class 14 to 50\n",
            "Reducing exemplars of class 15 to 50\n",
            "Reducing exemplars of class 16 to 50\n",
            "Reducing exemplars of class 17 to 50\n",
            "Reducing exemplars of class 18 to 50\n",
            "Reducing exemplars of class 19 to 50\n",
            "Reducing exemplars of class 20 to 50\n",
            "Reducing exemplars of class 21 to 50\n",
            "Reducing exemplars of class 22 to 50\n",
            "Reducing exemplars of class 23 to 50\n",
            "Reducing exemplars of class 24 to 50\n",
            "Reducing exemplars of class 25 to 50\n",
            "Reducing exemplars of class 26 to 50\n",
            "Reducing exemplars of class 27 to 50\n",
            "Reducing exemplars of class 28 to 50\n",
            "Reducing exemplars of class 29 to 50\n",
            "Constructing exemplar for class [30]\n",
            "Created exemplar set for class 30 of len 50\n",
            "Constructing exemplar for class [31]\n",
            "Created exemplar set for class 31 of len 50\n",
            "Constructing exemplar for class [32]\n",
            "Created exemplar set for class 32 of len 50\n",
            "Constructing exemplar for class [33]\n",
            "Created exemplar set for class 33 of len 50\n",
            "Constructing exemplar for class [34]\n",
            "Created exemplar set for class 34 of len 50\n",
            "Constructing exemplar for class [35]\n",
            "Created exemplar set for class 35 of len 50\n",
            "Constructing exemplar for class [36]\n",
            "Created exemplar set for class 36 of len 50\n",
            "Constructing exemplar for class [37]\n",
            "Created exemplar set for class 37 of len 50\n",
            "Constructing exemplar for class [38]\n",
            "Created exemplar set for class 38 of len 50\n",
            "Constructing exemplar for class [39]\n",
            "Created exemplar set for class 39 of len 50\n",
            "Validating classes ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Validating classes ['bicycle', 'plain', 'couch', 'lobster', 'lion', 'chair', 'tulip', 'television', 'skunk', 'girl'] -> [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Validating classes ['bowl', 'tank', 'lawn_mower', 'snake', 'ray', 'oak_tree', 'poppy', 'castle', 'telephone', 'clock'] -> [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "Validating classes ['worm', 'rabbit', 'tractor', 'cockroach', 'house', 'lamp', 'sweet_pepper', 'crab', 'beetle', 'dolphin'] -> [30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
            "Accuracy: 0.6695\n",
            "BATCH [4]\n",
            "Training on ['mouse', 'flatfish', 'pear', 'lizard', 'shark', 'orchid', 'cup', 'bus', 'sunflower', 'dinosaur'] -> [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
            "Known classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
            "Epoch [1/70], LR: [2.0]\n",
            "Loss: 0.11346173286437988\n",
            "Epoch [2/70], LR: [2.0]\n",
            "Loss: 0.10760890692472458\n",
            "Epoch [3/70], LR: [2.0]\n",
            "Loss: 0.11699464172124863\n",
            "Epoch [4/70], LR: [2.0]\n",
            "Loss: 0.1029733270406723\n",
            "Epoch [5/70], LR: [2.0]\n",
            "Loss: 0.10927467793226242\n",
            "Epoch [6/70], LR: [2.0]\n",
            "Loss: 0.10001851618289948\n",
            "Epoch [7/70], LR: [2.0]\n",
            "Loss: 0.09369469434022903\n",
            "Epoch [8/70], LR: [2.0]\n",
            "Loss: 0.1030532568693161\n",
            "Epoch [9/70], LR: [2.0]\n",
            "Loss: 0.10489711165428162\n",
            "Epoch [10/70], LR: [2.0]\n",
            "Loss: 0.10270172357559204\n",
            "Epoch [11/70], LR: [2.0]\n",
            "Loss: 0.10023344308137894\n",
            "Epoch [12/70], LR: [2.0]\n",
            "Loss: 0.09952591359615326\n",
            "Epoch [13/70], LR: [2.0]\n",
            "Loss: 0.09827709197998047\n",
            "Epoch [14/70], LR: [2.0]\n",
            "Loss: 0.10001812875270844\n",
            "Epoch [15/70], LR: [2.0]\n",
            "Loss: 0.09592611342668533\n",
            "Epoch [16/70], LR: [2.0]\n",
            "Loss: 0.09280220419168472\n",
            "Epoch [17/70], LR: [2.0]\n",
            "Loss: 0.09637291729450226\n",
            "Epoch [18/70], LR: [2.0]\n",
            "Loss: 0.10231850296258926\n",
            "Epoch [19/70], LR: [2.0]\n",
            "Loss: 0.09197776019573212\n",
            "Epoch [20/70], LR: [2.0]\n",
            "Loss: 0.08784092962741852\n",
            "Epoch [21/70], LR: [2.0]\n",
            "Loss: 0.08625775575637817\n",
            "Epoch [22/70], LR: [2.0]\n",
            "Loss: 0.10175901651382446\n",
            "Epoch [23/70], LR: [2.0]\n",
            "Loss: 0.0929669588804245\n",
            "Epoch [24/70], LR: [2.0]\n",
            "Loss: 0.09893254935741425\n",
            "Epoch [25/70], LR: [2.0]\n",
            "Loss: 0.09196142107248306\n",
            "Epoch [26/70], LR: [2.0]\n",
            "Loss: 0.09322526305913925\n",
            "Epoch [27/70], LR: [2.0]\n",
            "Loss: 0.08835005015134811\n",
            "Epoch [28/70], LR: [2.0]\n",
            "Loss: 0.0964876040816307\n",
            "Epoch [29/70], LR: [2.0]\n",
            "Loss: 0.09937144815921783\n",
            "Epoch [30/70], LR: [2.0]\n",
            "Loss: 0.09498491138219833\n",
            "Epoch [31/70], LR: [2.0]\n",
            "Loss: 0.09221151471138\n",
            "Epoch [32/70], LR: [2.0]\n",
            "Loss: 0.0874004140496254\n",
            "Epoch [33/70], LR: [2.0]\n",
            "Loss: 0.09335365891456604\n",
            "Epoch [34/70], LR: [2.0]\n",
            "Loss: 0.09150176495313644\n",
            "Epoch [35/70], LR: [2.0]\n",
            "Loss: 0.08508601784706116\n",
            "Epoch [36/70], LR: [2.0]\n",
            "Loss: 0.09435280412435532\n",
            "Epoch [37/70], LR: [2.0]\n",
            "Loss: 0.09266381710767746\n",
            "Epoch [38/70], LR: [2.0]\n",
            "Loss: 0.08870667964220047\n",
            "Epoch [39/70], LR: [2.0]\n",
            "Loss: 0.10134346783161163\n",
            "Epoch [40/70], LR: [2.0]\n",
            "Loss: 0.092142753303051\n",
            "Epoch [41/70], LR: [2.0]\n",
            "Loss: 0.09046044200658798\n",
            "Epoch [42/70], LR: [2.0]\n",
            "Loss: 0.09521498531103134\n",
            "Epoch [43/70], LR: [2.0]\n",
            "Loss: 0.08795478940010071\n",
            "Epoch [44/70], LR: [2.0]\n",
            "Loss: 0.08598816394805908\n",
            "Epoch [45/70], LR: [2.0]\n",
            "Loss: 0.09186546504497528\n",
            "Epoch [46/70], LR: [2.0]\n",
            "Loss: 0.09224040806293488\n",
            "Epoch [47/70], LR: [2.0]\n",
            "Loss: 0.0889824777841568\n",
            "Epoch [48/70], LR: [0.4]\n",
            "Loss: 0.08336080610752106\n",
            "Epoch [49/70], LR: [0.4]\n",
            "Loss: 0.08299733698368073\n",
            "Epoch [50/70], LR: [0.4]\n",
            "Loss: 0.08319616317749023\n",
            "Epoch [51/70], LR: [0.4]\n",
            "Loss: 0.08301550149917603\n",
            "Epoch [52/70], LR: [0.4]\n",
            "Loss: 0.08169400691986084\n",
            "Epoch [53/70], LR: [0.4]\n",
            "Loss: 0.08344676345586777\n",
            "Epoch [54/70], LR: [0.4]\n",
            "Loss: 0.08394619822502136\n",
            "Epoch [55/70], LR: [0.4]\n",
            "Loss: 0.08582329750061035\n",
            "Epoch [56/70], LR: [0.4]\n",
            "Loss: 0.07878657430410385\n",
            "Epoch [57/70], LR: [0.4]\n",
            "Loss: 0.08246257156133652\n",
            "Epoch [58/70], LR: [0.4]\n",
            "Loss: 0.0826815664768219\n",
            "Epoch [59/70], LR: [0.4]\n",
            "Loss: 0.0741545632481575\n",
            "Epoch [60/70], LR: [0.4]\n",
            "Loss: 0.08551603555679321\n",
            "Epoch [61/70], LR: [0.4]\n",
            "Loss: 0.077551931142807\n",
            "Epoch [62/70], LR: [0.4]\n",
            "Loss: 0.07756290584802628\n",
            "Epoch [63/70], LR: [0.4]\n",
            "Loss: 0.0780455619096756\n",
            "Epoch [64/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08138900995254517\n",
            "Epoch [65/70], LR: [0.08000000000000002]\n",
            "Loss: 0.0731659084558487\n",
            "Epoch [66/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08124864101409912\n",
            "Epoch [67/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08311230689287186\n",
            "Epoch [68/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08193106949329376\n",
            "Epoch [69/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08117310702800751\n",
            "Epoch [70/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08011101931333542\n",
            "num exemplar sets: 40\n",
            "Exemplar means len: 40\n",
            "Reducing exemplars of class 0 to 40\n",
            "Reducing exemplars of class 1 to 40\n",
            "Reducing exemplars of class 2 to 40\n",
            "Reducing exemplars of class 3 to 40\n",
            "Reducing exemplars of class 4 to 40\n",
            "Reducing exemplars of class 5 to 40\n",
            "Reducing exemplars of class 6 to 40\n",
            "Reducing exemplars of class 7 to 40\n",
            "Reducing exemplars of class 8 to 40\n",
            "Reducing exemplars of class 9 to 40\n",
            "Reducing exemplars of class 10 to 40\n",
            "Reducing exemplars of class 11 to 40\n",
            "Reducing exemplars of class 12 to 40\n",
            "Reducing exemplars of class 13 to 40\n",
            "Reducing exemplars of class 14 to 40\n",
            "Reducing exemplars of class 15 to 40\n",
            "Reducing exemplars of class 16 to 40\n",
            "Reducing exemplars of class 17 to 40\n",
            "Reducing exemplars of class 18 to 40\n",
            "Reducing exemplars of class 19 to 40\n",
            "Reducing exemplars of class 20 to 40\n",
            "Reducing exemplars of class 21 to 40\n",
            "Reducing exemplars of class 22 to 40\n",
            "Reducing exemplars of class 23 to 40\n",
            "Reducing exemplars of class 24 to 40\n",
            "Reducing exemplars of class 25 to 40\n",
            "Reducing exemplars of class 26 to 40\n",
            "Reducing exemplars of class 27 to 40\n",
            "Reducing exemplars of class 28 to 40\n",
            "Reducing exemplars of class 29 to 40\n",
            "Reducing exemplars of class 30 to 40\n",
            "Reducing exemplars of class 31 to 40\n",
            "Reducing exemplars of class 32 to 40\n",
            "Reducing exemplars of class 33 to 40\n",
            "Reducing exemplars of class 34 to 40\n",
            "Reducing exemplars of class 35 to 40\n",
            "Reducing exemplars of class 36 to 40\n",
            "Reducing exemplars of class 37 to 40\n",
            "Reducing exemplars of class 38 to 40\n",
            "Reducing exemplars of class 39 to 40\n",
            "Constructing exemplar for class [40]\n",
            "Created exemplar set for class 40 of len 40\n",
            "Constructing exemplar for class [41]\n",
            "Created exemplar set for class 41 of len 40\n",
            "Constructing exemplar for class [42]\n",
            "Created exemplar set for class 42 of len 40\n",
            "Constructing exemplar for class [43]\n",
            "Created exemplar set for class 43 of len 40\n",
            "Constructing exemplar for class [44]\n",
            "Created exemplar set for class 44 of len 40\n",
            "Constructing exemplar for class [45]\n",
            "Created exemplar set for class 45 of len 40\n",
            "Constructing exemplar for class [46]\n",
            "Created exemplar set for class 46 of len 40\n",
            "Constructing exemplar for class [47]\n",
            "Created exemplar set for class 47 of len 40\n",
            "Constructing exemplar for class [48]\n",
            "Created exemplar set for class 48 of len 40\n",
            "Constructing exemplar for class [49]\n",
            "Created exemplar set for class 49 of len 40\n",
            "Validating classes ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Validating classes ['bicycle', 'plain', 'couch', 'lobster', 'lion', 'chair', 'tulip', 'television', 'skunk', 'girl'] -> [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Validating classes ['bowl', 'tank', 'lawn_mower', 'snake', 'ray', 'oak_tree', 'poppy', 'castle', 'telephone', 'clock'] -> [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "Validating classes ['worm', 'rabbit', 'tractor', 'cockroach', 'house', 'lamp', 'sweet_pepper', 'crab', 'beetle', 'dolphin'] -> [30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
            "Validating classes ['mouse', 'flatfish', 'pear', 'lizard', 'shark', 'orchid', 'cup', 'bus', 'sunflower', 'dinosaur'] -> [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
            "Accuracy: 0.6036\n",
            "BATCH [5]\n",
            "Training on ['whale', 'wolf', 'woman', 'cloud', 'porcupine', 'road', 'plate', 'table', 'sea', 'seal'] -> [50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
            "Known classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
            "Epoch [1/70], LR: [2.0]\n",
            "Loss: 0.11296764016151428\n",
            "Epoch [2/70], LR: [2.0]\n",
            "Loss: 0.10441529005765915\n",
            "Epoch [3/70], LR: [2.0]\n",
            "Loss: 0.10395073145627975\n",
            "Epoch [4/70], LR: [2.0]\n",
            "Loss: 0.10987749695777893\n",
            "Epoch [5/70], LR: [2.0]\n",
            "Loss: 0.09942608326673508\n",
            "Epoch [6/70], LR: [2.0]\n",
            "Loss: 0.0987917110323906\n",
            "Epoch [7/70], LR: [2.0]\n",
            "Loss: 0.08798953145742416\n",
            "Epoch [8/70], LR: [2.0]\n",
            "Loss: 0.09343062341213226\n",
            "Epoch [9/70], LR: [2.0]\n",
            "Loss: 0.09772106260061264\n",
            "Epoch [10/70], LR: [2.0]\n",
            "Loss: 0.0970277488231659\n",
            "Epoch [11/70], LR: [2.0]\n",
            "Loss: 0.09725093096494675\n",
            "Epoch [12/70], LR: [2.0]\n",
            "Loss: 0.0902176946401596\n",
            "Epoch [13/70], LR: [2.0]\n",
            "Loss: 0.09158708155155182\n",
            "Epoch [14/70], LR: [2.0]\n",
            "Loss: 0.08704441040754318\n",
            "Epoch [15/70], LR: [2.0]\n",
            "Loss: 0.09721418470144272\n",
            "Epoch [16/70], LR: [2.0]\n",
            "Loss: 0.09037558734416962\n",
            "Epoch [17/70], LR: [2.0]\n",
            "Loss: 0.09147496521472931\n",
            "Epoch [18/70], LR: [2.0]\n",
            "Loss: 0.0879335030913353\n",
            "Epoch [19/70], LR: [2.0]\n",
            "Loss: 0.09180105477571487\n",
            "Epoch [20/70], LR: [2.0]\n",
            "Loss: 0.09239721298217773\n",
            "Epoch [21/70], LR: [2.0]\n",
            "Loss: 0.09146583825349808\n",
            "Epoch [22/70], LR: [2.0]\n",
            "Loss: 0.08684639632701874\n",
            "Epoch [23/70], LR: [2.0]\n",
            "Loss: 0.08810295909643173\n",
            "Epoch [24/70], LR: [2.0]\n",
            "Loss: 0.08759207278490067\n",
            "Epoch [25/70], LR: [2.0]\n",
            "Loss: 0.08793799579143524\n",
            "Epoch [26/70], LR: [2.0]\n",
            "Loss: 0.09299828112125397\n",
            "Epoch [27/70], LR: [2.0]\n",
            "Loss: 0.0889042466878891\n",
            "Epoch [28/70], LR: [2.0]\n",
            "Loss: 0.09470473229885101\n",
            "Epoch [29/70], LR: [2.0]\n",
            "Loss: 0.08873008191585541\n",
            "Epoch [30/70], LR: [2.0]\n",
            "Loss: 0.08569169789552689\n",
            "Epoch [31/70], LR: [2.0]\n",
            "Loss: 0.08816740661859512\n",
            "Epoch [32/70], LR: [2.0]\n",
            "Loss: 0.08189786225557327\n",
            "Epoch [33/70], LR: [2.0]\n",
            "Loss: 0.08337350189685822\n",
            "Epoch [34/70], LR: [2.0]\n",
            "Loss: 0.09003555774688721\n",
            "Epoch [35/70], LR: [2.0]\n",
            "Loss: 0.08117493242025375\n",
            "Epoch [36/70], LR: [2.0]\n",
            "Loss: 0.09040842205286026\n",
            "Epoch [37/70], LR: [2.0]\n",
            "Loss: 0.08856236934661865\n",
            "Epoch [38/70], LR: [2.0]\n",
            "Loss: 0.09324527531862259\n",
            "Epoch [39/70], LR: [2.0]\n",
            "Loss: 0.0901898518204689\n",
            "Epoch [40/70], LR: [2.0]\n",
            "Loss: 0.08681655675172806\n",
            "Epoch [41/70], LR: [2.0]\n",
            "Loss: 0.08463528752326965\n",
            "Epoch [42/70], LR: [2.0]\n",
            "Loss: 0.09385503083467484\n",
            "Epoch [43/70], LR: [2.0]\n",
            "Loss: 0.08944869041442871\n",
            "Epoch [44/70], LR: [2.0]\n",
            "Loss: 0.08710905909538269\n",
            "Epoch [45/70], LR: [2.0]\n",
            "Loss: 0.09188094735145569\n",
            "Epoch [46/70], LR: [2.0]\n",
            "Loss: 0.09145265072584152\n",
            "Epoch [47/70], LR: [2.0]\n",
            "Loss: 0.088795505464077\n",
            "Epoch [48/70], LR: [0.4]\n",
            "Loss: 0.078666090965271\n",
            "Epoch [49/70], LR: [0.4]\n",
            "Loss: 0.08259247988462448\n",
            "Epoch [50/70], LR: [0.4]\n",
            "Loss: 0.07845639437437057\n",
            "Epoch [51/70], LR: [0.4]\n",
            "Loss: 0.08027715981006622\n",
            "Epoch [52/70], LR: [0.4]\n",
            "Loss: 0.07272898405790329\n",
            "Epoch [53/70], LR: [0.4]\n",
            "Loss: 0.07092275470495224\n",
            "Epoch [54/70], LR: [0.4]\n",
            "Loss: 0.08054676651954651\n",
            "Epoch [55/70], LR: [0.4]\n",
            "Loss: 0.08002274483442307\n",
            "Epoch [56/70], LR: [0.4]\n",
            "Loss: 0.08030564337968826\n",
            "Epoch [57/70], LR: [0.4]\n",
            "Loss: 0.08052048832178116\n",
            "Epoch [58/70], LR: [0.4]\n",
            "Loss: 0.0819343626499176\n",
            "Epoch [59/70], LR: [0.4]\n",
            "Loss: 0.07574506103992462\n",
            "Epoch [60/70], LR: [0.4]\n",
            "Loss: 0.07858510315418243\n",
            "Epoch [61/70], LR: [0.4]\n",
            "Loss: 0.07543976604938507\n",
            "Epoch [62/70], LR: [0.4]\n",
            "Loss: 0.08268506824970245\n",
            "Epoch [63/70], LR: [0.4]\n",
            "Loss: 0.08338338881731033\n",
            "Epoch [64/70], LR: [0.08000000000000002]\n",
            "Loss: 0.0765179768204689\n",
            "Epoch [65/70], LR: [0.08000000000000002]\n",
            "Loss: 0.07841108739376068\n",
            "Epoch [66/70], LR: [0.08000000000000002]\n",
            "Loss: 0.07757093012332916\n",
            "Epoch [67/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08109673112630844\n",
            "Epoch [68/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08112727105617523\n",
            "Epoch [69/70], LR: [0.08000000000000002]\n",
            "Loss: 0.0760275200009346\n",
            "Epoch [70/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08061296492815018\n",
            "num exemplar sets: 50\n",
            "Exemplar means len: 50\n",
            "Reducing exemplars of class 0 to 33\n",
            "Reducing exemplars of class 1 to 33\n",
            "Reducing exemplars of class 2 to 33\n",
            "Reducing exemplars of class 3 to 33\n",
            "Reducing exemplars of class 4 to 33\n",
            "Reducing exemplars of class 5 to 33\n",
            "Reducing exemplars of class 6 to 33\n",
            "Reducing exemplars of class 7 to 33\n",
            "Reducing exemplars of class 8 to 33\n",
            "Reducing exemplars of class 9 to 33\n",
            "Reducing exemplars of class 10 to 33\n",
            "Reducing exemplars of class 11 to 33\n",
            "Reducing exemplars of class 12 to 33\n",
            "Reducing exemplars of class 13 to 33\n",
            "Reducing exemplars of class 14 to 33\n",
            "Reducing exemplars of class 15 to 33\n",
            "Reducing exemplars of class 16 to 33\n",
            "Reducing exemplars of class 17 to 33\n",
            "Reducing exemplars of class 18 to 33\n",
            "Reducing exemplars of class 19 to 33\n",
            "Reducing exemplars of class 20 to 33\n",
            "Reducing exemplars of class 21 to 33\n",
            "Reducing exemplars of class 22 to 33\n",
            "Reducing exemplars of class 23 to 33\n",
            "Reducing exemplars of class 24 to 33\n",
            "Reducing exemplars of class 25 to 33\n",
            "Reducing exemplars of class 26 to 33\n",
            "Reducing exemplars of class 27 to 33\n",
            "Reducing exemplars of class 28 to 33\n",
            "Reducing exemplars of class 29 to 33\n",
            "Reducing exemplars of class 30 to 33\n",
            "Reducing exemplars of class 31 to 33\n",
            "Reducing exemplars of class 32 to 33\n",
            "Reducing exemplars of class 33 to 33\n",
            "Reducing exemplars of class 34 to 33\n",
            "Reducing exemplars of class 35 to 33\n",
            "Reducing exemplars of class 36 to 33\n",
            "Reducing exemplars of class 37 to 33\n",
            "Reducing exemplars of class 38 to 33\n",
            "Reducing exemplars of class 39 to 33\n",
            "Reducing exemplars of class 40 to 33\n",
            "Reducing exemplars of class 41 to 33\n",
            "Reducing exemplars of class 42 to 33\n",
            "Reducing exemplars of class 43 to 33\n",
            "Reducing exemplars of class 44 to 33\n",
            "Reducing exemplars of class 45 to 33\n",
            "Reducing exemplars of class 46 to 33\n",
            "Reducing exemplars of class 47 to 33\n",
            "Reducing exemplars of class 48 to 33\n",
            "Reducing exemplars of class 49 to 33\n",
            "Constructing exemplar for class [50]\n",
            "Created exemplar set for class 50 of len 33\n",
            "Constructing exemplar for class [51]\n",
            "Created exemplar set for class 51 of len 33\n",
            "Constructing exemplar for class [52]\n",
            "Created exemplar set for class 52 of len 33\n",
            "Constructing exemplar for class [53]\n",
            "Created exemplar set for class 53 of len 33\n",
            "Constructing exemplar for class [54]\n",
            "Created exemplar set for class 54 of len 33\n",
            "Constructing exemplar for class [55]\n",
            "Created exemplar set for class 55 of len 33\n",
            "Constructing exemplar for class [56]\n",
            "Created exemplar set for class 56 of len 33\n",
            "Constructing exemplar for class [57]\n",
            "Created exemplar set for class 57 of len 33\n",
            "Constructing exemplar for class [58]\n",
            "Created exemplar set for class 58 of len 33\n",
            "Constructing exemplar for class [59]\n",
            "Created exemplar set for class 59 of len 33\n",
            "Validating classes ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Validating classes ['bicycle', 'plain', 'couch', 'lobster', 'lion', 'chair', 'tulip', 'television', 'skunk', 'girl'] -> [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Validating classes ['bowl', 'tank', 'lawn_mower', 'snake', 'ray', 'oak_tree', 'poppy', 'castle', 'telephone', 'clock'] -> [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "Validating classes ['worm', 'rabbit', 'tractor', 'cockroach', 'house', 'lamp', 'sweet_pepper', 'crab', 'beetle', 'dolphin'] -> [30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
            "Validating classes ['mouse', 'flatfish', 'pear', 'lizard', 'shark', 'orchid', 'cup', 'bus', 'sunflower', 'dinosaur'] -> [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
            "Validating classes ['whale', 'wolf', 'woman', 'cloud', 'porcupine', 'road', 'plate', 'table', 'sea', 'seal'] -> [50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
            "Accuracy: 0.5563333333333333\n",
            "BATCH [6]\n",
            "Training on ['bear', 'apple', 'forest', 'streetcar', 'can', 'bed', 'crocodile', 'keyboard', 'boy', 'raccoon'] -> [60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
            "Known classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
            "Epoch [1/70], LR: [2.0]\n",
            "Loss: 0.11799006164073944\n",
            "Epoch [2/70], LR: [2.0]\n",
            "Loss: 0.10357670485973358\n",
            "Epoch [3/70], LR: [2.0]\n",
            "Loss: 0.09412600845098495\n",
            "Epoch [4/70], LR: [2.0]\n",
            "Loss: 0.1038612499833107\n",
            "Epoch [5/70], LR: [2.0]\n",
            "Loss: 0.10286275297403336\n",
            "Epoch [6/70], LR: [2.0]\n",
            "Loss: 0.09775417298078537\n",
            "Epoch [7/70], LR: [2.0]\n",
            "Loss: 0.0992107018828392\n",
            "Epoch [8/70], LR: [2.0]\n",
            "Loss: 0.09538096189498901\n",
            "Epoch [9/70], LR: [2.0]\n",
            "Loss: 0.09818179905414581\n",
            "Epoch [10/70], LR: [2.0]\n",
            "Loss: 0.09147674590349197\n",
            "Epoch [11/70], LR: [2.0]\n",
            "Loss: 0.08724980056285858\n",
            "Epoch [12/70], LR: [2.0]\n",
            "Loss: 0.10657872259616852\n",
            "Epoch [13/70], LR: [2.0]\n",
            "Loss: 0.09471917897462845\n",
            "Epoch [14/70], LR: [2.0]\n",
            "Loss: 0.09210728108882904\n",
            "Epoch [15/70], LR: [2.0]\n",
            "Loss: 0.09363945573568344\n",
            "Epoch [16/70], LR: [2.0]\n",
            "Loss: 0.0913812518119812\n",
            "Epoch [17/70], LR: [2.0]\n",
            "Loss: 0.09122195839881897\n",
            "Epoch [18/70], LR: [2.0]\n",
            "Loss: 0.09913777559995651\n",
            "Epoch [19/70], LR: [2.0]\n",
            "Loss: 0.08730746805667877\n",
            "Epoch [20/70], LR: [2.0]\n",
            "Loss: 0.09550903737545013\n",
            "Epoch [21/70], LR: [2.0]\n",
            "Loss: 0.09414492547512054\n",
            "Epoch [22/70], LR: [2.0]\n",
            "Loss: 0.09174303710460663\n",
            "Epoch [23/70], LR: [2.0]\n",
            "Loss: 0.09459888935089111\n",
            "Epoch [24/70], LR: [2.0]\n",
            "Loss: 0.0932082012295723\n",
            "Epoch [25/70], LR: [2.0]\n",
            "Loss: 0.08662141859531403\n",
            "Epoch [26/70], LR: [2.0]\n",
            "Loss: 0.09516261518001556\n",
            "Epoch [27/70], LR: [2.0]\n",
            "Loss: 0.09430450946092606\n",
            "Epoch [28/70], LR: [2.0]\n",
            "Loss: 0.08531346172094345\n",
            "Epoch [29/70], LR: [2.0]\n",
            "Loss: 0.09260833263397217\n",
            "Epoch [30/70], LR: [2.0]\n",
            "Loss: 0.09089545905590057\n",
            "Epoch [31/70], LR: [2.0]\n",
            "Loss: 0.09147278964519501\n",
            "Epoch [32/70], LR: [2.0]\n",
            "Loss: 0.09159106016159058\n",
            "Epoch [33/70], LR: [2.0]\n",
            "Loss: 0.08826537430286407\n",
            "Epoch [34/70], LR: [2.0]\n",
            "Loss: 0.08694998174905777\n",
            "Epoch [35/70], LR: [2.0]\n",
            "Loss: 0.0949697196483612\n",
            "Epoch [36/70], LR: [2.0]\n",
            "Loss: 0.09971382468938828\n",
            "Epoch [37/70], LR: [2.0]\n",
            "Loss: 0.08912575244903564\n",
            "Epoch [38/70], LR: [2.0]\n",
            "Loss: 0.09130160510540009\n",
            "Epoch [39/70], LR: [2.0]\n",
            "Loss: 0.0918155312538147\n",
            "Epoch [40/70], LR: [2.0]\n",
            "Loss: 0.08950968831777573\n",
            "Epoch [41/70], LR: [2.0]\n",
            "Loss: 0.08567457646131516\n",
            "Epoch [42/70], LR: [2.0]\n",
            "Loss: 0.0955619141459465\n",
            "Epoch [43/70], LR: [2.0]\n",
            "Loss: 0.08114243298768997\n",
            "Epoch [44/70], LR: [2.0]\n",
            "Loss: 0.08858327567577362\n",
            "Epoch [45/70], LR: [2.0]\n",
            "Loss: 0.08398573845624924\n",
            "Epoch [46/70], LR: [2.0]\n",
            "Loss: 0.08981220424175262\n",
            "Epoch [47/70], LR: [2.0]\n",
            "Loss: 0.08612201362848282\n",
            "Epoch [48/70], LR: [0.4]\n",
            "Loss: 0.08175142109394073\n",
            "Epoch [49/70], LR: [0.4]\n",
            "Loss: 0.08374948054552078\n",
            "Epoch [50/70], LR: [0.4]\n",
            "Loss: 0.07627005875110626\n",
            "Epoch [51/70], LR: [0.4]\n",
            "Loss: 0.07692521810531616\n",
            "Epoch [52/70], LR: [0.4]\n",
            "Loss: 0.08047538995742798\n",
            "Epoch [53/70], LR: [0.4]\n",
            "Loss: 0.08379659056663513\n",
            "Epoch [54/70], LR: [0.4]\n",
            "Loss: 0.08572717010974884\n",
            "Epoch [55/70], LR: [0.4]\n",
            "Loss: 0.08330585062503815\n",
            "Epoch [56/70], LR: [0.4]\n",
            "Loss: 0.08418099582195282\n",
            "Epoch [57/70], LR: [0.4]\n",
            "Loss: 0.08793466538190842\n",
            "Epoch [58/70], LR: [0.4]\n",
            "Loss: 0.08383851498365402\n",
            "Epoch [59/70], LR: [0.4]\n",
            "Loss: 0.0890178233385086\n",
            "Epoch [60/70], LR: [0.4]\n",
            "Loss: 0.0806817039847374\n",
            "Epoch [61/70], LR: [0.4]\n",
            "Loss: 0.08359456807374954\n",
            "Epoch [62/70], LR: [0.4]\n",
            "Loss: 0.08315478265285492\n",
            "Epoch [63/70], LR: [0.4]\n",
            "Loss: 0.08424203097820282\n",
            "Epoch [64/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08005884289741516\n",
            "Epoch [65/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08163861185312271\n",
            "Epoch [66/70], LR: [0.08000000000000002]\n",
            "Loss: 0.0811973437666893\n",
            "Epoch [67/70], LR: [0.08000000000000002]\n",
            "Loss: 0.0789758637547493\n",
            "Epoch [68/70], LR: [0.08000000000000002]\n",
            "Loss: 0.07829459011554718\n",
            "Epoch [69/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08301884680986404\n",
            "Epoch [70/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08585590124130249\n",
            "num exemplar sets: 60\n",
            "Exemplar means len: 60\n",
            "Reducing exemplars of class 0 to 28\n",
            "Reducing exemplars of class 1 to 28\n",
            "Reducing exemplars of class 2 to 28\n",
            "Reducing exemplars of class 3 to 28\n",
            "Reducing exemplars of class 4 to 28\n",
            "Reducing exemplars of class 5 to 28\n",
            "Reducing exemplars of class 6 to 28\n",
            "Reducing exemplars of class 7 to 28\n",
            "Reducing exemplars of class 8 to 28\n",
            "Reducing exemplars of class 9 to 28\n",
            "Reducing exemplars of class 10 to 28\n",
            "Reducing exemplars of class 11 to 28\n",
            "Reducing exemplars of class 12 to 28\n",
            "Reducing exemplars of class 13 to 28\n",
            "Reducing exemplars of class 14 to 28\n",
            "Reducing exemplars of class 15 to 28\n",
            "Reducing exemplars of class 16 to 28\n",
            "Reducing exemplars of class 17 to 28\n",
            "Reducing exemplars of class 18 to 28\n",
            "Reducing exemplars of class 19 to 28\n",
            "Reducing exemplars of class 20 to 28\n",
            "Reducing exemplars of class 21 to 28\n",
            "Reducing exemplars of class 22 to 28\n",
            "Reducing exemplars of class 23 to 28\n",
            "Reducing exemplars of class 24 to 28\n",
            "Reducing exemplars of class 25 to 28\n",
            "Reducing exemplars of class 26 to 28\n",
            "Reducing exemplars of class 27 to 28\n",
            "Reducing exemplars of class 28 to 28\n",
            "Reducing exemplars of class 29 to 28\n",
            "Reducing exemplars of class 30 to 28\n",
            "Reducing exemplars of class 31 to 28\n",
            "Reducing exemplars of class 32 to 28\n",
            "Reducing exemplars of class 33 to 28\n",
            "Reducing exemplars of class 34 to 28\n",
            "Reducing exemplars of class 35 to 28\n",
            "Reducing exemplars of class 36 to 28\n",
            "Reducing exemplars of class 37 to 28\n",
            "Reducing exemplars of class 38 to 28\n",
            "Reducing exemplars of class 39 to 28\n",
            "Reducing exemplars of class 40 to 28\n",
            "Reducing exemplars of class 41 to 28\n",
            "Reducing exemplars of class 42 to 28\n",
            "Reducing exemplars of class 43 to 28\n",
            "Reducing exemplars of class 44 to 28\n",
            "Reducing exemplars of class 45 to 28\n",
            "Reducing exemplars of class 46 to 28\n",
            "Reducing exemplars of class 47 to 28\n",
            "Reducing exemplars of class 48 to 28\n",
            "Reducing exemplars of class 49 to 28\n",
            "Reducing exemplars of class 50 to 28\n",
            "Reducing exemplars of class 51 to 28\n",
            "Reducing exemplars of class 52 to 28\n",
            "Reducing exemplars of class 53 to 28\n",
            "Reducing exemplars of class 54 to 28\n",
            "Reducing exemplars of class 55 to 28\n",
            "Reducing exemplars of class 56 to 28\n",
            "Reducing exemplars of class 57 to 28\n",
            "Reducing exemplars of class 58 to 28\n",
            "Reducing exemplars of class 59 to 28\n",
            "Constructing exemplar for class [60]\n",
            "Created exemplar set for class 60 of len 28\n",
            "Constructing exemplar for class [61]\n",
            "Created exemplar set for class 61 of len 28\n",
            "Constructing exemplar for class [62]\n",
            "Created exemplar set for class 62 of len 28\n",
            "Constructing exemplar for class [63]\n",
            "Created exemplar set for class 63 of len 28\n",
            "Constructing exemplar for class [64]\n",
            "Created exemplar set for class 64 of len 28\n",
            "Constructing exemplar for class [65]\n",
            "Created exemplar set for class 65 of len 28\n",
            "Constructing exemplar for class [66]\n",
            "Created exemplar set for class 66 of len 28\n",
            "Constructing exemplar for class [67]\n",
            "Created exemplar set for class 67 of len 28\n",
            "Constructing exemplar for class [68]\n",
            "Created exemplar set for class 68 of len 28\n",
            "Constructing exemplar for class [69]\n",
            "Created exemplar set for class 69 of len 28\n",
            "Validating classes ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Validating classes ['bicycle', 'plain', 'couch', 'lobster', 'lion', 'chair', 'tulip', 'television', 'skunk', 'girl'] -> [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Validating classes ['bowl', 'tank', 'lawn_mower', 'snake', 'ray', 'oak_tree', 'poppy', 'castle', 'telephone', 'clock'] -> [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "Validating classes ['worm', 'rabbit', 'tractor', 'cockroach', 'house', 'lamp', 'sweet_pepper', 'crab', 'beetle', 'dolphin'] -> [30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
            "Validating classes ['mouse', 'flatfish', 'pear', 'lizard', 'shark', 'orchid', 'cup', 'bus', 'sunflower', 'dinosaur'] -> [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
            "Validating classes ['whale', 'wolf', 'woman', 'cloud', 'porcupine', 'road', 'plate', 'table', 'sea', 'seal'] -> [50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
            "Validating classes ['bear', 'apple', 'forest', 'streetcar', 'can', 'bed', 'crocodile', 'keyboard', 'boy', 'raccoon'] -> [60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
            "Accuracy: 0.5254285714285715\n",
            "BATCH [7]\n",
            "Training on ['willow_tree', 'maple_tree', 'orange', 'rocket', 'spider', 'chimpanzee', 'cattle', 'kangaroo', 'bridge', 'fox'] -> [70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
            "Known classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
            "Epoch [1/70], LR: [2.0]\n",
            "Loss: 0.11643677204847336\n",
            "Epoch [2/70], LR: [2.0]\n",
            "Loss: 0.09698188304901123\n",
            "Epoch [3/70], LR: [2.0]\n",
            "Loss: 0.09625563770532608\n",
            "Epoch [4/70], LR: [2.0]\n",
            "Loss: 0.0925481915473938\n",
            "Epoch [5/70], LR: [2.0]\n",
            "Loss: 0.09186818450689316\n",
            "Epoch [6/70], LR: [2.0]\n",
            "Loss: 0.10286816954612732\n",
            "Epoch [7/70], LR: [2.0]\n",
            "Loss: 0.08734879642724991\n",
            "Epoch [8/70], LR: [2.0]\n",
            "Loss: 0.09994827955961227\n",
            "Epoch [9/70], LR: [2.0]\n",
            "Loss: 0.10368238389492035\n",
            "Epoch [10/70], LR: [2.0]\n",
            "Loss: 0.09083497524261475\n",
            "Epoch [11/70], LR: [2.0]\n",
            "Loss: 0.09616915881633759\n",
            "Epoch [12/70], LR: [2.0]\n",
            "Loss: 0.09445777535438538\n",
            "Epoch [13/70], LR: [2.0]\n",
            "Loss: 0.09191562235355377\n",
            "Epoch [14/70], LR: [2.0]\n",
            "Loss: 0.09277249872684479\n",
            "Epoch [15/70], LR: [2.0]\n",
            "Loss: 0.08963131904602051\n",
            "Epoch [16/70], LR: [2.0]\n",
            "Loss: 0.09309840947389603\n",
            "Epoch [17/70], LR: [2.0]\n",
            "Loss: 0.08923737704753876\n",
            "Epoch [18/70], LR: [2.0]\n",
            "Loss: 0.0878479853272438\n",
            "Epoch [19/70], LR: [2.0]\n",
            "Loss: 0.09689819812774658\n",
            "Epoch [20/70], LR: [2.0]\n",
            "Loss: 0.09063498675823212\n",
            "Epoch [21/70], LR: [2.0]\n",
            "Loss: 0.092951700091362\n",
            "Epoch [22/70], LR: [2.0]\n",
            "Loss: 0.09140339493751526\n",
            "Epoch [23/70], LR: [2.0]\n",
            "Loss: 0.09290225058794022\n",
            "Epoch [24/70], LR: [2.0]\n",
            "Loss: 0.08481854945421219\n",
            "Epoch [25/70], LR: [2.0]\n",
            "Loss: 0.08934730291366577\n",
            "Epoch [26/70], LR: [2.0]\n",
            "Loss: 0.08577307313680649\n",
            "Epoch [27/70], LR: [2.0]\n",
            "Loss: 0.09063508361577988\n",
            "Epoch [28/70], LR: [2.0]\n",
            "Loss: 0.09158255159854889\n",
            "Epoch [29/70], LR: [2.0]\n",
            "Loss: 0.08680368214845657\n",
            "Epoch [30/70], LR: [2.0]\n",
            "Loss: 0.0811850056052208\n",
            "Epoch [31/70], LR: [2.0]\n",
            "Loss: 0.09881249815225601\n",
            "Epoch [32/70], LR: [2.0]\n",
            "Loss: 0.08579504489898682\n",
            "Epoch [33/70], LR: [2.0]\n",
            "Loss: 0.10133658349514008\n",
            "Epoch [34/70], LR: [2.0]\n",
            "Loss: 0.08749902248382568\n",
            "Epoch [35/70], LR: [2.0]\n",
            "Loss: 0.08981055021286011\n",
            "Epoch [36/70], LR: [2.0]\n",
            "Loss: 0.08630545437335968\n",
            "Epoch [37/70], LR: [2.0]\n",
            "Loss: 0.08813979476690292\n",
            "Epoch [38/70], LR: [2.0]\n",
            "Loss: 0.0908198133111\n",
            "Epoch [39/70], LR: [2.0]\n",
            "Loss: 0.09164448082447052\n",
            "Epoch [40/70], LR: [2.0]\n",
            "Loss: 0.08188103139400482\n",
            "Epoch [41/70], LR: [2.0]\n",
            "Loss: 0.0803559347987175\n",
            "Epoch [42/70], LR: [2.0]\n",
            "Loss: 0.09286048263311386\n",
            "Epoch [43/70], LR: [2.0]\n",
            "Loss: 0.08491441607475281\n",
            "Epoch [44/70], LR: [2.0]\n",
            "Loss: 0.09385726600885391\n",
            "Epoch [45/70], LR: [2.0]\n",
            "Loss: 0.09491116553544998\n",
            "Epoch [46/70], LR: [2.0]\n",
            "Loss: 0.0919414758682251\n",
            "Epoch [47/70], LR: [2.0]\n",
            "Loss: 0.08401229232549667\n",
            "Epoch [48/70], LR: [0.4]\n",
            "Loss: 0.08717749267816544\n",
            "Epoch [49/70], LR: [0.4]\n",
            "Loss: 0.07859877496957779\n",
            "Epoch [50/70], LR: [0.4]\n",
            "Loss: 0.08071234822273254\n",
            "Epoch [51/70], LR: [0.4]\n",
            "Loss: 0.0799999013543129\n",
            "Epoch [52/70], LR: [0.4]\n",
            "Loss: 0.07964209467172623\n",
            "Epoch [53/70], LR: [0.4]\n",
            "Loss: 0.07941512018442154\n",
            "Epoch [54/70], LR: [0.4]\n",
            "Loss: 0.0829215869307518\n",
            "Epoch [55/70], LR: [0.4]\n",
            "Loss: 0.0828254371881485\n",
            "Epoch [56/70], LR: [0.4]\n",
            "Loss: 0.07875636965036392\n",
            "Epoch [57/70], LR: [0.4]\n",
            "Loss: 0.08117425441741943\n",
            "Epoch [58/70], LR: [0.4]\n",
            "Loss: 0.07825952023267746\n",
            "Epoch [59/70], LR: [0.4]\n",
            "Loss: 0.0834391862154007\n",
            "Epoch [60/70], LR: [0.4]\n",
            "Loss: 0.08452559262514114\n",
            "Epoch [61/70], LR: [0.4]\n",
            "Loss: 0.0783407986164093\n",
            "Epoch [62/70], LR: [0.4]\n",
            "Loss: 0.0797976404428482\n",
            "Epoch [63/70], LR: [0.4]\n",
            "Loss: 0.07421490550041199\n",
            "Epoch [64/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08406868577003479\n",
            "Epoch [65/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08658447861671448\n",
            "Epoch [66/70], LR: [0.08000000000000002]\n",
            "Loss: 0.0765017420053482\n",
            "Epoch [67/70], LR: [0.08000000000000002]\n",
            "Loss: 0.0833502858877182\n",
            "Epoch [68/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08047949522733688\n",
            "Epoch [69/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08377102762460709\n",
            "Epoch [70/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08300384134054184\n",
            "num exemplar sets: 70\n",
            "Exemplar means len: 70\n",
            "Reducing exemplars of class 0 to 25\n",
            "Reducing exemplars of class 1 to 25\n",
            "Reducing exemplars of class 2 to 25\n",
            "Reducing exemplars of class 3 to 25\n",
            "Reducing exemplars of class 4 to 25\n",
            "Reducing exemplars of class 5 to 25\n",
            "Reducing exemplars of class 6 to 25\n",
            "Reducing exemplars of class 7 to 25\n",
            "Reducing exemplars of class 8 to 25\n",
            "Reducing exemplars of class 9 to 25\n",
            "Reducing exemplars of class 10 to 25\n",
            "Reducing exemplars of class 11 to 25\n",
            "Reducing exemplars of class 12 to 25\n",
            "Reducing exemplars of class 13 to 25\n",
            "Reducing exemplars of class 14 to 25\n",
            "Reducing exemplars of class 15 to 25\n",
            "Reducing exemplars of class 16 to 25\n",
            "Reducing exemplars of class 17 to 25\n",
            "Reducing exemplars of class 18 to 25\n",
            "Reducing exemplars of class 19 to 25\n",
            "Reducing exemplars of class 20 to 25\n",
            "Reducing exemplars of class 21 to 25\n",
            "Reducing exemplars of class 22 to 25\n",
            "Reducing exemplars of class 23 to 25\n",
            "Reducing exemplars of class 24 to 25\n",
            "Reducing exemplars of class 25 to 25\n",
            "Reducing exemplars of class 26 to 25\n",
            "Reducing exemplars of class 27 to 25\n",
            "Reducing exemplars of class 28 to 25\n",
            "Reducing exemplars of class 29 to 25\n",
            "Reducing exemplars of class 30 to 25\n",
            "Reducing exemplars of class 31 to 25\n",
            "Reducing exemplars of class 32 to 25\n",
            "Reducing exemplars of class 33 to 25\n",
            "Reducing exemplars of class 34 to 25\n",
            "Reducing exemplars of class 35 to 25\n",
            "Reducing exemplars of class 36 to 25\n",
            "Reducing exemplars of class 37 to 25\n",
            "Reducing exemplars of class 38 to 25\n",
            "Reducing exemplars of class 39 to 25\n",
            "Reducing exemplars of class 40 to 25\n",
            "Reducing exemplars of class 41 to 25\n",
            "Reducing exemplars of class 42 to 25\n",
            "Reducing exemplars of class 43 to 25\n",
            "Reducing exemplars of class 44 to 25\n",
            "Reducing exemplars of class 45 to 25\n",
            "Reducing exemplars of class 46 to 25\n",
            "Reducing exemplars of class 47 to 25\n",
            "Reducing exemplars of class 48 to 25\n",
            "Reducing exemplars of class 49 to 25\n",
            "Reducing exemplars of class 50 to 25\n",
            "Reducing exemplars of class 51 to 25\n",
            "Reducing exemplars of class 52 to 25\n",
            "Reducing exemplars of class 53 to 25\n",
            "Reducing exemplars of class 54 to 25\n",
            "Reducing exemplars of class 55 to 25\n",
            "Reducing exemplars of class 56 to 25\n",
            "Reducing exemplars of class 57 to 25\n",
            "Reducing exemplars of class 58 to 25\n",
            "Reducing exemplars of class 59 to 25\n",
            "Reducing exemplars of class 60 to 25\n",
            "Reducing exemplars of class 61 to 25\n",
            "Reducing exemplars of class 62 to 25\n",
            "Reducing exemplars of class 63 to 25\n",
            "Reducing exemplars of class 64 to 25\n",
            "Reducing exemplars of class 65 to 25\n",
            "Reducing exemplars of class 66 to 25\n",
            "Reducing exemplars of class 67 to 25\n",
            "Reducing exemplars of class 68 to 25\n",
            "Reducing exemplars of class 69 to 25\n",
            "Constructing exemplar for class [70]\n",
            "Created exemplar set for class 70 of len 25\n",
            "Constructing exemplar for class [71]\n",
            "Created exemplar set for class 71 of len 25\n",
            "Constructing exemplar for class [72]\n",
            "Created exemplar set for class 72 of len 25\n",
            "Constructing exemplar for class [73]\n",
            "Created exemplar set for class 73 of len 25\n",
            "Constructing exemplar for class [74]\n",
            "Created exemplar set for class 74 of len 25\n",
            "Constructing exemplar for class [75]\n",
            "Created exemplar set for class 75 of len 25\n",
            "Constructing exemplar for class [76]\n",
            "Created exemplar set for class 76 of len 25\n",
            "Constructing exemplar for class [77]\n",
            "Created exemplar set for class 77 of len 25\n",
            "Constructing exemplar for class [78]\n",
            "Created exemplar set for class 78 of len 25\n",
            "Constructing exemplar for class [79]\n",
            "Created exemplar set for class 79 of len 25\n",
            "Validating classes ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Validating classes ['bicycle', 'plain', 'couch', 'lobster', 'lion', 'chair', 'tulip', 'television', 'skunk', 'girl'] -> [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Validating classes ['bowl', 'tank', 'lawn_mower', 'snake', 'ray', 'oak_tree', 'poppy', 'castle', 'telephone', 'clock'] -> [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "Validating classes ['worm', 'rabbit', 'tractor', 'cockroach', 'house', 'lamp', 'sweet_pepper', 'crab', 'beetle', 'dolphin'] -> [30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
            "Validating classes ['mouse', 'flatfish', 'pear', 'lizard', 'shark', 'orchid', 'cup', 'bus', 'sunflower', 'dinosaur'] -> [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
            "Validating classes ['whale', 'wolf', 'woman', 'cloud', 'porcupine', 'road', 'plate', 'table', 'sea', 'seal'] -> [50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
            "Validating classes ['bear', 'apple', 'forest', 'streetcar', 'can', 'bed', 'crocodile', 'keyboard', 'boy', 'raccoon'] -> [60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
            "Validating classes ['willow_tree', 'maple_tree', 'orange', 'rocket', 'spider', 'chimpanzee', 'cattle', 'kangaroo', 'bridge', 'fox'] -> [70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
            "Accuracy: 0.48975\n",
            "BATCH [8]\n",
            "Training on ['butterfly', 'baby', 'elephant', 'shrew', 'pine_tree', 'squirrel', 'mountain', 'caterpillar', 'bee', 'camel'] -> [80, 81, 82, 83, 84, 85, 86, 87, 88, 89]\n",
            "Known classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]\n",
            "Epoch [1/70], LR: [2.0]\n",
            "Loss: 0.10811387002468109\n",
            "Epoch [2/70], LR: [2.0]\n",
            "Loss: 0.09678105264902115\n",
            "Epoch [3/70], LR: [2.0]\n",
            "Loss: 0.10024087131023407\n",
            "Epoch [4/70], LR: [2.0]\n",
            "Loss: 0.09490705281496048\n",
            "Epoch [5/70], LR: [2.0]\n",
            "Loss: 0.09835486114025116\n",
            "Epoch [6/70], LR: [2.0]\n",
            "Loss: 0.09475009143352509\n",
            "Epoch [7/70], LR: [2.0]\n",
            "Loss: 0.09230507165193558\n",
            "Epoch [8/70], LR: [2.0]\n",
            "Loss: 0.09458447247743607\n",
            "Epoch [9/70], LR: [2.0]\n",
            "Loss: 0.0970037505030632\n",
            "Epoch [10/70], LR: [2.0]\n",
            "Loss: 0.09556050598621368\n",
            "Epoch [11/70], LR: [2.0]\n",
            "Loss: 0.09337067604064941\n",
            "Epoch [12/70], LR: [2.0]\n",
            "Loss: 0.09331608563661575\n",
            "Epoch [13/70], LR: [2.0]\n",
            "Loss: 0.09336984157562256\n",
            "Epoch [14/70], LR: [2.0]\n",
            "Loss: 0.09698443114757538\n",
            "Epoch [15/70], LR: [2.0]\n",
            "Loss: 0.09203322976827621\n",
            "Epoch [16/70], LR: [2.0]\n",
            "Loss: 0.09742231667041779\n",
            "Epoch [17/70], LR: [2.0]\n",
            "Loss: 0.09545129537582397\n",
            "Epoch [18/70], LR: [2.0]\n",
            "Loss: 0.09357982873916626\n",
            "Epoch [19/70], LR: [2.0]\n",
            "Loss: 0.09220383316278458\n",
            "Epoch [20/70], LR: [2.0]\n",
            "Loss: 0.09393160045146942\n",
            "Epoch [21/70], LR: [2.0]\n",
            "Loss: 0.0945729911327362\n",
            "Epoch [22/70], LR: [2.0]\n",
            "Loss: 0.08886609971523285\n",
            "Epoch [23/70], LR: [2.0]\n",
            "Loss: 0.08793143928050995\n",
            "Epoch [24/70], LR: [2.0]\n",
            "Loss: 0.09391948580741882\n",
            "Epoch [25/70], LR: [2.0]\n",
            "Loss: 0.0939548909664154\n",
            "Epoch [26/70], LR: [2.0]\n",
            "Loss: 0.09157457947731018\n",
            "Epoch [27/70], LR: [2.0]\n",
            "Loss: 0.0879443883895874\n",
            "Epoch [28/70], LR: [2.0]\n",
            "Loss: 0.09099037200212479\n",
            "Epoch [29/70], LR: [2.0]\n",
            "Loss: 0.09496890753507614\n",
            "Epoch [30/70], LR: [2.0]\n",
            "Loss: 0.08945142477750778\n",
            "Epoch [31/70], LR: [2.0]\n",
            "Loss: 0.09063257277011871\n",
            "Epoch [32/70], LR: [2.0]\n",
            "Loss: 0.0893339291214943\n",
            "Epoch [33/70], LR: [2.0]\n",
            "Loss: 0.08942189067602158\n",
            "Epoch [34/70], LR: [2.0]\n",
            "Loss: 0.09229352325201035\n",
            "Epoch [35/70], LR: [2.0]\n",
            "Loss: 0.08950107544660568\n",
            "Epoch [36/70], LR: [2.0]\n",
            "Loss: 0.09148341417312622\n",
            "Epoch [37/70], LR: [2.0]\n",
            "Loss: 0.09056217968463898\n",
            "Epoch [38/70], LR: [2.0]\n",
            "Loss: 0.08627518266439438\n",
            "Epoch [39/70], LR: [2.0]\n",
            "Loss: 0.08675401657819748\n",
            "Epoch [40/70], LR: [2.0]\n",
            "Loss: 0.09337559342384338\n",
            "Epoch [41/70], LR: [2.0]\n",
            "Loss: 0.09193525463342667\n",
            "Epoch [42/70], LR: [2.0]\n",
            "Loss: 0.08427011966705322\n",
            "Epoch [43/70], LR: [2.0]\n",
            "Loss: 0.0886448323726654\n",
            "Epoch [44/70], LR: [2.0]\n",
            "Loss: 0.092044398188591\n",
            "Epoch [45/70], LR: [2.0]\n",
            "Loss: 0.0924496129155159\n",
            "Epoch [46/70], LR: [2.0]\n",
            "Loss: 0.09060300886631012\n",
            "Epoch [47/70], LR: [2.0]\n",
            "Loss: 0.09516730159521103\n",
            "Epoch [48/70], LR: [0.4]\n",
            "Loss: 0.08815792948007584\n",
            "Epoch [49/70], LR: [0.4]\n",
            "Loss: 0.08837273716926575\n",
            "Epoch [50/70], LR: [0.4]\n",
            "Loss: 0.08505351096391678\n",
            "Epoch [51/70], LR: [0.4]\n",
            "Loss: 0.0820460170507431\n",
            "Epoch [52/70], LR: [0.4]\n",
            "Loss: 0.09118897467851639\n",
            "Epoch [53/70], LR: [0.4]\n",
            "Loss: 0.08535848557949066\n",
            "Epoch [54/70], LR: [0.4]\n",
            "Loss: 0.0831802561879158\n",
            "Epoch [55/70], LR: [0.4]\n",
            "Loss: 0.0832122191786766\n",
            "Epoch [56/70], LR: [0.4]\n",
            "Loss: 0.08430769294500351\n",
            "Epoch [57/70], LR: [0.4]\n",
            "Loss: 0.08480671048164368\n",
            "Epoch [58/70], LR: [0.4]\n",
            "Loss: 0.08518374711275101\n",
            "Epoch [59/70], LR: [0.4]\n",
            "Loss: 0.0876968577504158\n",
            "Epoch [60/70], LR: [0.4]\n",
            "Loss: 0.08481638133525848\n",
            "Epoch [61/70], LR: [0.4]\n",
            "Loss: 0.08794067054986954\n",
            "Epoch [62/70], LR: [0.4]\n",
            "Loss: 0.08740536868572235\n",
            "Epoch [63/70], LR: [0.4]\n",
            "Loss: 0.07887056469917297\n",
            "Epoch [64/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08247750252485275\n",
            "Epoch [65/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08329252153635025\n",
            "Epoch [66/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08566472679376602\n",
            "Epoch [67/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08389370143413544\n",
            "Epoch [68/70], LR: [0.08000000000000002]\n",
            "Loss: 0.07872162759304047\n",
            "Epoch [69/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08587782829999924\n",
            "Epoch [70/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08290018886327744\n",
            "num exemplar sets: 80\n",
            "Exemplar means len: 80\n",
            "Reducing exemplars of class 0 to 22\n",
            "Reducing exemplars of class 1 to 22\n",
            "Reducing exemplars of class 2 to 22\n",
            "Reducing exemplars of class 3 to 22\n",
            "Reducing exemplars of class 4 to 22\n",
            "Reducing exemplars of class 5 to 22\n",
            "Reducing exemplars of class 6 to 22\n",
            "Reducing exemplars of class 7 to 22\n",
            "Reducing exemplars of class 8 to 22\n",
            "Reducing exemplars of class 9 to 22\n",
            "Reducing exemplars of class 10 to 22\n",
            "Reducing exemplars of class 11 to 22\n",
            "Reducing exemplars of class 12 to 22\n",
            "Reducing exemplars of class 13 to 22\n",
            "Reducing exemplars of class 14 to 22\n",
            "Reducing exemplars of class 15 to 22\n",
            "Reducing exemplars of class 16 to 22\n",
            "Reducing exemplars of class 17 to 22\n",
            "Reducing exemplars of class 18 to 22\n",
            "Reducing exemplars of class 19 to 22\n",
            "Reducing exemplars of class 20 to 22\n",
            "Reducing exemplars of class 21 to 22\n",
            "Reducing exemplars of class 22 to 22\n",
            "Reducing exemplars of class 23 to 22\n",
            "Reducing exemplars of class 24 to 22\n",
            "Reducing exemplars of class 25 to 22\n",
            "Reducing exemplars of class 26 to 22\n",
            "Reducing exemplars of class 27 to 22\n",
            "Reducing exemplars of class 28 to 22\n",
            "Reducing exemplars of class 29 to 22\n",
            "Reducing exemplars of class 30 to 22\n",
            "Reducing exemplars of class 31 to 22\n",
            "Reducing exemplars of class 32 to 22\n",
            "Reducing exemplars of class 33 to 22\n",
            "Reducing exemplars of class 34 to 22\n",
            "Reducing exemplars of class 35 to 22\n",
            "Reducing exemplars of class 36 to 22\n",
            "Reducing exemplars of class 37 to 22\n",
            "Reducing exemplars of class 38 to 22\n",
            "Reducing exemplars of class 39 to 22\n",
            "Reducing exemplars of class 40 to 22\n",
            "Reducing exemplars of class 41 to 22\n",
            "Reducing exemplars of class 42 to 22\n",
            "Reducing exemplars of class 43 to 22\n",
            "Reducing exemplars of class 44 to 22\n",
            "Reducing exemplars of class 45 to 22\n",
            "Reducing exemplars of class 46 to 22\n",
            "Reducing exemplars of class 47 to 22\n",
            "Reducing exemplars of class 48 to 22\n",
            "Reducing exemplars of class 49 to 22\n",
            "Reducing exemplars of class 50 to 22\n",
            "Reducing exemplars of class 51 to 22\n",
            "Reducing exemplars of class 52 to 22\n",
            "Reducing exemplars of class 53 to 22\n",
            "Reducing exemplars of class 54 to 22\n",
            "Reducing exemplars of class 55 to 22\n",
            "Reducing exemplars of class 56 to 22\n",
            "Reducing exemplars of class 57 to 22\n",
            "Reducing exemplars of class 58 to 22\n",
            "Reducing exemplars of class 59 to 22\n",
            "Reducing exemplars of class 60 to 22\n",
            "Reducing exemplars of class 61 to 22\n",
            "Reducing exemplars of class 62 to 22\n",
            "Reducing exemplars of class 63 to 22\n",
            "Reducing exemplars of class 64 to 22\n",
            "Reducing exemplars of class 65 to 22\n",
            "Reducing exemplars of class 66 to 22\n",
            "Reducing exemplars of class 67 to 22\n",
            "Reducing exemplars of class 68 to 22\n",
            "Reducing exemplars of class 69 to 22\n",
            "Reducing exemplars of class 70 to 22\n",
            "Reducing exemplars of class 71 to 22\n",
            "Reducing exemplars of class 72 to 22\n",
            "Reducing exemplars of class 73 to 22\n",
            "Reducing exemplars of class 74 to 22\n",
            "Reducing exemplars of class 75 to 22\n",
            "Reducing exemplars of class 76 to 22\n",
            "Reducing exemplars of class 77 to 22\n",
            "Reducing exemplars of class 78 to 22\n",
            "Reducing exemplars of class 79 to 22\n",
            "Constructing exemplar for class [80]\n",
            "Created exemplar set for class 80 of len 22\n",
            "Constructing exemplar for class [81]\n",
            "Created exemplar set for class 81 of len 22\n",
            "Constructing exemplar for class [82]\n",
            "Created exemplar set for class 82 of len 22\n",
            "Constructing exemplar for class [83]\n",
            "Created exemplar set for class 83 of len 22\n",
            "Constructing exemplar for class [84]\n",
            "Created exemplar set for class 84 of len 22\n",
            "Constructing exemplar for class [85]\n",
            "Created exemplar set for class 85 of len 22\n",
            "Constructing exemplar for class [86]\n",
            "Created exemplar set for class 86 of len 22\n",
            "Constructing exemplar for class [87]\n",
            "Created exemplar set for class 87 of len 22\n",
            "Constructing exemplar for class [88]\n",
            "Created exemplar set for class 88 of len 22\n",
            "Constructing exemplar for class [89]\n",
            "Created exemplar set for class 89 of len 22\n",
            "Validating classes ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Validating classes ['bicycle', 'plain', 'couch', 'lobster', 'lion', 'chair', 'tulip', 'television', 'skunk', 'girl'] -> [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Validating classes ['bowl', 'tank', 'lawn_mower', 'snake', 'ray', 'oak_tree', 'poppy', 'castle', 'telephone', 'clock'] -> [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "Validating classes ['worm', 'rabbit', 'tractor', 'cockroach', 'house', 'lamp', 'sweet_pepper', 'crab', 'beetle', 'dolphin'] -> [30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
            "Validating classes ['mouse', 'flatfish', 'pear', 'lizard', 'shark', 'orchid', 'cup', 'bus', 'sunflower', 'dinosaur'] -> [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
            "Validating classes ['whale', 'wolf', 'woman', 'cloud', 'porcupine', 'road', 'plate', 'table', 'sea', 'seal'] -> [50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
            "Validating classes ['bear', 'apple', 'forest', 'streetcar', 'can', 'bed', 'crocodile', 'keyboard', 'boy', 'raccoon'] -> [60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
            "Validating classes ['willow_tree', 'maple_tree', 'orange', 'rocket', 'spider', 'chimpanzee', 'cattle', 'kangaroo', 'bridge', 'fox'] -> [70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
            "Validating classes ['butterfly', 'baby', 'elephant', 'shrew', 'pine_tree', 'squirrel', 'mountain', 'caterpillar', 'bee', 'camel'] -> [80, 81, 82, 83, 84, 85, 86, 87, 88, 89]\n",
            "Accuracy: 0.4583333333333333\n",
            "BATCH [9]\n",
            "Training on ['leopard', 'trout', 'turtle', 'rose', 'aquarium_fish', 'possum', 'hamster', 'otter', 'motorcycle', 'pickup_truck'] -> [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
            "Known classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
            "Epoch [1/70], LR: [2.0]\n",
            "Loss: 0.10410110652446747\n",
            "Epoch [2/70], LR: [2.0]\n",
            "Loss: 0.10171711444854736\n",
            "Epoch [3/70], LR: [2.0]\n",
            "Loss: 0.10302776098251343\n",
            "Epoch [4/70], LR: [2.0]\n",
            "Loss: 0.09877970814704895\n",
            "Epoch [5/70], LR: [2.0]\n",
            "Loss: 0.09405763447284698\n",
            "Epoch [6/70], LR: [2.0]\n",
            "Loss: 0.0915413573384285\n",
            "Epoch [7/70], LR: [2.0]\n",
            "Loss: 0.09517303854227066\n",
            "Epoch [8/70], LR: [2.0]\n",
            "Loss: 0.09326709806919098\n",
            "Epoch [9/70], LR: [2.0]\n",
            "Loss: 0.09250931441783905\n",
            "Epoch [10/70], LR: [2.0]\n",
            "Loss: 0.09721735119819641\n",
            "Epoch [11/70], LR: [2.0]\n",
            "Loss: 0.098822221159935\n",
            "Epoch [12/70], LR: [2.0]\n",
            "Loss: 0.09620959311723709\n",
            "Epoch [13/70], LR: [2.0]\n",
            "Loss: 0.09197979420423508\n",
            "Epoch [14/70], LR: [2.0]\n",
            "Loss: 0.09297943115234375\n",
            "Epoch [15/70], LR: [2.0]\n",
            "Loss: 0.09000780433416367\n",
            "Epoch [16/70], LR: [2.0]\n",
            "Loss: 0.10092764347791672\n",
            "Epoch [17/70], LR: [2.0]\n",
            "Loss: 0.09242807328701019\n",
            "Epoch [18/70], LR: [2.0]\n",
            "Loss: 0.09254764020442963\n",
            "Epoch [19/70], LR: [2.0]\n",
            "Loss: 0.0966770276427269\n",
            "Epoch [20/70], LR: [2.0]\n",
            "Loss: 0.09566718339920044\n",
            "Epoch [21/70], LR: [2.0]\n",
            "Loss: 0.09238090366125107\n",
            "Epoch [22/70], LR: [2.0]\n",
            "Loss: 0.09276999533176422\n",
            "Epoch [23/70], LR: [2.0]\n",
            "Loss: 0.09817449003458023\n",
            "Epoch [24/70], LR: [2.0]\n",
            "Loss: 0.09366071969270706\n",
            "Epoch [25/70], LR: [2.0]\n",
            "Loss: 0.09409547597169876\n",
            "Epoch [26/70], LR: [2.0]\n",
            "Loss: 0.09371885657310486\n",
            "Epoch [27/70], LR: [2.0]\n",
            "Loss: 0.09333513677120209\n",
            "Epoch [28/70], LR: [2.0]\n",
            "Loss: 0.09798534214496613\n",
            "Epoch [29/70], LR: [2.0]\n",
            "Loss: 0.08971555531024933\n",
            "Epoch [30/70], LR: [2.0]\n",
            "Loss: 0.09681981056928635\n",
            "Epoch [31/70], LR: [2.0]\n",
            "Loss: 0.09203005582094193\n",
            "Epoch [32/70], LR: [2.0]\n",
            "Loss: 0.09749019145965576\n",
            "Epoch [33/70], LR: [2.0]\n",
            "Loss: 0.08947732299566269\n",
            "Epoch [34/70], LR: [2.0]\n",
            "Loss: 0.1012006625533104\n",
            "Epoch [35/70], LR: [2.0]\n",
            "Loss: 0.09396591037511826\n",
            "Epoch [36/70], LR: [2.0]\n",
            "Loss: 0.09502209722995758\n",
            "Epoch [37/70], LR: [2.0]\n",
            "Loss: 0.09146204590797424\n",
            "Epoch [38/70], LR: [2.0]\n",
            "Loss: 0.09039106965065002\n",
            "Epoch [39/70], LR: [2.0]\n",
            "Loss: 0.09193721413612366\n",
            "Epoch [40/70], LR: [2.0]\n",
            "Loss: 0.09234490990638733\n",
            "Epoch [41/70], LR: [2.0]\n",
            "Loss: 0.08659791201353073\n",
            "Epoch [42/70], LR: [2.0]\n",
            "Loss: 0.09272496402263641\n",
            "Epoch [43/70], LR: [2.0]\n",
            "Loss: 0.09106969833374023\n",
            "Epoch [44/70], LR: [2.0]\n",
            "Loss: 0.08876480907201767\n",
            "Epoch [45/70], LR: [2.0]\n",
            "Loss: 0.09289249777793884\n",
            "Epoch [46/70], LR: [2.0]\n",
            "Loss: 0.09624183923006058\n",
            "Epoch [47/70], LR: [2.0]\n",
            "Loss: 0.09024110436439514\n",
            "Epoch [48/70], LR: [0.4]\n",
            "Loss: 0.08890561759471893\n",
            "Epoch [49/70], LR: [0.4]\n",
            "Loss: 0.08612146973609924\n",
            "Epoch [50/70], LR: [0.4]\n",
            "Loss: 0.09035008400678635\n",
            "Epoch [51/70], LR: [0.4]\n",
            "Loss: 0.08314356952905655\n",
            "Epoch [52/70], LR: [0.4]\n",
            "Loss: 0.08289999514818192\n",
            "Epoch [53/70], LR: [0.4]\n",
            "Loss: 0.08350497484207153\n",
            "Epoch [54/70], LR: [0.4]\n",
            "Loss: 0.08758293837308884\n",
            "Epoch [55/70], LR: [0.4]\n",
            "Loss: 0.09120500832796097\n",
            "Epoch [56/70], LR: [0.4]\n",
            "Loss: 0.08953778445720673\n",
            "Epoch [57/70], LR: [0.4]\n",
            "Loss: 0.0845889076590538\n",
            "Epoch [58/70], LR: [0.4]\n",
            "Loss: 0.0895221009850502\n",
            "Epoch [59/70], LR: [0.4]\n",
            "Loss: 0.08696407079696655\n",
            "Epoch [60/70], LR: [0.4]\n",
            "Loss: 0.0903739407658577\n",
            "Epoch [61/70], LR: [0.4]\n",
            "Loss: 0.08949464559555054\n",
            "Epoch [62/70], LR: [0.4]\n",
            "Loss: 0.08931071311235428\n",
            "Epoch [63/70], LR: [0.4]\n",
            "Loss: 0.08889037370681763\n",
            "Epoch [64/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08736519515514374\n",
            "Epoch [65/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08615853637456894\n",
            "Epoch [66/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08834117650985718\n",
            "Epoch [67/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08751510083675385\n",
            "Epoch [68/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08582627028226852\n",
            "Epoch [69/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08679046481847763\n",
            "Epoch [70/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08565226197242737\n",
            "num exemplar sets: 90\n",
            "Exemplar means len: 90\n",
            "Reducing exemplars of class 0 to 20\n",
            "Reducing exemplars of class 1 to 20\n",
            "Reducing exemplars of class 2 to 20\n",
            "Reducing exemplars of class 3 to 20\n",
            "Reducing exemplars of class 4 to 20\n",
            "Reducing exemplars of class 5 to 20\n",
            "Reducing exemplars of class 6 to 20\n",
            "Reducing exemplars of class 7 to 20\n",
            "Reducing exemplars of class 8 to 20\n",
            "Reducing exemplars of class 9 to 20\n",
            "Reducing exemplars of class 10 to 20\n",
            "Reducing exemplars of class 11 to 20\n",
            "Reducing exemplars of class 12 to 20\n",
            "Reducing exemplars of class 13 to 20\n",
            "Reducing exemplars of class 14 to 20\n",
            "Reducing exemplars of class 15 to 20\n",
            "Reducing exemplars of class 16 to 20\n",
            "Reducing exemplars of class 17 to 20\n",
            "Reducing exemplars of class 18 to 20\n",
            "Reducing exemplars of class 19 to 20\n",
            "Reducing exemplars of class 20 to 20\n",
            "Reducing exemplars of class 21 to 20\n",
            "Reducing exemplars of class 22 to 20\n",
            "Reducing exemplars of class 23 to 20\n",
            "Reducing exemplars of class 24 to 20\n",
            "Reducing exemplars of class 25 to 20\n",
            "Reducing exemplars of class 26 to 20\n",
            "Reducing exemplars of class 27 to 20\n",
            "Reducing exemplars of class 28 to 20\n",
            "Reducing exemplars of class 29 to 20\n",
            "Reducing exemplars of class 30 to 20\n",
            "Reducing exemplars of class 31 to 20\n",
            "Reducing exemplars of class 32 to 20\n",
            "Reducing exemplars of class 33 to 20\n",
            "Reducing exemplars of class 34 to 20\n",
            "Reducing exemplars of class 35 to 20\n",
            "Reducing exemplars of class 36 to 20\n",
            "Reducing exemplars of class 37 to 20\n",
            "Reducing exemplars of class 38 to 20\n",
            "Reducing exemplars of class 39 to 20\n",
            "Reducing exemplars of class 40 to 20\n",
            "Reducing exemplars of class 41 to 20\n",
            "Reducing exemplars of class 42 to 20\n",
            "Reducing exemplars of class 43 to 20\n",
            "Reducing exemplars of class 44 to 20\n",
            "Reducing exemplars of class 45 to 20\n",
            "Reducing exemplars of class 46 to 20\n",
            "Reducing exemplars of class 47 to 20\n",
            "Reducing exemplars of class 48 to 20\n",
            "Reducing exemplars of class 49 to 20\n",
            "Reducing exemplars of class 50 to 20\n",
            "Reducing exemplars of class 51 to 20\n",
            "Reducing exemplars of class 52 to 20\n",
            "Reducing exemplars of class 53 to 20\n",
            "Reducing exemplars of class 54 to 20\n",
            "Reducing exemplars of class 55 to 20\n",
            "Reducing exemplars of class 56 to 20\n",
            "Reducing exemplars of class 57 to 20\n",
            "Reducing exemplars of class 58 to 20\n",
            "Reducing exemplars of class 59 to 20\n",
            "Reducing exemplars of class 60 to 20\n",
            "Reducing exemplars of class 61 to 20\n",
            "Reducing exemplars of class 62 to 20\n",
            "Reducing exemplars of class 63 to 20\n",
            "Reducing exemplars of class 64 to 20\n",
            "Reducing exemplars of class 65 to 20\n",
            "Reducing exemplars of class 66 to 20\n",
            "Reducing exemplars of class 67 to 20\n",
            "Reducing exemplars of class 68 to 20\n",
            "Reducing exemplars of class 69 to 20\n",
            "Reducing exemplars of class 70 to 20\n",
            "Reducing exemplars of class 71 to 20\n",
            "Reducing exemplars of class 72 to 20\n",
            "Reducing exemplars of class 73 to 20\n",
            "Reducing exemplars of class 74 to 20\n",
            "Reducing exemplars of class 75 to 20\n",
            "Reducing exemplars of class 76 to 20\n",
            "Reducing exemplars of class 77 to 20\n",
            "Reducing exemplars of class 78 to 20\n",
            "Reducing exemplars of class 79 to 20\n",
            "Reducing exemplars of class 80 to 20\n",
            "Reducing exemplars of class 81 to 20\n",
            "Reducing exemplars of class 82 to 20\n",
            "Reducing exemplars of class 83 to 20\n",
            "Reducing exemplars of class 84 to 20\n",
            "Reducing exemplars of class 85 to 20\n",
            "Reducing exemplars of class 86 to 20\n",
            "Reducing exemplars of class 87 to 20\n",
            "Reducing exemplars of class 88 to 20\n",
            "Reducing exemplars of class 89 to 20\n",
            "Constructing exemplar for class [90]\n",
            "Created exemplar set for class 90 of len 20\n",
            "Constructing exemplar for class [91]\n",
            "Created exemplar set for class 91 of len 20\n",
            "Constructing exemplar for class [92]\n",
            "Created exemplar set for class 92 of len 20\n",
            "Constructing exemplar for class [93]\n",
            "Created exemplar set for class 93 of len 20\n",
            "Constructing exemplar for class [94]\n",
            "Created exemplar set for class 94 of len 20\n",
            "Constructing exemplar for class [95]\n",
            "Created exemplar set for class 95 of len 20\n",
            "Constructing exemplar for class [96]\n",
            "Created exemplar set for class 96 of len 20\n",
            "Constructing exemplar for class [97]\n",
            "Created exemplar set for class 97 of len 20\n",
            "Constructing exemplar for class [98]\n",
            "Created exemplar set for class 98 of len 20\n",
            "Constructing exemplar for class [99]\n",
            "Created exemplar set for class 99 of len 20\n",
            "Validating classes ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Validating classes ['bicycle', 'plain', 'couch', 'lobster', 'lion', 'chair', 'tulip', 'television', 'skunk', 'girl'] -> [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Validating classes ['bowl', 'tank', 'lawn_mower', 'snake', 'ray', 'oak_tree', 'poppy', 'castle', 'telephone', 'clock'] -> [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "Validating classes ['worm', 'rabbit', 'tractor', 'cockroach', 'house', 'lamp', 'sweet_pepper', 'crab', 'beetle', 'dolphin'] -> [30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
            "Validating classes ['mouse', 'flatfish', 'pear', 'lizard', 'shark', 'orchid', 'cup', 'bus', 'sunflower', 'dinosaur'] -> [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
            "Validating classes ['whale', 'wolf', 'woman', 'cloud', 'porcupine', 'road', 'plate', 'table', 'sea', 'seal'] -> [50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
            "Validating classes ['bear', 'apple', 'forest', 'streetcar', 'can', 'bed', 'crocodile', 'keyboard', 'boy', 'raccoon'] -> [60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
            "Validating classes ['willow_tree', 'maple_tree', 'orange', 'rocket', 'spider', 'chimpanzee', 'cattle', 'kangaroo', 'bridge', 'fox'] -> [70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
            "Validating classes ['butterfly', 'baby', 'elephant', 'shrew', 'pine_tree', 'squirrel', 'mountain', 'caterpillar', 'bee', 'camel'] -> [80, 81, 82, 83, 84, 85, 86, 87, 88, 89]\n",
            "Validating classes ['leopard', 'trout', 'turtle', 'rose', 'aquarium_fish', 'possum', 'hamster', 'otter', 'motorcycle', 'pickup_truck'] -> [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
            "Accuracy: 0.4333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4NhX3N5cdm0",
        "colab_type": "text"
      },
      "source": [
        "**Plot of accuracy over learned classes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9ZTpRJEcc3m",
        "colab_type": "code",
        "outputId": "ffe36795-02b2-49c1-d5ec-1e9465eed9d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure()\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Learned classes')\n",
        "plt.plot(np.arange(10, 110, 10), accuracies)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEJCAYAAACOr7BbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3dd5xU1fnH8c+zyy5LLwIL0ntVQFZ6dCkiJgoaG2gQjYoF7JpoivrDX0w0P2MDRVCisbCIBTFBjSIrvQqCgFKWLlFEQFeQ+vz+mEuYXRcZYO/Olu/79ZrXzj333plnzusyD/ecOeeYuyMiIpJbQrwDEBGRwkkJQkRE8qQEISIieVKCEBGRPClBiIhInpQgREQkT6EmCDPra2afm9lqM7s7j/31zGyqmS0ysyVm9vOgvIGZ7TazxcFjVJhxiojIj1lY4yDMLBFYCZwFbALmAwPdfXnUMaOBRe7+tJm1Aia7ewMzawD8093bhBKciIgcVakQX7sjsNrdswDMLAPoDyyPOsaBisHzSsAXx/tm1apV8wYNGhzv6YXC999/T7ly5eIdRqGh+shJ9XGY6iKnE6mPhQsXfu3u1fPaF2aCqA1sjNreBHTKdcz9wL/N7CagHNA7al9DM1sEfAv8wd2n/9SbNWjQgAULFpxw0PGUmZlJenp6vMMoNFQfOak+DlNd5HQi9WFm64+0L8wEEYuBwPPu/oiZdQFeNLM2wBagnrtvM7MOwEQza+3u30afbGZDgCEAqampZGZmFnD4+Ss7O7vIf4b8pPrISfVxmOoip7DqI8wEsRmoG7VdJyiLdjXQF8DdZ5tZClDN3b8C9gTlC81sDdAMyHGL4O6jgdEAaWlpXtT/R6H/FeWk+shJ9XGY6iKnsOojzF8xzQeamllDM0sGBgCTch2zAegFYGYtgRRgq5lVDzq5MbNGQFMgK8RYRUQkl9DuINx9v5kNA94DEoGx7r7MzIYDC9x9EnAHMMbMbiPSYX2lu7uZnQEMN7N9wEHgenf/JqxYRUTkx0Ltg3D3ycDkXGX3Rj1fDnTL47zXgdfDjE1ERH6aRlKLiEielCBERCRPJT5BHDzo/Olfy9n4za54hyIiUqiU+ASxbtv3jJ+/kfNGzGDGqq/jHY6ISKFR4hNEo+rlmTSsOzUqlOaKsXMZMy0LrdMtIqIEAUCDauV488ZunN26Jn+avIJbMhaze++BeIclIhJXShCBcqVL8dTlp3HX2c15e8kXXPj0LPVLiEiJpgQRxcwY2qMJYwefzsbtu+g3YgYzV6tfQkRKJiWIPPRoUYNJw7pTrXxpBj03l2enq19CREoeJYgjaFitHG8O7cZZrVL533+t4Lbx6pcQkZJFCeInlC9diqcv78AdZzXjrU++4KJRs9i0Xf0SIlIyKEEcRUKCcVOvpjw3OI0N23bRb8RMZq1Rv4SIFH9KEDHq2SKVt4Z1o2q5ZAY9N4+xM9aqX0JEijUliGPQqHp53ryxK71a1GD4P5dzx6uf8MM+9UuISPGkBHGMKqQkMepXHbitdzPeWLSZi0fNZvOO3fEOS0Qk3ylBHIeEBOOW3k159oo01n39Pf2enMGcrG3xDktEJF8pQZyA3q1SmTisG5XKJnH5s3N5fqb6JUSk+Ag1QZhZXzP73MxWm9ndeeyvZ2ZTzWyRmS0xs59H7bsnOO9zMzs7zDhPROPq5Zk4tBs9mtfg/reXc+eEJeqXEJFiIbQEYWaJwEjgHKAVMNDMWuU67A/Aq+7eHhgAPBWc2yrYbg30BZ4KXq9QqpiSxOhBHbi1d1Ne/3gTlzwzmy/ULyEiRVyYdxAdgdXunuXue4EMoH+uYxyoGDyvBHwRPO8PZLj7HndfC6wOXq/QSkgwbu3djNGDOpC19Xv6jZjBXPVLiEgRZmG1mZvZRUBfd78m2B4EdHL3YVHH1AL+DVQBygG93X2hmY0A5rj7S8FxzwHvuPtrud5jCDAEIDU1tUNGRkYon+VYfZF9kCc+/oGtu52BLZLpVa8UZnbU87KzsylfvnwBRFg0qD5yUn0cprrI6UTqo0ePHgvdPS2vfaVOKKoTNxB43t0fMbMuwItm1ibWk919NDAaIC0tzdPT08OJ8jic23sft2Us5qUVX7GnbA0eOL8NKUk/3UqWmZlJYfoM8ab6yEn1cZjqIqew6iPMJqbNQN2o7TpBWbSrgVcB3H02kAJUi/HcQq1iShJjrkjj5p5NmLBwE5eOnsOWneqXEJGiI8wEMR9oamYNzSyZSKfzpFzHbAB6AZhZSyIJYmtw3AAzK21mDYGmwLwQYw1FQoJxe5/mPDOoA6u//I7znpzB/HXfxDssEZGYhJYg3H0/MAx4D1hB5NdKy8xsuJn1Cw67A7jWzD4BxgFXesQyIncWy4F3gaHuXmR/O3p265pMHNqNCilJDBw9hxfnrNd4CREp9ELtg3D3ycDkXGX3Rj1fDnQ7wrl/Av4UZnwFqWlqBSYO7catGYv448RP+XTTToaf35rSpQrtr3dFpITTSOoCVKlMEs8NPp2bejZh/IKNXPrMHP6z84d4hyUikicliAKWkGDc0ac5o351Giu//I7zRsxggfolRKQQUoKIk75tajFxaDfKJScycMwcXp67Pt4hiYjkoAQRR81SK/DW0O50a1KN37/5KX//dA979hfZvngRKWaUIOKsUtlIv8TQHo35aNN+Lhszl63f7Yl3WCIiShCFQWKCcdfZLbixbWmWfbGTfiNmsHTTzniHJSIlnBJEIdKxVileu74rBlw0ahaTPvniqOeIiIRFCaKQaVO7EpNu6s6pdSpx87hFPPTuZxw4qEF1IlLwlCAKoWrlS/PyNZ0Z2LEeT2eu4dp/LOC7H/bFOywRKWGUIAqp5FIJPHhBGx7o35ppK7dywVOzWPv19/EOS0RKECWIQszMGNSlAS9e3Ylt2XvoP2IG01ZujXdYIlJCKEEUAV0an8SkYd05uXIZrvz7PJ6dnqXJ/kQkdEoQRUTdqmV5/Yau9GlVk//91wrunLCEH/ZpUJ2IhEcJoggpV7oUT11+Grf2bsrrH29iwOg5fPWtJvsTkXAoQRQxCQnGrb2b5Zjs75ONO+IdlogUQ0oQRVTfNrV4/YauJCUmcPEzs3lz0aZ4hyQixUyoCcLM+prZ52a22szuzmP/o2a2OHisNLMdUfsORO3LvVSpAC1rVWTSsO6cVq8yt43/hAcnr9CgOhHJN6GtKGdmicBI4CxgEzDfzCYFq8gB4O63RR1/E9A+6iV2u3u7sOIrLqqWS+bFqzsx/O3ljJ6Wxef/+Y4nBranUpmkeIcmIkVcmHcQHYHV7p7l7nuBDKD/Txw/kMi61HKMkhITeOD8Njx4wSnMXP01F4ycyZqt2fEOS0SKuDATRG1gY9T2pqDsR8ysPtAQ+DCqOMXMFpjZHDM7P7wwi4/LOtXjlWs7s3P3Ps4fMZOpn38V75BEpAizsAZcmdlFQF93vybYHgR0cvdheRz7W6COu98UVVbb3TebWSMiiaOXu6/Jdd4QYAhAampqh4yMjFA+S0HJzs6mfPnyJ/w6X+8+yBMf72Hjdwe5uHkS5zRIwszyIcKClV/1UVyoPg5TXeR0IvXRo0ePhe6elte+0PoggM1A3ajtOkFZXgYAQ6ML3H1z8DfLzDKJ9E+syXXMaGA0QFpamqenp+dH3HGTmZlJfn2Gn/faz10TlvDq0i3sLVOdv1x4KilJifny2gUlP+ujOFB9HKa6yCms+giziWk+0NTMGppZMpEk8KNfI5lZC6AKMDuqrIqZlQ6eVwO6ActznytHVja5FCMua8+dfZoxcfEXXPLMbP6zU4PqRCR2oSUId98PDAPeA1YAr7r7MjMbbmb9og4dAGR4zraulsACM/sEmAr8JfrXTxIbM2NYz6aMuSKNNV9lc96IGXy8YXu8wxKRIiLMJibcfTIwOVfZvbm278/jvFnAKWHGVpKc1SqVN4d249p/LGDAM3P40wVtuDit7tFPFJESTSOpS4hmqRV4a2g3Tm9YhbteW8Lwt5ez/8DBeIclIoWYEkQJUrlsMi9c1ZGrujVg7My1XPX8fHbs2hvvsESkkFKCKGFKJSZw33mtefjCU5mTtY3zR85k1ZffxTssESmElCBKqEtOr0vGkM5k7znABU/N4oPlX8Y7JBEpZJQgSrAO9avy9k3daFitHNe+uICRU1drpToR+S8liBKuVqUyTLi+C/3ansxf3/ucm8YtYvderVQnIiH/zFWKhpSkRB67tB0ta1XkoXc/Y+3X3zPmijROrlwm3qGJSBzpDkKAyKC6689szHOD09iwbRf9RszUoDqREk4JQnLo2SKVN27sSrnSiQwYPYc3PtZKdSIllRKE/EjT1ApMvLEbHepV4fZXP+Ev73zGQa1UJ1LiKEFInqqUS+YfV3fk8k71GPXRGoa8uIDsPfvjHZaIFCAlCDmipMQE/nTBKTzQvzVTP9/KhU/NYuM3u+IdlogUECUIOapBXRrwwlUd2bJzN/1GzGBu1rZ4hyQiBUAJQmLSvWk13hrWnSrlkrn82blkzNsQ75BEJGRKEBKzhtXK8eaN3ejapBp3v7GU/3l7mWaEFSnGlCDkmFQqk8TYwWlc3b0hf5+5jquen8/O3fviHZaIhEAJQo5ZqcQE/nhuKx668BTmZG3jgpEzydqaHe+wRCSfhZogzKyvmX1uZqvN7O489j9qZouDx0oz2xG1b7CZrQoeg8OMU47PpafX4+VrOrNj9z7OHzmT6au2xjskEclHoSUIM0sERgLnAK2AgWbWKvoYd7/N3du5ezvgSeCN4NyqwH1AJ6AjcJ+ZVQkrVjl+HRtW5a2h3ahVqQxX/n0+L8xapxlhRYqJMO8gOgKr3T3L3fcCGUD/nzh+IDAueH428L67f+Pu24H3gb4hxionoG7Vsrx+Y1d6NK/BfZOW8fuJn7JPndciRV6Ys7nWBjZGbW8ickfwI2ZWH2gIfPgT59bO47whwBCA1NRUMjMzTzjoeMrOzi7Sn+Gyek7KniRembuBj1dtYli7FMon23G/XlGvj/ym+jhMdZFTWPVRWKb7HgC85u7HtBCBu48GRgOkpaV5enp6CKEVnMzMTIr6Z+jZA3ov2sxvXl/Cw4vh2cEdaJZa4bheqzjUR35SfRymusgprPoIs4lpM1A3artOUJaXARxuXjrWc6WQOb99bcYP6czufQf45VOzmLJCy5mKFEVhJoj5QFMza2hmyUSSwKTcB5lZC6AKMDuq+D2gj5lVCTqn+wRlUkS0r1eFScO60aBaWa75xwKe+WiNOq9FipjQEoS77weGEfliXwG86u7LzGy4mfWLOnQAkOFR3x7u/g3wAJEkMx8YHpRJEVKrUhkmXNeVn7epxZ/f+Yw7Jyxhz34tZypSVITaB+Huk4HJucruzbV9/xHOHQuMDS04KRBlkhMZcVl7mk2pwKMfrGTt19k8MyiN6hVKxzs0ETkKjaSW0JkZt/RuylOXn8byLd/Sf8QMPt28M95hichRKEFIgfn5KbV47fquOHDxqNm8s3RLvEMSkZ+gBCEFqk3tSrw1rBstalXghpc/5okpq9R5LVJIKUFIgatRIYVx13bml+1r87f3V3LTuEXs3qvOa5HCprAMlJMSJiUpkUcuaUuzmhV46N3PWL9tF2OuSKNmpZR4hyYigaPeQZjZeWamOw3Jd2bG9Wc2ZsygNLK2ZtNvxAwWb9xx9BNFpEDE8sV/KbDKzB4OBrWJ5KverVJ548ZulE5K4JJnZvPWYg2aFykMjpog3P1XQHtgDfC8mc02syFmdnwT7IjkoXnNCrw1tDvt6lbmlozF/PW9zziozmuRuIqp6cjdvwVeIzJldy3gAuBjM7spxNikhKlaLpmXru7EgNPrMnLqGkYu3sMP+9R5LRIvsfRB9DOzN4FMIAno6O7nAG2BO8INT0qa5FIJ/PmXp/DHc1ux8MsDDB47j+9+0JrXIvEQyx3EhcCj7n6Ku//V3b8CcPddwNWhRiclkplxdfeGXHdqaRau387AMXP4OntPvMMSKXFiSRD3A/MObZhZGTNrAODuU0KJSgTocnIpxgxOY/VX2Vw8ajabtu+Kd0giJUosCWICEL1+5IGgTCR0PZrX4KWrO7Etew8XPT2bVV9+F++QREqMWBJEqWBNaQCC58nhhSSSU1qDqoy/rgsH3Ln4mdks2rA93iGJlAixJIit0es3mFl/4OvwQhL5sZa1KvLa9V2omJLE5c/OZfqqrfEOSaTYiyVBXA/8zsw2mNlG4LfAdeGGJfJj9U8qx2vXd6Fe1bL8+vn5TNZssCKhimWg3Bp37wy0Alq6e1d3Xx3Li5tZXzP73MxWm9ndRzjmEjNbbmbLzOyVqPIDZrY4ePxoqVIpmWpUTGH8kC60rVOZoa98zLh5G+IdkkixFdNkfWb2C6A1kGJmALj78KOckwiMBM4CNgHzzWySuy+POqYpcA/Qzd23m1mNqJfY7e7tjuXDSMlQqWwSL17diRteXsg9byxl+6693HBmYw5dmyKSP2IZKDeKyHxMNwEGXAzUj+G1OwKr3T0r6NjOAPrnOuZaYKS7bwc4NMZC5GjKJCcy5oo0+rc7mYff/ZwHJ6/QuhIi+SyWPoiu7n4FsN3d/wfoAjSL4bzawMao7U1BWbRmQDMzm2lmc8ysb9S+FDNbEJSfH8P7SQmTlJjAo5e0Y3CX+oyZvpa7XlvC/gMHj36iiMQkliamH4K/u8zsZGAbkfmY8uv9mwLpQB1gmpmd4u47gPruvtnMGgEfmtlSd18TfbKZDQGGAKSmppKZmZlPYcVHdnZ2kf8M+SnW+kiv6OxsksRrCzexZuMWbmhbmuTE4tfcpOvjMNVFTmHVRywJ4m0zqwz8FfgYcGBMDOdtBupGbdcJyqJtAua6+z5grZmtJJIw5rv7ZgB3zzKzTA7PKPtf7j4aGA2Qlpbm6enpMYRVeGVmZlLUP0N+Opb66NED2s9ax32TlvHc6hSeHZxGhZSkcAMsYLo+DlNd5BRWffxkE1OwUNAUd9/h7q8T6Xto4e73xvDa84GmZtbQzJKBAUDuXyNNJHL3gJlVI9LklGVmVcysdFR5N2A5Ij9hcNcGPD6gneZvEsknP5kg3P0gkV8iHdre4+47Y3lhd98PDAPeA1YAr7r7MjMbHjXw7j1gm5ktB6YCd7n7NqAlsMDMPgnK/xL96yeRI+nfrjZjrojM33SJ5m8SOSGxdFJPMbML7Th+Q+juk929mbs3dvc/BWX3uvuk4Lm7++3u3iqYLTYjKJ8VbLcN/j53rO8tJVePFpH5m77W/E0iJySWBHEdkcn59pjZt2b2nZl9G3JcIick9/xNWuta5NjFMpK6grsnuHuyu1cMtisWRHAiJyJ6/qbLxsxhxipNISZyLGIZKHdGXo+CCE7kRGn+JpHjF0sT011Rjz8CbxNZREikSDg0f9MpdSpp/iaRYxBLE9N5UY+zgDaAJuSXIqVS2SReuroTZzarzj1vLOWpzNWamkPkKGK5g8htE5GfoYoUKZq/SeTYHHUktZk9SWT0NEQSSjsiI6pFipxD8zdVKpPEmOlr2b5rH3/55SmUSjye/yuJFG+xTLWxIOr5fmCcu88MKR6R0CUkGP/TrzVVyibz+JRV7Ny9jycHticlKTHeoYkUKrEkiNeAH9z9AETWeTCzsu6uIapSZJkZt53VjCplk7j/7eVc+fd5jLmi+M3fJHIiYhpJDZSJ2i4DfBBOOCIF68puDXns0nYsWKf5m0RyiyVBpLh79qGN4HnZ8EISKVjnt9f8TSJ5iSVBfG9mpx3aMLMOwO7wQhIpeD1a1ODFqzuxNXsPF4+azeqvNH+TSCwJ4lZggplNN7MZwHgis7SKFCunN6jKq9d1Yd8B5+JRmr9JJJaBcvOBFsANwPVAS3dfGHZgIvHQslZFXr+hC+VTSmn+JinxYpmLaShQzt0/dfdPgfJmdmP4oYnER/2TyvH69V3/O3/T6ws3aUCdlEixNDFdG6wRDYC7bweuDS8kkfg7NH9Tu7qVuWPCJ1wxdh7rt30f77BEClQsCSIxerEgM0sEksMLSaRwqFQ2iXFDOnP/ea1YtGEHfR6dxsipq9m7/2C8QxMpELEkiHeB8WbWy8x6AeOAd2J5cTPra2afm9lqM7v7CMdcYmbLzWyZmb0SVT7YzFYFj8GxvJ9IfktMMK7s1pAPbj+Tni1q8Nf3PucXT0xn/rpv4h2aSOhiSRC/BT4k0kF9PbCUnAPn8hTcaYwEzgFaAQPNrFWuY5oC9wDd3L01kV9MYWZVgfuATkBH4D4zqxLjZxLJdzUrpfD0rzrw3OA0du09wMWjZnP360vYsWtvvEMTCU0sv2I6CMwF1hH5su4JrIjhtTsCq909y933AhlA/1zHXAuMDPo1cPevgvKzgffd/Ztg3/tA3xjeUyRUvVqm8v7tZ3DdGY2YsHATvR75iDcXqRNbiqcjzsVkZs2AgcHjayLjH3D3HjG+dm1gY9T2JiJ3BNGaBe81E0gE7nf3d49wbu08YhwCDAFITU0lMzMzxtAKp+zs7CL/GfJTYa6PLmWhdufSPL9sL7eN/4QxHyzlilalqVkuvFlhC3N9FDTVRU5h1cdPTdb3GTAdONfdVwOY2W0hvH9TIB2oA0wzs1NiPdndRwOjAdLS0jw9PT2fwytYmZmZFPXPkJ+KQn1cfq7zyrwNPPzuZ9w7ew/DejThujMbUbpU/s8MWxTqo6CoLnIKqz5+6r87vwS2AFPNbEzQQW0/cXxum4G6Udt1grJom4BJ7r7P3dcCK4kkjFjOFYm7xARjUOf6TLn9TPq0SuVv76/knMenMydrW7xDEzlhR0wQ7j7R3QcQGUU9lUgHcg0ze9rM+sTw2vOBpmbW0MySgQHApFzHTCRy94CZVSPS5JQFvAf0MbMqQed0n6BMpFCqUTGFEZedxvNXnc6+AwcZMHoOd074hG++Vye2FF2xdFJ/7+6vuPt5RP4nv4jIL5uOdt5+InM2vUekU/tVd19mZsPNrF9w2HvANjNbTiQJ3eXu29z9G+ABIklmPjA8KBMp1NKb1+Dft57JjemNmbhoM70eyWTCgo3qxJYiKZYFg/4r+EXRf9v9Yzh+MjA5V9m9Uc8duD145D53LDD2WOITKQzKJCfym74t6N+uNr9/cyl3vbaE1xZu4k8XnEKTGuXjHZ5IzLQQr0hImteswKvXdeHPvzyFFVu+5ZzHp/G3f3/OD/sOxDs0kZgoQYiEKCHBGNixHlPuSOcXp9TiiQ9Xc87j05m5WrPESuGnBCFSAKpXKM1jA9rz4tUdcXcuf3Yut41frCVOpVBTghApQD9rWp13bz2Dm3o24Z9LvqDXIx+RMW8DBw+qE1sKHyUIkQKWkpTIHX2a884tP6N5zQrc/cZSLh09m5VfaplTKVyUIETipEmNCowf0pmHLzqVVV9l8/PHp/PX9z5TJ7YUGkoQInFkZlySVpcpt59J/3a1GTl1DX0encZHK7fGOzQRJQiRwuCk8qV55JK2vHJtJ0olGIPHzuOmcYv46rsf4h2alGBKECKFSNfG1Xjn1p9xa++mvPfpf+j1yEe8NGe9OrElLpQgRAqZ0qUSubV3M9659We0ObkSf5j4KReOmsW6neqbkIKlBCFSSDWuXp5Xru3E3y5py/ptu7h/9g8MfeVj1n79fbxDkxJCCUKkEDMzfnlaHTLvSue8xkl8uOIrev/tI3735lK+/Fb9ExIuJQiRIqBiShIXNk3mo9+kc3mnerw6fyNn/nUqD737GTt374t3eFJMKUGIFCE1KqQwvH8bptxxJme3rsnTmWs44+GpPPPRGo2fkHynBCFSBNU/qRyPD2jPv27uTvt6lfnzO5+R/tdMMuZtYP+Bg/EOT4oJJQiRIqz1yZV4/qqOZAzpTM1KKdz9xlL6PDaNd5Zu0SJFcsKUIESKgc6NTuLNG7vyzKAOJJhxw8sfc/7ImczStOJyAkJNEGbW18w+N7PVZnZ3HvuvNLOtZrY4eFwTte9AVHnutaxFJBcz4+zWNXn3lp/x8EWnsvW7PVz27FwGPTeXTzfvjHd4UgQd05Kjx8LMEoGRwFnAJmC+mU1y9+W5Dh3v7sPyeInd7t4urPhEiqtSiQlcklaXfm1P5qU56xkxdTXnPjmDc0+txZ19mtOgWrl4hyhFRJh3EB2B1e6e5e57gQygf4jvJyJRUpISueZnjZj2mx4M69GEKcEYit+/uZSvNIZCYmBhdWSZ2UVAX3e/JtgeBHSKvlswsyuBPwNbgZXAbe6+Mdi3H1gM7Af+4u4T83iPIcAQgNTU1A4ZGRmhfJaCkp2dTfnyWtT+ENVHTidaHzt+OMikrH18tHE/iQnQp34S5zRMolyS5WOUBUPXRk4nUh89evRY6O5pee0LrYkpRm8D49x9j5ldB7wA9Az21Xf3zWbWCPjQzJa6+5rok919NDAaIC0tzdPT0wsw9PyXmZlJUf8M+Un1kVN+1Mf5wLqvv+dv769k0idfMOM/cGN6Y67o0oCUpMR8ibMg6NrIKaz6CLOJaTNQN2q7TlD2X+6+zd0PLcr7LNAhat/m4G8WkAm0DzFWkRKjQbVyPDGwPf+8qTtt61Tmwcmf0eP/Mhk/X2MoJKcwE8R8oKmZNTSzZGAAkOPXSGZWK2qzH7AiKK9iZqWD59WAbkDuzm0ROQFtalfihV93ZNy1nalRMYXfvr6Usx+bxrufagyFRISWINx9PzAMeI/IF/+r7r7MzIabWb/gsJvNbJmZfQLcDFwZlLcEFgTlU4n0QShBiISgS+OTmHhjV0b9KnIDf/1LH3P+U7OYtUZjKEq6UPsg3H0yMDlX2b1Rz+8B7snjvFnAKWHGJiKHmRl929Skd8savPHxZh79YCWXjZnLGc2q85uzm9OmdqV4hyhxoJHUIvJfpRITuOT0uky9M53f/bwFn2zcwblPzuCmcYtYp3UoSpx4/4pJRAqhlKREhpzRmEtPr8foaWt4bsZa3lm6hQEd63LdGY2pW7VsvEOUAqAEISJHVKlMEned3YLBXRrwxIeryJi3kVfmbqB3y1Su7NaALo1OwqzojaOQ2ChBiMhR1aiYwv+efwo3pjfhpTnrGTdvA/9e/iXNUyswuCG+rwkAABA8SURBVGsDLmhfmzLJRWcchcRGfRAiErOTK5fhN31bMPueXjx80akkJhi/e3Mpnf88hQcnr2DjN7viHaLkI91BiMgxS0lK5JK0ulzcoQ7z123nhVnreG7GWp6dnqXmp2JECUJEjpuZ0bFhVTo2rMoXO3ar+amYUROTiOQLNT8VP7qDEJF8pean4kMJQkRCoeanok9NTCISOjU/FU26gxCRAqPmp6JFCUJECpyan4oGNTGJSFyp+anw0h2EiBQKan4qfJQgRKRQiaX56aQDWvGuIITaxGRmfc3sczNbbWZ357H/SjPbamaLg8c1UfsGm9mq4DE4zDhFpHA6UvPT7Zm7eOyDlezYtTfeIRZrod1BmFkiMBI4C9gEzDezSXksHTre3YflOrcqcB+QBjiwMDh3e1jxikjhFd38tGD9dh58Yx6PfbCKMdOyGNSlAVd3b0j1CqXjHWaxE+YdREdgtbtnufteIAPoH+O5ZwPvu/s3QVJ4H+gbUpwiUkSYGac3qMotp6Xw7q0/o2fLVJ6ZtobuD33I/ZOWsWXn7niHWKyEmSBqAxujtjcFZbldaGZLzOw1M6t7jOeKSAnVomZFnhzYng9uP5Pz2p7MS3PWc8bDU7nnjSVs2KZfPuWHeHdSvw2Mc/c9ZnYd8ALQM9aTzWwIMAQgNTWVzMzMUIIsKNnZ2UX+M+Qn1UdOqo/DctfFudWhU/cU3lm7jwnzNzJ+/kY61yrFuY2SOLl88f81f1jXRpgJYjNQN2q7TlD2X+6+LWrzWeDhqHPTc52bmfsN3H00MBogLS3N09PTcx9SpGRmZlLUP0N+Un3kpPo47Eh1cTHw5bc/MGZaFi/P3cDsLbs5p01NhvZoQuuTKxV4nAUlrGsjzNQ6H2hqZg3NLBkYAEyKPsDMakVt9gNWBM/fA/qYWRUzqwL0CcpERH5SasUU/nBuK2be3ZOh6U2YvvJrfvHEDH79/HwWrtfvXI5FaHcQ7r7fzIYR+WJPBMa6+zIzGw4scPdJwM1m1g/YD3wDXBmc+42ZPUAkyQAMd/dvwopVRIqfquWSufPs5lx7RiNenB0ZdHfh07Po2vgkhvVsokF3MQi1D8LdJwOTc5XdG/X8HuCeI5w7FhgbZnwiUvxVKpPEsJ5NuapbQ16Zu4HR07O4bMxcOtSvwrAeTUhvXl2J4giKf++NiAhQrnQprj2jEdN/04MH+rfmPzt/4Krn53PukzN4Z+kWDh7U6OzclCBEpERJSUpkUJcGTL0znYcvOpXv9+znhpc/5uzHpjFx0Wb2HzgY7xALDSUIESmRkkslcElaXabckc4TA9uTYMat4xfT85GPyJi3gb37lSiUIESkREtMMPq1PZl3bvkZowd1oHLZJO5+Yyln/nUqz89cyw/7DsQ7xLhRghARARISjD6ta/LW0G688OuO1KlShvvfXk73h6Yy6qM1ZO/ZH+8QC1y8R1KLiBQqZsaZzapzZrPqzM3axoipq/nLO5/xdOYaft2tIVd2bUClsknxDrNAKEGIiBxBp0Yn0anRSSzeuIMRH67m0Q9WMmZ6FoO61Ofq7g2pVr54zyCrBCEichTt6lbm2cFprNjyLSOnrmbUR2v4+8y1DDi9Hr/qXJ8mNcrHO8RQKEGIiMSoZa2KjLjsNG7bms3TmWt4ee56np+1js6NqvKrzvXp06omyaWKT9euEoSIyDFqXL08/3dxW37btwUTFm7klbkbGPbKIqqVT+aStLoM7FiPulXLxjvME6YEISJynKpXKM2N6U24/ozGTFu1lZfmbGDUR2t4+qM1pDerzuWd6tOjRQ0SE4rmVB5KECIiJyghwUhvXoP05jX4YsduMuZvJGPeBq75xwJOrpTCwI71uPT0utSomBLvUI+JEoSISD46uXIZbj+rGTf1bMKUFV/y8twNPPL+Sh6fsoo+rVO5vFN9ujQ6iYQicFehBCEiEoKkxAT6tqlF3za1WPv194ybt4EJCzYyeel/aFitHJd1rMdFHepQpVxyvEM9ouLT3S4iUkg1rFaO3/28JbPv6cWjl7blpHLJ/GnyCjr9eQq3j1/MwvXf4F74ZpPVHYSISAFJSUrkgvZ1uKB9HVZs+ZZX5m7gzUWbeWPRZlrUrMDlnetzfruTqZBSOEZq6w5CRCQOWtaqyAPnt2Hu73rx4AWnkGDGHyd+SucHp/C7N5ey7Iud8Q4x3ARhZn3N7HMzW21md//EcReamZtZWrDdwMx2m9ni4DEqzDhFROKlXOlSXNapHv+6uTsTh3bjnFNq8frCTfziiRmcP3Imry3cFLcZZUNrYjKzRGAkcBawCZhvZpPcfXmu4yoAtwBzc73EGndvF1Z8IiKFiZnRrm5l2tWtzB9/0YrXP97Ey3PXc+eET3jgn8u58LQ6XN65Ho2rF9y0HmH2QXQEVrt7FoCZZQD9geW5jnsAeAi4K8RYRESKjEplk/h194Zc1a0Bc7K+4eW563lxzjrGzlxLl0YncXnnegUyrYeF1XNuZhcBfd39mmB7ENDJ3YdFHXMa8Ht3v9DMMoE73X2BmTUAlgErgW+BP7j79DzeYwgwBCA1NbVDRkZGKJ+loGRnZ1O+fPGc9Ot4qD5yUn0cVhLrYuceZ/qmfUzduJ9tPzgVk40z6pTizDqlKHNw13HXR48ePRa6e1pe++L2KyYzSwD+BlyZx+4tQD1332ZmHYCJZtba3b+NPsjdRwOjAdLS0jw9PT3coEOWmZlJUf8M+Un1kZPq47CSWhf9gQMHnWmrtvLynPVM/uwr/rV2H6enlmL8LWdilr+D78JMEJuBulHbdYKyQyoAbYDM4EPVBCaZWT93XwDsAXD3hWa2BmgGLAgxXhGRQi8xwejRvAY9mtdg847djJ+3gax16/M9OUC4v2KaDzQ1s4ZmlgwMACYd2unuO929mrs3cPcGwBygX9DEVD3o5MbMGgFNgawQYxURKXJqVy7D7X2ac1GzcEZjh3YH4e77zWwY8B6QCIx192VmNhxY4O6TfuL0M4DhZrYPOAhc7+7fhBWriIj8WKh9EO4+GZicq+zeIxybHvX8deD1MGMTEZGfppHUIiKSJyUIERHJkxKEiIjkSQlCRETypAQhIiJ5UoIQEZE8hTYXU0Ezs63A+njHcYKqAV/HO4hCRPWRk+rjMNVFTidSH/XdvXpeO4pNgigOzGzBkSbNKolUHzmpPg5TXeQUVn2oiUlERPKkBCEiInlSgihcRsc7gEJG9ZGT6uMw1UVOodSH+iBERCRPuoMQEZE8KUHEiZnVNbOpZrbczJaZ2S1BeVUze9/MVgV/q8Q71oJiZolmtsjM/hlsNzSzuWa22szGB+uKlAhmVtnMXjOzz8xshZl1KeHXxm3Bv5NPzWycmaWUpOvDzMaa2Vdm9mlUWZ7Xg0U8EdTLkmBp5+OiBBE/+4E73L0V0BkYamatgLuBKe7eFJgSbJcUtwArorYfAh519ybAduDquEQVH48D77p7C6AtkXopkdeGmdUGbgbS3L0NkfVlBlCyro/ngb65yo50PZxDZJG1psAQ4OnjfVMliDhx9y3u/nHw/DsiXwC1iSw7+0Jw2AvA+fGJsGCZWR3gF8CzwbYBPYHXgkNKUl1UIrJo1nMA7r7X3XdQQq+NQCmgjJmVAsoSWbe+xFwf7j4NyL1o2pGuh/7APzxiDlDZzGodz/sqQRQCZtYAaA/MBVLdfUuw6z9AapzCKmiPAb8hsoIgwEnADnffH2xvIpJAS4KGwFbg70GT27NmVo4Sem24+2bg/4ANRBLDTmAhJff6OORI10NtYGPUccddN0oQcWZm5Ymsnneru38bvc8jPzEr9j8zM7Nzga/cfWG8YykkSgGnAU+7e3vge3I1J5WUawMgaFvvTyRxngyU48fNLSVaWNeDEkQcmVkSkeTwsru/ERR/eeh2MPj7VbziK0DdgH5mtg7IINJ08DiRW+NDy+LWATbHJ7wCtwnY5O5zg+3XiCSMknhtAPQG1rr7VnffB7xB5JopqdfHIUe6HjYDdaOOO+66UYKIk6CN/Tlghbv/LWrXJGBw8Hww8FZBx1bQ3P0ed6/j7g2IdD5+6O6XA1OBi4LDSkRdALj7f4CNZtY8KOoFLKcEXhuBDUBnMysb/Ls5VB8l8vqIcqTrYRJwRfBrps7AzqimqGOigXJxYmbdgenAUg63u/+OSD/Eq0A9IrPTXuLuuTunii0zSwfudPdzzawRkTuKqsAi4Ffuviee8RUUM2tHpMM+GcgCriLyH7oSeW2Y2f8AlxL59d8i4Boi7eol4vows3FAOpFZW78E7gMmksf1ECTREUSa4XYBV7n7guN6XyUIERHJi5qYREQkT0oQIiKSJyUIERHJkxKEiIjkSQlCRETypAQhxYaZZcc7htzM7EozG3EMx68zs2phxiQSKyUIkSOIGqUrUiIpQUixZmaNzexdM1toZtPNrEVQfl6wlsAiM/vAzFKD8vvN7EUzmwm8GGyPNbNMM8sys5ujXvtXZjbPzBab2TNmlhiUX2VmK81sHpEpIfKKq7yZ/d3MlgZz9l+YxzETg7iXmdmQoCzRzJ4P1kVYama3BeU3W2RtkSVmlhGUlQtinxd8zv5BeeuouJeYWdP8rHMpRtxdDz2KxQPIzqNsCtA0eN6JyDQeAFU4PFD0GuCR4Pn9RGYKLRO1PQsoTWQU6zYgCWgJvA0kBcc9BVwB1CIyNUR1IqOgZwIj8ojrIeCxqO0qwd91QLXgedXgbxngUyIz3HYA3o86r3Lw9wugdK6yB4mMLgaoDKwkMtHdk8DlQXnyoc+qhx65H7qFlmIrmCm3KzAhMvsAEPmih8gEZuODSc6SgbVRp05y991R2//yyBQOe8zsKyLTKvci8mU9P3jtMkQmS+sEZLr71iCG8UCzPMLrTWTeKQDcfXsex9xsZhcEz+sSWQDmc6CRmT0J/Av4d7B/CfCymU0kMgUDQB8ikyDeGWynEJmWYTbw+2ANjjfcfVUe7y2iJiYp1hKIrBnQLurRMtj3JJH/2Z8CXEfky/OQ73O9TvT8PgeITMdtwAtRr9vc3e/Pr8CDOal6A13cvS2RuYZSgkTSFsgEridYYInIYksjicz6Oj/oPzHgwqgY67n7Cnd/BegH7AYmm1nP/IpbihclCCm2PLK+xlozuxj+u1Zv22B3JQ5PgTw4r/OPYgpwkZnVCF67qpnVJzLZ4plmdlIwnfvFRzj/fWDooQ378frSlYDt7r4r6DfpHBxXDUhw99eBPwCnmVkCUNfdpwK/Dc4tD7wH3BRM3oaZtQ/+NgKy3P0JIjOAnnocn19KACUIKU7KmtmmqMftwOXA1Wb2CbCMyMIzEOlbmGBmC4Gvj/WN3H05kS/of5vZEiJf+LU8Mq3y/USacWaSc43taP8LVAk6mz8BeuTa/y5QysxWAH8B5gTltYFMM1sMvATcQ2SN5pfMbCmRO40nPLJE6QNE+kuWmNmyYBvgEuDT4DXaAP841s8vJYNmcxURkTzpDkJERPKkBCEiInlSghARkTwpQYiISJ6UIEREJE9KECIikiclCBERyZMShIiI5On/AawFgoYj2VnlAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vwt2ztUXFaH",
        "colab_type": "text"
      },
      "source": [
        "# **Learning without Fortgetting (LwF) model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ie3wtV4DTdu",
        "colab_type": "text"
      },
      "source": [
        "**LwF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnA3s8D9rItr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class LwF(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        self.total_num_classes = n_classes\n",
        "        self.known_classes = 0\n",
        "        self.list_known_classes=[]\n",
        "        self.exemplar_sets = []\n",
        "        self.flag_mean = True\n",
        "        self.exemplar_means = []\n",
        "\n",
        "        # We take a standard ResNet and Extend it\n",
        "        super(LwF, self).__init__()\n",
        "        self.extractor = resnet32()\n",
        "        self.fully_connected = nn.Linear(self.extractor.out_dim, 0, bias=True)\n",
        "        torch.nn.init.xavier_uniform_(self.fully_connected.weight)\n",
        "\n",
        "        self.fully_connected.bias.data.fill_(0.01)\n",
        "        self.loss=nn.BCEWithLogitsLoss()\n",
        "        self.optimizer = optim.SGD(self.parameters(), lr=2.0,weight_decay=0.00001)\n",
        "        self.scheduler=MultiStepLR(self.optimizer,[47,63],gamma=GAMMA)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # X: input data\n",
        "        self.extractor.to(DEVICE)\n",
        "        self.fully_connected.to(DEVICE)\n",
        "        x = self.extractor(x)\n",
        "        x = self.fully_connected(x)\n",
        "        return x\n",
        "\n",
        "    def increment_classes(self, classes_to_add):          # increments the number of classes we are using\n",
        "      n_classes_to_add = len(classes_to_add)\n",
        "      self.list_known_classes+=classes_to_add             #add the new classes\n",
        "      print(f\"Known classes {self.list_known_classes}\")\n",
        "      weight = self.fully_connected.weight.data\n",
        "      feature_size = self.fully_connected.in_features\n",
        "      old_num_classes = self.fully_connected.out_features\n",
        "      self.fully_connected = nn.Linear(\n",
        "          feature_size, old_num_classes+n_classes_to_add, bias = True)\n",
        "      self.fully_connected.weight.data[:old_num_classes] = weight\n",
        "\n",
        "    def update_representation(self, dataset):             \n",
        "        # self.combine_dataset_with_exemplars(dataset)\n",
        "        # self.flag_mean = True\n",
        "        classes_to_idx = dataset.labels_to_int\n",
        "        new_classes = [cls for cls in classes_to_idx if cls not in self.list_known_classes] #gets indexes of new classes only\n",
        "\n",
        "        train_data_loader = DataLoader(\n",
        "            dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "        \n",
        "        preds_old_net = []\n",
        "        #preds_old_net = torch.zeros(len(dataset), self.num_classes).cuda()\n",
        "        \n",
        "        if self.known_classes > 0:\n",
        "\n",
        "          self.eval()\n",
        "          logits_old_net = torch.zeros(len(dataset.data), self.known_classes).cuda()\n",
        "          with torch.no_grad():\n",
        "            for indices, images, labels in train_data_loader:\n",
        "              images = images.to(DEVICE)\n",
        "              g = self.forward(images)\n",
        "              sigmoid = torch.nn.Sigmoid()\n",
        "              logits_old_net[indices] = sigmoid(g)\n",
        "\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "        self.increment_classes(new_classes)\n",
        "        \n",
        "        optimizer = optim.SGD(self.parameters(), lr=2.0,weight_decay=0.00001, momentum = 0.9)\n",
        "        scheduler = MultiStepLR(optimizer,[47,63],gamma=GAMMA)\n",
        "        self.train()\n",
        "\n",
        "        eye = torch.eye(self.known_classes+len(new_classes))\n",
        "\n",
        "        for epoch in range(NUM_EPOCHS):\n",
        "          print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], LR: {scheduler.get_last_lr()}\")\n",
        "          i=0\n",
        "          for indices, images, labels in train_data_loader:\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "            indices = indices.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            output = self.forward(images)\n",
        "\n",
        "            if self.known_classes==0:\n",
        "              labels_one_hot_new_classes = []\n",
        "              for label in labels:\n",
        "                labels_one_hot_new_classes.append(eye[label])\n",
        "              labels_one_hot_new_classes = torch.stack(labels_one_hot_new_classes).cuda()\n",
        "              loss=self.loss(output,labels_one_hot_new_classes)\n",
        "              loss.backward()\n",
        "            if self.known_classes > 0:\n",
        "              labels_one_hot_new_classes = eye[:, self.known_classes:]\n",
        " \n",
        "              labels_one_hot = []\n",
        "              for label in labels:\n",
        "                labels_one_hot.append(labels_one_hot_new_classes[label])\n",
        "              labels_one_hot = torch.stack(labels_one_hot).cuda()\n",
        "              logits = logits_old_net[indices].cuda()\n",
        "              labels_concatenate=torch.cat((logits,labels_one_hot),dim=1)\n",
        "              loss=self.loss(output,labels_concatenate)\n",
        "              loss.backward()\n",
        "            optimizer.step()\n",
        "            i+=1\n",
        "          torch.cuda.empty_cache()\n",
        "          print(f\"Loss: {loss.item()}\")\n",
        "          scheduler.step() \n",
        "        self.known_classes += len(new_classes)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1TeLZSGXj8e",
        "colab_type": "text"
      },
      "source": [
        "**Training LwF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulrc98yTDaDD",
        "colab_type": "code",
        "outputId": "2e24841b-a8ed-4517-dd29-7184b4520c07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "net = LwF(n_classes=100)\n",
        "net.to(DEVICE)\n",
        "\n",
        "for i, train_dataset in enumerate(train_datasets):\n",
        "  torch.cuda.empty_cache()\n",
        "  print(f\"BATCH [{i}]\")\n",
        "  print(f\"Training on {train_dataset.labels} -> {train_dataset.labels_to_int}\")\n",
        "  net.train()\n",
        "  net.update_representation(copy.deepcopy(train_dataset))\n",
        "\n",
        "  net.eval()\n",
        "  corrects = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for num in range(i+1):\n",
        "      print(f\"Validating classes {test_datasets[num].labels} -> {test_datasets[num].labels_to_int}\")\n",
        "      test_dataloader = DataLoader(test_datasets[num], batch_size=BATCH_SIZE, num_workers=4, shuffle = True)\n",
        "      for _, images, labels in test_dataloader:\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        outputs = net(images)\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        # Update Corrects\n",
        "        corrects += torch.sum(preds == labels.data).data.item()\n",
        "        total += len(images)\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    # Calculate Accuracy\n",
        "    accuracy = corrects / float(total)\n",
        "    print(f\"Accuracy: {accuracy}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BATCH [0]\n",
            "Training on ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Known classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Epoch [1/70], LR: [2.0]\n",
            "Loss: 0.32896044850349426\n",
            "Epoch [2/70], LR: [2.0]\n",
            "Loss: 0.344553679227829\n",
            "Epoch [3/70], LR: [2.0]\n",
            "Loss: 0.31547585129737854\n",
            "Epoch [4/70], LR: [2.0]\n",
            "Loss: 0.28766587376594543\n",
            "Epoch [5/70], LR: [2.0]\n",
            "Loss: 0.297115296125412\n",
            "Epoch [6/70], LR: [2.0]\n",
            "Loss: 0.33089154958724976\n",
            "Epoch [7/70], LR: [2.0]\n",
            "Loss: 0.2954370677471161\n",
            "Epoch [8/70], LR: [2.0]\n",
            "Loss: 0.2477138489484787\n",
            "Epoch [9/70], LR: [2.0]\n",
            "Loss: 0.29497846961021423\n",
            "Epoch [10/70], LR: [2.0]\n",
            "Loss: 0.25300318002700806\n",
            "Epoch [11/70], LR: [2.0]\n",
            "Loss: 0.32428231835365295\n",
            "Epoch [12/70], LR: [2.0]\n",
            "Loss: 0.29303112626075745\n",
            "Epoch [13/70], LR: [2.0]\n",
            "Loss: 0.3104630708694458\n",
            "Epoch [14/70], LR: [2.0]\n",
            "Loss: 0.29044392704963684\n",
            "Epoch [15/70], LR: [2.0]\n",
            "Loss: 0.31014809012413025\n",
            "Epoch [16/70], LR: [2.0]\n",
            "Loss: 0.26986250281333923\n",
            "Epoch [17/70], LR: [2.0]\n",
            "Loss: 0.2634470760822296\n",
            "Epoch [18/70], LR: [2.0]\n",
            "Loss: 0.23878677189350128\n",
            "Epoch [19/70], LR: [2.0]\n",
            "Loss: 0.21753230690956116\n",
            "Epoch [20/70], LR: [2.0]\n",
            "Loss: 0.2974422872066498\n",
            "Epoch [21/70], LR: [2.0]\n",
            "Loss: 0.31420326232910156\n",
            "Epoch [22/70], LR: [2.0]\n",
            "Loss: 0.27624860405921936\n",
            "Epoch [23/70], LR: [2.0]\n",
            "Loss: 0.292058527469635\n",
            "Epoch [24/70], LR: [2.0]\n",
            "Loss: 0.09930384159088135\n",
            "Epoch [25/70], LR: [2.0]\n",
            "Loss: 0.22470378875732422\n",
            "Epoch [26/70], LR: [2.0]\n",
            "Loss: 0.16533397138118744\n",
            "Epoch [27/70], LR: [2.0]\n",
            "Loss: 0.278205543756485\n",
            "Epoch [28/70], LR: [2.0]\n",
            "Loss: 0.326704204082489\n",
            "Epoch [29/70], LR: [2.0]\n",
            "Loss: 0.11920326203107834\n",
            "Epoch [30/70], LR: [2.0]\n",
            "Loss: 0.25399306416511536\n",
            "Epoch [31/70], LR: [2.0]\n",
            "Loss: 0.29589125514030457\n",
            "Epoch [32/70], LR: [2.0]\n",
            "Loss: 0.22868147492408752\n",
            "Epoch [33/70], LR: [2.0]\n",
            "Loss: 0.2043766975402832\n",
            "Epoch [34/70], LR: [2.0]\n",
            "Loss: 0.22293691337108612\n",
            "Epoch [35/70], LR: [2.0]\n",
            "Loss: 0.23493866622447968\n",
            "Epoch [36/70], LR: [2.0]\n",
            "Loss: 0.16928553581237793\n",
            "Epoch [37/70], LR: [2.0]\n",
            "Loss: 0.24563036859035492\n",
            "Epoch [38/70], LR: [2.0]\n",
            "Loss: 0.30193448066711426\n",
            "Epoch [39/70], LR: [2.0]\n",
            "Loss: 0.10617430508136749\n",
            "Epoch [40/70], LR: [2.0]\n",
            "Loss: 0.09590116888284683\n",
            "Epoch [41/70], LR: [2.0]\n",
            "Loss: 0.21260209381580353\n",
            "Epoch [42/70], LR: [2.0]\n",
            "Loss: 0.20011039078235626\n",
            "Epoch [43/70], LR: [2.0]\n",
            "Loss: 0.19383087754249573\n",
            "Epoch [44/70], LR: [2.0]\n",
            "Loss: 0.17195317149162292\n",
            "Epoch [45/70], LR: [2.0]\n",
            "Loss: 0.2520540654659271\n",
            "Epoch [46/70], LR: [2.0]\n",
            "Loss: 0.13423781096935272\n",
            "Epoch [47/70], LR: [2.0]\n",
            "Loss: 0.059209417551755905\n",
            "Epoch [48/70], LR: [0.4]\n",
            "Loss: 0.16229069232940674\n",
            "Epoch [49/70], LR: [0.4]\n",
            "Loss: 0.19964547455310822\n",
            "Epoch [50/70], LR: [0.4]\n",
            "Loss: 0.09795496612787247\n",
            "Epoch [51/70], LR: [0.4]\n",
            "Loss: 0.191963791847229\n",
            "Epoch [52/70], LR: [0.4]\n",
            "Loss: 0.15524117648601532\n",
            "Epoch [53/70], LR: [0.4]\n",
            "Loss: 0.18793480098247528\n",
            "Epoch [54/70], LR: [0.4]\n",
            "Loss: 0.1891760379076004\n",
            "Epoch [55/70], LR: [0.4]\n",
            "Loss: 0.12648352980613708\n",
            "Epoch [56/70], LR: [0.4]\n",
            "Loss: 0.1409953534603119\n",
            "Epoch [57/70], LR: [0.4]\n",
            "Loss: 0.05775342509150505\n",
            "Epoch [58/70], LR: [0.4]\n",
            "Loss: 0.11593013256788254\n",
            "Epoch [59/70], LR: [0.4]\n",
            "Loss: 0.08851159363985062\n",
            "Epoch [60/70], LR: [0.4]\n",
            "Loss: 0.13316170871257782\n",
            "Epoch [61/70], LR: [0.4]\n",
            "Loss: 0.16142402589321136\n",
            "Epoch [62/70], LR: [0.4]\n",
            "Loss: 0.09856275469064713\n",
            "Epoch [63/70], LR: [0.4]\n",
            "Loss: 0.19276995956897736\n",
            "Epoch [64/70], LR: [0.08000000000000002]\n",
            "Loss: 0.15404252707958221\n",
            "Epoch [65/70], LR: [0.08000000000000002]\n",
            "Loss: 0.14324526488780975\n",
            "Epoch [66/70], LR: [0.08000000000000002]\n",
            "Loss: 0.25752291083335876\n",
            "Epoch [67/70], LR: [0.08000000000000002]\n",
            "Loss: 0.17547839879989624\n",
            "Epoch [68/70], LR: [0.08000000000000002]\n",
            "Loss: 0.16561353206634521\n",
            "Epoch [69/70], LR: [0.08000000000000002]\n",
            "Loss: 0.13673780858516693\n",
            "Epoch [70/70], LR: [0.08000000000000002]\n",
            "Loss: 0.08620663732290268\n",
            "Validating classes ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Accuracy: 0.823\n",
            "BATCH [1]\n",
            "Training on ['bicycle', 'plain', 'couch', 'lobster', 'lion', 'chair', 'tulip', 'television', 'skunk', 'girl'] -> [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Known classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Epoch [1/70], LR: [2.0]\n",
            "Loss: 0.22945120930671692\n",
            "Epoch [2/70], LR: [2.0]\n",
            "Loss: 0.17091292142868042\n",
            "Epoch [3/70], LR: [2.0]\n",
            "Loss: 0.2268267124891281\n",
            "Epoch [4/70], LR: [2.0]\n",
            "Loss: 0.22244893014431\n",
            "Epoch [5/70], LR: [2.0]\n",
            "Loss: 0.16757157444953918\n",
            "Epoch [6/70], LR: [2.0]\n",
            "Loss: 0.12925949692726135\n",
            "Epoch [7/70], LR: [2.0]\n",
            "Loss: 0.22128747403621674\n",
            "Epoch [8/70], LR: [2.0]\n",
            "Loss: 0.18087950348854065\n",
            "Epoch [9/70], LR: [2.0]\n",
            "Loss: 0.18739470839500427\n",
            "Epoch [10/70], LR: [2.0]\n",
            "Loss: 0.25103697180747986\n",
            "Epoch [11/70], LR: [2.0]\n",
            "Loss: 0.23899097740650177\n",
            "Epoch [12/70], LR: [2.0]\n",
            "Loss: 0.16663432121276855\n",
            "Epoch [13/70], LR: [2.0]\n",
            "Loss: 0.15683642029762268\n",
            "Epoch [14/70], LR: [2.0]\n",
            "Loss: 0.2075432538986206\n",
            "Epoch [15/70], LR: [2.0]\n",
            "Loss: 0.17595742642879486\n",
            "Epoch [16/70], LR: [2.0]\n",
            "Loss: 0.17987044155597687\n",
            "Epoch [17/70], LR: [2.0]\n",
            "Loss: 0.16400475800037384\n",
            "Epoch [18/70], LR: [2.0]\n",
            "Loss: 0.11153977364301682\n",
            "Epoch [19/70], LR: [2.0]\n",
            "Loss: 0.17699484527111053\n",
            "Epoch [20/70], LR: [2.0]\n",
            "Loss: 0.16193661093711853\n",
            "Epoch [21/70], LR: [2.0]\n",
            "Loss: 0.13741141557693481\n",
            "Epoch [22/70], LR: [2.0]\n",
            "Loss: 0.15330186486244202\n",
            "Epoch [23/70], LR: [2.0]\n",
            "Loss: 0.15610329806804657\n",
            "Epoch [24/70], LR: [2.0]\n",
            "Loss: 0.17167043685913086\n",
            "Epoch [25/70], LR: [2.0]\n",
            "Loss: 0.1949252188205719\n",
            "Epoch [26/70], LR: [2.0]\n",
            "Loss: 0.15055672824382782\n",
            "Epoch [27/70], LR: [2.0]\n",
            "Loss: 0.17430941760540009\n",
            "Epoch [28/70], LR: [2.0]\n",
            "Loss: 0.19131962954998016\n",
            "Epoch [29/70], LR: [2.0]\n",
            "Loss: 0.2841126620769501\n",
            "Epoch [30/70], LR: [2.0]\n",
            "Loss: 0.23291614651679993\n",
            "Epoch [31/70], LR: [2.0]\n",
            "Loss: 0.21486937999725342\n",
            "Epoch [32/70], LR: [2.0]\n",
            "Loss: 0.16639560461044312\n",
            "Epoch [33/70], LR: [2.0]\n",
            "Loss: 0.14202287793159485\n",
            "Epoch [34/70], LR: [2.0]\n",
            "Loss: 0.17851577699184418\n",
            "Epoch [35/70], LR: [2.0]\n",
            "Loss: 0.11085369437932968\n",
            "Epoch [36/70], LR: [2.0]\n",
            "Loss: 0.1893669217824936\n",
            "Epoch [37/70], LR: [2.0]\n",
            "Loss: 0.1560143530368805\n",
            "Epoch [38/70], LR: [2.0]\n",
            "Loss: 0.15569396317005157\n",
            "Epoch [39/70], LR: [2.0]\n",
            "Loss: 0.11798657476902008\n",
            "Epoch [40/70], LR: [2.0]\n",
            "Loss: 0.16678445041179657\n",
            "Epoch [41/70], LR: [2.0]\n",
            "Loss: 0.19198362529277802\n",
            "Epoch [42/70], LR: [2.0]\n",
            "Loss: 0.1274174302816391\n",
            "Epoch [43/70], LR: [2.0]\n",
            "Loss: 0.13050833344459534\n",
            "Epoch [44/70], LR: [2.0]\n",
            "Loss: 0.14730949699878693\n",
            "Epoch [45/70], LR: [2.0]\n",
            "Loss: 0.12090640515089035\n",
            "Epoch [46/70], LR: [2.0]\n",
            "Loss: 0.3312333822250366\n",
            "Epoch [47/70], LR: [2.0]\n",
            "Loss: 0.17029587924480438\n",
            "Epoch [48/70], LR: [0.4]\n",
            "Loss: 0.160390242934227\n",
            "Epoch [49/70], LR: [0.4]\n",
            "Loss: 0.1212538629770279\n",
            "Epoch [50/70], LR: [0.4]\n",
            "Loss: 0.2458653301000595\n",
            "Epoch [51/70], LR: [0.4]\n",
            "Loss: 0.24279485642910004\n",
            "Epoch [52/70], LR: [0.4]\n",
            "Loss: 0.1698247194290161\n",
            "Epoch [53/70], LR: [0.4]\n",
            "Loss: 0.08425192534923553\n",
            "Epoch [54/70], LR: [0.4]\n",
            "Loss: 0.14648541808128357\n",
            "Epoch [55/70], LR: [0.4]\n",
            "Loss: 0.13960754871368408\n",
            "Epoch [56/70], LR: [0.4]\n",
            "Loss: 0.1312263458967209\n",
            "Epoch [57/70], LR: [0.4]\n",
            "Loss: 0.1673111766576767\n",
            "Epoch [58/70], LR: [0.4]\n",
            "Loss: 0.12862806022167206\n",
            "Epoch [59/70], LR: [0.4]\n",
            "Loss: 0.19598257541656494\n",
            "Epoch [60/70], LR: [0.4]\n",
            "Loss: 0.09679736942052841\n",
            "Epoch [61/70], LR: [0.4]\n",
            "Loss: 0.1076500192284584\n",
            "Epoch [62/70], LR: [0.4]\n",
            "Loss: 0.08484522998332977\n",
            "Epoch [63/70], LR: [0.4]\n",
            "Loss: 0.10417642444372177\n",
            "Epoch [64/70], LR: [0.08000000000000002]\n",
            "Loss: 0.14001964032649994\n",
            "Epoch [65/70], LR: [0.08000000000000002]\n",
            "Loss: 0.12633340060710907\n",
            "Epoch [66/70], LR: [0.08000000000000002]\n",
            "Loss: 0.12125033140182495\n",
            "Epoch [67/70], LR: [0.08000000000000002]\n",
            "Loss: 0.11874880641698837\n",
            "Epoch [68/70], LR: [0.08000000000000002]\n",
            "Loss: 0.09080799669027328\n",
            "Epoch [69/70], LR: [0.08000000000000002]\n",
            "Loss: 0.12444889545440674\n",
            "Epoch [70/70], LR: [0.08000000000000002]\n",
            "Loss: 0.1245853528380394\n",
            "Validating classes ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Validating classes ['bicycle', 'plain', 'couch', 'lobster', 'lion', 'chair', 'tulip', 'television', 'skunk', 'girl'] -> [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Accuracy: 0.658\n",
            "BATCH [2]\n",
            "Training on ['bowl', 'tank', 'lawn_mower', 'snake', 'ray', 'oak_tree', 'poppy', 'castle', 'telephone', 'clock'] -> [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "Known classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "Epoch [1/70], LR: [2.0]\n",
            "Loss: 0.25265493988990784\n",
            "Epoch [2/70], LR: [2.0]\n",
            "Loss: 0.18584001064300537\n",
            "Epoch [3/70], LR: [2.0]\n",
            "Loss: 0.16487713158130646\n",
            "Epoch [4/70], LR: [2.0]\n",
            "Loss: 0.18887852132320404\n",
            "Epoch [5/70], LR: [2.0]\n",
            "Loss: 0.2007371336221695\n",
            "Epoch [6/70], LR: [2.0]\n",
            "Loss: 0.1600755900144577\n",
            "Epoch [7/70], LR: [2.0]\n",
            "Loss: 0.17972469329833984\n",
            "Epoch [8/70], LR: [2.0]\n",
            "Loss: 0.11667363345623016\n",
            "Epoch [9/70], LR: [2.0]\n",
            "Loss: 0.1445712149143219\n",
            "Epoch [10/70], LR: [2.0]\n",
            "Loss: 0.18757574260234833\n",
            "Epoch [11/70], LR: [2.0]\n",
            "Loss: 0.1494865119457245\n",
            "Epoch [12/70], LR: [2.0]\n",
            "Loss: 0.15924522280693054\n",
            "Epoch [13/70], LR: [2.0]\n",
            "Loss: 0.19231249392032623\n",
            "Epoch [14/70], LR: [2.0]\n",
            "Loss: 0.17990274727344513\n",
            "Epoch [15/70], LR: [2.0]\n",
            "Loss: 0.16303011775016785\n",
            "Epoch [16/70], LR: [2.0]\n",
            "Loss: 0.1747284084558487\n",
            "Epoch [17/70], LR: [2.0]\n",
            "Loss: 0.14171631634235382\n",
            "Epoch [18/70], LR: [2.0]\n",
            "Loss: 0.15077778697013855\n",
            "Epoch [19/70], LR: [2.0]\n",
            "Loss: 0.13316893577575684\n",
            "Epoch [20/70], LR: [2.0]\n",
            "Loss: 0.17053262889385223\n",
            "Epoch [21/70], LR: [2.0]\n",
            "Loss: 0.2120504528284073\n",
            "Epoch [22/70], LR: [2.0]\n",
            "Loss: 0.16401556134223938\n",
            "Epoch [23/70], LR: [2.0]\n",
            "Loss: 0.16673390567302704\n",
            "Epoch [24/70], LR: [2.0]\n",
            "Loss: 0.1327112317085266\n",
            "Epoch [25/70], LR: [2.0]\n",
            "Loss: 0.15417566895484924\n",
            "Epoch [26/70], LR: [2.0]\n",
            "Loss: 0.12224545329809189\n",
            "Epoch [27/70], LR: [2.0]\n",
            "Loss: 0.16630104184150696\n",
            "Epoch [28/70], LR: [2.0]\n",
            "Loss: 0.15852487087249756\n",
            "Epoch [29/70], LR: [2.0]\n",
            "Loss: 0.1613248884677887\n",
            "Epoch [30/70], LR: [2.0]\n",
            "Loss: 0.1438891738653183\n",
            "Epoch [31/70], LR: [2.0]\n",
            "Loss: 0.17035377025604248\n",
            "Epoch [32/70], LR: [2.0]\n",
            "Loss: 0.2536841332912445\n",
            "Epoch [33/70], LR: [2.0]\n",
            "Loss: 0.15039269626140594\n",
            "Epoch [34/70], LR: [2.0]\n",
            "Loss: 0.17234644293785095\n",
            "Epoch [35/70], LR: [2.0]\n",
            "Loss: 0.14694014191627502\n",
            "Epoch [36/70], LR: [2.0]\n",
            "Loss: 0.17439919710159302\n",
            "Epoch [37/70], LR: [2.0]\n",
            "Loss: 0.1579388678073883\n",
            "Epoch [38/70], LR: [2.0]\n",
            "Loss: 0.1312914937734604\n",
            "Epoch [39/70], LR: [2.0]\n",
            "Loss: 0.11771230399608612\n",
            "Epoch [40/70], LR: [2.0]\n",
            "Loss: 0.14483608305454254\n",
            "Epoch [41/70], LR: [2.0]\n",
            "Loss: 0.12372696399688721\n",
            "Epoch [42/70], LR: [2.0]\n",
            "Loss: 0.1683557778596878\n",
            "Epoch [43/70], LR: [2.0]\n",
            "Loss: 0.1817609667778015\n",
            "Epoch [44/70], LR: [2.0]\n",
            "Loss: 0.18659168481826782\n",
            "Epoch [45/70], LR: [2.0]\n",
            "Loss: 0.18838101625442505\n",
            "Epoch [46/70], LR: [2.0]\n",
            "Loss: 0.18258993327617645\n",
            "Epoch [47/70], LR: [2.0]\n",
            "Loss: 0.21657799184322357\n",
            "Epoch [48/70], LR: [0.4]\n",
            "Loss: 0.1400967687368393\n",
            "Epoch [49/70], LR: [0.4]\n",
            "Loss: 0.1475696712732315\n",
            "Epoch [50/70], LR: [0.4]\n",
            "Loss: 0.17163830995559692\n",
            "Epoch [51/70], LR: [0.4]\n",
            "Loss: 0.12701977789402008\n",
            "Epoch [52/70], LR: [0.4]\n",
            "Loss: 0.1376034915447235\n",
            "Epoch [53/70], LR: [0.4]\n",
            "Loss: 0.13767112791538239\n",
            "Epoch [54/70], LR: [0.4]\n",
            "Loss: 0.20054195821285248\n",
            "Epoch [55/70], LR: [0.4]\n",
            "Loss: 0.1633823662996292\n",
            "Epoch [56/70], LR: [0.4]\n",
            "Loss: 0.12640294432640076\n",
            "Epoch [57/70], LR: [0.4]\n",
            "Loss: 0.10097602754831314\n",
            "Epoch [58/70], LR: [0.4]\n",
            "Loss: 0.15352430939674377\n",
            "Epoch [59/70], LR: [0.4]\n",
            "Loss: 0.13426092267036438\n",
            "Epoch [60/70], LR: [0.4]\n",
            "Loss: 0.1267060786485672\n",
            "Epoch [61/70], LR: [0.4]\n",
            "Loss: 0.14172370731830597\n",
            "Epoch [62/70], LR: [0.4]\n",
            "Loss: 0.15214528143405914\n",
            "Epoch [63/70], LR: [0.4]\n",
            "Loss: 0.1686236560344696\n",
            "Epoch [64/70], LR: [0.08000000000000002]\n",
            "Loss: 0.1482943296432495\n",
            "Epoch [65/70], LR: [0.08000000000000002]\n",
            "Loss: 0.14645513892173767\n",
            "Epoch [66/70], LR: [0.08000000000000002]\n",
            "Loss: 0.13601846992969513\n",
            "Epoch [67/70], LR: [0.08000000000000002]\n",
            "Loss: 0.11043107509613037\n",
            "Epoch [68/70], LR: [0.08000000000000002]\n",
            "Loss: 0.19997040927410126\n",
            "Epoch [69/70], LR: [0.08000000000000002]\n",
            "Loss: 0.18439966440200806\n",
            "Epoch [70/70], LR: [0.08000000000000002]\n",
            "Loss: 0.15946359932422638\n",
            "Validating classes ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Validating classes ['bicycle', 'plain', 'couch', 'lobster', 'lion', 'chair', 'tulip', 'television', 'skunk', 'girl'] -> [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Validating classes ['bowl', 'tank', 'lawn_mower', 'snake', 'ray', 'oak_tree', 'poppy', 'castle', 'telephone', 'clock'] -> [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "Accuracy: 0.5696666666666667\n",
            "BATCH [3]\n",
            "Training on ['worm', 'rabbit', 'tractor', 'cockroach', 'house', 'lamp', 'sweet_pepper', 'crab', 'beetle', 'dolphin'] -> [30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
            "Known classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
            "Epoch [1/70], LR: [2.0]\n",
            "Loss: 0.20760193467140198\n",
            "Epoch [2/70], LR: [2.0]\n",
            "Loss: 0.18382099270820618\n",
            "Epoch [3/70], LR: [2.0]\n",
            "Loss: 0.18771730363368988\n",
            "Epoch [4/70], LR: [2.0]\n",
            "Loss: 0.21529415249824524\n",
            "Epoch [5/70], LR: [2.0]\n",
            "Loss: 0.17767052352428436\n",
            "Epoch [6/70], LR: [2.0]\n",
            "Loss: 0.201595738530159\n",
            "Epoch [7/70], LR: [2.0]\n",
            "Loss: 0.1721585988998413\n",
            "Epoch [8/70], LR: [2.0]\n",
            "Loss: 0.17615416646003723\n",
            "Epoch [9/70], LR: [2.0]\n",
            "Loss: 0.17729957401752472\n",
            "Epoch [10/70], LR: [2.0]\n",
            "Loss: 0.1771591603755951\n",
            "Epoch [11/70], LR: [2.0]\n",
            "Loss: 0.19509653747081757\n",
            "Epoch [12/70], LR: [2.0]\n",
            "Loss: 0.18764832615852356\n",
            "Epoch [13/70], LR: [2.0]\n",
            "Loss: 0.1683720201253891\n",
            "Epoch [14/70], LR: [2.0]\n",
            "Loss: 0.19753722846508026\n",
            "Epoch [15/70], LR: [2.0]\n",
            "Loss: 0.14845727384090424\n",
            "Epoch [16/70], LR: [2.0]\n",
            "Loss: 0.1982109099626541\n",
            "Epoch [17/70], LR: [2.0]\n",
            "Loss: 0.19630038738250732\n",
            "Epoch [18/70], LR: [2.0]\n",
            "Loss: 0.1723148673772812\n",
            "Epoch [19/70], LR: [2.0]\n",
            "Loss: 0.16225509345531464\n",
            "Epoch [20/70], LR: [2.0]\n",
            "Loss: 0.16875731945037842\n",
            "Epoch [21/70], LR: [2.0]\n",
            "Loss: 0.15711049735546112\n",
            "Epoch [22/70], LR: [2.0]\n",
            "Loss: 0.16072894632816315\n",
            "Epoch [23/70], LR: [2.0]\n",
            "Loss: 0.14891289174556732\n",
            "Epoch [24/70], LR: [2.0]\n",
            "Loss: 0.159696564078331\n",
            "Epoch [25/70], LR: [2.0]\n",
            "Loss: 0.18513308465480804\n",
            "Epoch [26/70], LR: [2.0]\n",
            "Loss: 0.14457358419895172\n",
            "Epoch [27/70], LR: [2.0]\n",
            "Loss: 0.15785710513591766\n",
            "Epoch [28/70], LR: [2.0]\n",
            "Loss: 0.17384488880634308\n",
            "Epoch [29/70], LR: [2.0]\n",
            "Loss: 0.1843205839395523\n",
            "Epoch [30/70], LR: [2.0]\n",
            "Loss: 0.18588121235370636\n",
            "Epoch [31/70], LR: [2.0]\n",
            "Loss: 0.1643364429473877\n",
            "Epoch [32/70], LR: [2.0]\n",
            "Loss: 0.1908334493637085\n",
            "Epoch [33/70], LR: [2.0]\n",
            "Loss: 0.177348330616951\n",
            "Epoch [34/70], LR: [2.0]\n",
            "Loss: 0.1791011244058609\n",
            "Epoch [35/70], LR: [2.0]\n",
            "Loss: 0.1691409796476364\n",
            "Epoch [36/70], LR: [2.0]\n",
            "Loss: 0.18683132529258728\n",
            "Epoch [37/70], LR: [2.0]\n",
            "Loss: 0.19144831597805023\n",
            "Epoch [38/70], LR: [2.0]\n",
            "Loss: 0.16475170850753784\n",
            "Epoch [39/70], LR: [2.0]\n",
            "Loss: 0.20816972851753235\n",
            "Epoch [40/70], LR: [2.0]\n",
            "Loss: 0.17056787014007568\n",
            "Epoch [41/70], LR: [2.0]\n",
            "Loss: 0.1892746537923813\n",
            "Epoch [42/70], LR: [2.0]\n",
            "Loss: 0.14347052574157715\n",
            "Epoch [43/70], LR: [2.0]\n",
            "Loss: 0.1670948714017868\n",
            "Epoch [44/70], LR: [2.0]\n",
            "Loss: 0.12382885068655014\n",
            "Epoch [45/70], LR: [2.0]\n",
            "Loss: 0.18827736377716064\n",
            "Epoch [46/70], LR: [2.0]\n",
            "Loss: 0.2110399305820465\n",
            "Epoch [47/70], LR: [2.0]\n",
            "Loss: 0.14196859300136566\n",
            "Epoch [48/70], LR: [0.4]\n",
            "Loss: 0.17282722890377045\n",
            "Epoch [49/70], LR: [0.4]\n",
            "Loss: 0.1553361415863037\n",
            "Epoch [50/70], LR: [0.4]\n",
            "Loss: 0.17149601876735687\n",
            "Epoch [51/70], LR: [0.4]\n",
            "Loss: 0.1315133422613144\n",
            "Epoch [52/70], LR: [0.4]\n",
            "Loss: 0.16558994352817535\n",
            "Epoch [53/70], LR: [0.4]\n",
            "Loss: 0.13953828811645508\n",
            "Epoch [54/70], LR: [0.4]\n",
            "Loss: 0.11617391556501389\n",
            "Epoch [55/70], LR: [0.4]\n",
            "Loss: 0.14671306312084198\n",
            "Epoch [56/70], LR: [0.4]\n",
            "Loss: 0.1767125427722931\n",
            "Epoch [57/70], LR: [0.4]\n",
            "Loss: 0.16166305541992188\n",
            "Epoch [58/70], LR: [0.4]\n",
            "Loss: 0.17875809967517853\n",
            "Epoch [59/70], LR: [0.4]\n",
            "Loss: 0.1588004231452942\n",
            "Epoch [60/70], LR: [0.4]\n",
            "Loss: 0.15162678062915802\n",
            "Epoch [61/70], LR: [0.4]\n",
            "Loss: 0.17802849411964417\n",
            "Epoch [62/70], LR: [0.4]\n",
            "Loss: 0.15165160596370697\n",
            "Epoch [63/70], LR: [0.4]\n",
            "Loss: 0.2171870768070221\n",
            "Epoch [64/70], LR: [0.08000000000000002]\n",
            "Loss: 0.1331581324338913\n",
            "Epoch [65/70], LR: [0.08000000000000002]\n",
            "Loss: 0.13799318671226501\n",
            "Epoch [66/70], LR: [0.08000000000000002]\n",
            "Loss: 0.14240145683288574\n",
            "Epoch [67/70], LR: [0.08000000000000002]\n",
            "Loss: 0.1365596055984497\n",
            "Epoch [68/70], LR: [0.08000000000000002]\n",
            "Loss: 0.14952097833156586\n",
            "Epoch [69/70], LR: [0.08000000000000002]\n",
            "Loss: 0.23773406445980072\n",
            "Epoch [70/70], LR: [0.08000000000000002]\n",
            "Loss: 0.17000454664230347\n",
            "Validating classes ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Validating classes ['bicycle', 'plain', 'couch', 'lobster', 'lion', 'chair', 'tulip', 'television', 'skunk', 'girl'] -> [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Validating classes ['bowl', 'tank', 'lawn_mower', 'snake', 'ray', 'oak_tree', 'poppy', 'castle', 'telephone', 'clock'] -> [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "Validating classes ['worm', 'rabbit', 'tractor', 'cockroach', 'house', 'lamp', 'sweet_pepper', 'crab', 'beetle', 'dolphin'] -> [30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
            "Accuracy: 0.4755\n",
            "BATCH [4]\n",
            "Training on ['mouse', 'flatfish', 'pear', 'lizard', 'shark', 'orchid', 'cup', 'bus', 'sunflower', 'dinosaur'] -> [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
            "Known classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
            "Epoch [1/70], LR: [2.0]\n",
            "Loss: 0.209864541888237\n",
            "Epoch [2/70], LR: [2.0]\n",
            "Loss: 0.19701991975307465\n",
            "Epoch [3/70], LR: [2.0]\n",
            "Loss: 0.1490725874900818\n",
            "Epoch [4/70], LR: [2.0]\n",
            "Loss: 0.1698741316795349\n",
            "Epoch [5/70], LR: [2.0]\n",
            "Loss: 0.1959872841835022\n",
            "Epoch [6/70], LR: [2.0]\n",
            "Loss: 0.19677424430847168\n",
            "Epoch [7/70], LR: [2.0]\n",
            "Loss: 0.1827155500650406\n",
            "Epoch [8/70], LR: [2.0]\n",
            "Loss: 0.16573616862297058\n",
            "Epoch [9/70], LR: [2.0]\n",
            "Loss: 0.18377958238124847\n",
            "Epoch [10/70], LR: [2.0]\n",
            "Loss: 0.16237184405326843\n",
            "Epoch [11/70], LR: [2.0]\n",
            "Loss: 0.1662440448999405\n",
            "Epoch [12/70], LR: [2.0]\n",
            "Loss: 0.18413041532039642\n",
            "Epoch [13/70], LR: [2.0]\n",
            "Loss: 0.1870686113834381\n",
            "Epoch [14/70], LR: [2.0]\n",
            "Loss: 0.16384638845920563\n",
            "Epoch [15/70], LR: [2.0]\n",
            "Loss: 0.13624146580696106\n",
            "Epoch [16/70], LR: [2.0]\n",
            "Loss: 0.22320206463336945\n",
            "Epoch [17/70], LR: [2.0]\n",
            "Loss: 0.1501689851284027\n",
            "Epoch [18/70], LR: [2.0]\n",
            "Loss: 0.1567964255809784\n",
            "Epoch [19/70], LR: [2.0]\n",
            "Loss: 0.17948909103870392\n",
            "Epoch [20/70], LR: [2.0]\n",
            "Loss: 0.158511221408844\n",
            "Epoch [21/70], LR: [2.0]\n",
            "Loss: 0.1547229140996933\n",
            "Epoch [22/70], LR: [2.0]\n",
            "Loss: 0.1369544416666031\n",
            "Epoch [23/70], LR: [2.0]\n",
            "Loss: 0.19484488666057587\n",
            "Epoch [24/70], LR: [2.0]\n",
            "Loss: 0.1566125452518463\n",
            "Epoch [25/70], LR: [2.0]\n",
            "Loss: 0.19986434280872345\n",
            "Epoch [26/70], LR: [2.0]\n",
            "Loss: 0.15911132097244263\n",
            "Epoch [27/70], LR: [2.0]\n",
            "Loss: 0.13988450169563293\n",
            "Epoch [28/70], LR: [2.0]\n",
            "Loss: 0.13390082120895386\n",
            "Epoch [29/70], LR: [2.0]\n",
            "Loss: 0.2029205858707428\n",
            "Epoch [30/70], LR: [2.0]\n",
            "Loss: 0.15954943001270294\n",
            "Epoch [31/70], LR: [2.0]\n",
            "Loss: 0.16085006296634674\n",
            "Epoch [32/70], LR: [2.0]\n",
            "Loss: 0.16725073754787445\n",
            "Epoch [33/70], LR: [2.0]\n",
            "Loss: 0.13897885382175446\n",
            "Epoch [34/70], LR: [2.0]\n",
            "Loss: 0.1679191142320633\n",
            "Epoch [35/70], LR: [2.0]\n",
            "Loss: 0.18754462897777557\n",
            "Epoch [36/70], LR: [2.0]\n",
            "Loss: 0.15519364178180695\n",
            "Epoch [37/70], LR: [2.0]\n",
            "Loss: 0.1469709724187851\n",
            "Epoch [38/70], LR: [2.0]\n",
            "Loss: 0.18529701232910156\n",
            "Epoch [39/70], LR: [2.0]\n",
            "Loss: 0.1804097294807434\n",
            "Epoch [40/70], LR: [2.0]\n",
            "Loss: 0.1946018934249878\n",
            "Epoch [41/70], LR: [2.0]\n",
            "Loss: 0.1883196383714676\n",
            "Epoch [42/70], LR: [2.0]\n",
            "Loss: 0.16794933378696442\n",
            "Epoch [43/70], LR: [2.0]\n",
            "Loss: 0.1787736713886261\n",
            "Epoch [44/70], LR: [2.0]\n",
            "Loss: 0.15422135591506958\n",
            "Epoch [45/70], LR: [2.0]\n",
            "Loss: 0.1629534512758255\n",
            "Epoch [46/70], LR: [2.0]\n",
            "Loss: 0.17438837885856628\n",
            "Epoch [47/70], LR: [2.0]\n",
            "Loss: 0.1400197595357895\n",
            "Epoch [48/70], LR: [0.4]\n",
            "Loss: 0.13282032310962677\n",
            "Epoch [49/70], LR: [0.4]\n",
            "Loss: 0.19674161076545715\n",
            "Epoch [50/70], LR: [0.4]\n",
            "Loss: 0.14234255254268646\n",
            "Epoch [51/70], LR: [0.4]\n",
            "Loss: 0.1568111926317215\n",
            "Epoch [52/70], LR: [0.4]\n",
            "Loss: 0.16483233869075775\n",
            "Epoch [53/70], LR: [0.4]\n",
            "Loss: 0.16560180485248566\n",
            "Epoch [54/70], LR: [0.4]\n",
            "Loss: 0.15430404245853424\n",
            "Epoch [55/70], LR: [0.4]\n",
            "Loss: 0.1625867635011673\n",
            "Epoch [56/70], LR: [0.4]\n",
            "Loss: 0.17894351482391357\n",
            "Epoch [57/70], LR: [0.4]\n",
            "Loss: 0.1382257491350174\n",
            "Epoch [58/70], LR: [0.4]\n",
            "Loss: 0.14517544209957123\n",
            "Epoch [59/70], LR: [0.4]\n",
            "Loss: 0.14224211871623993\n",
            "Epoch [60/70], LR: [0.4]\n",
            "Loss: 0.12173867970705032\n",
            "Epoch [61/70], LR: [0.4]\n",
            "Loss: 0.15769250690937042\n",
            "Epoch [62/70], LR: [0.4]\n",
            "Loss: 0.16091056168079376\n",
            "Epoch [63/70], LR: [0.4]\n",
            "Loss: 0.2067510485649109\n",
            "Epoch [64/70], LR: [0.08000000000000002]\n",
            "Loss: 0.15315744280815125\n",
            "Epoch [65/70], LR: [0.08000000000000002]\n",
            "Loss: 0.14234350621700287\n",
            "Epoch [66/70], LR: [0.08000000000000002]\n",
            "Loss: 0.1329239308834076\n",
            "Epoch [67/70], LR: [0.08000000000000002]\n",
            "Loss: 0.14548566937446594\n",
            "Epoch [68/70], LR: [0.08000000000000002]\n",
            "Loss: 0.14028705656528473\n",
            "Epoch [69/70], LR: [0.08000000000000002]\n",
            "Loss: 0.12909018993377686\n",
            "Epoch [70/70], LR: [0.08000000000000002]\n",
            "Loss: 0.16239053010940552\n",
            "Validating classes ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Validating classes ['bicycle', 'plain', 'couch', 'lobster', 'lion', 'chair', 'tulip', 'television', 'skunk', 'girl'] -> [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Validating classes ['bowl', 'tank', 'lawn_mower', 'snake', 'ray', 'oak_tree', 'poppy', 'castle', 'telephone', 'clock'] -> [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "Validating classes ['worm', 'rabbit', 'tractor', 'cockroach', 'house', 'lamp', 'sweet_pepper', 'crab', 'beetle', 'dolphin'] -> [30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
            "Validating classes ['mouse', 'flatfish', 'pear', 'lizard', 'shark', 'orchid', 'cup', 'bus', 'sunflower', 'dinosaur'] -> [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
            "Accuracy: 0.383\n",
            "BATCH [5]\n",
            "Training on ['whale', 'wolf', 'woman', 'cloud', 'porcupine', 'road', 'plate', 'table', 'sea', 'seal'] -> [50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
            "Known classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
            "Epoch [1/70], LR: [2.0]\n",
            "Loss: 0.196896493434906\n",
            "Epoch [2/70], LR: [2.0]\n",
            "Loss: 0.162576824426651\n",
            "Epoch [3/70], LR: [2.0]\n",
            "Loss: 0.1868281215429306\n",
            "Epoch [4/70], LR: [2.0]\n",
            "Loss: 0.16385896503925323\n",
            "Epoch [5/70], LR: [2.0]\n",
            "Loss: 0.1985386312007904\n",
            "Epoch [6/70], LR: [2.0]\n",
            "Loss: 0.15580815076828003\n",
            "Epoch [7/70], LR: [2.0]\n",
            "Loss: 0.1710803061723709\n",
            "Epoch [8/70], LR: [2.0]\n",
            "Loss: 0.20536090433597565\n",
            "Epoch [9/70], LR: [2.0]\n",
            "Loss: 0.1422349363565445\n",
            "Epoch [10/70], LR: [2.0]\n",
            "Loss: 0.19333671033382416\n",
            "Epoch [11/70], LR: [2.0]\n",
            "Loss: 0.18267574906349182\n",
            "Epoch [12/70], LR: [2.0]\n",
            "Loss: 0.19697561860084534\n",
            "Epoch [13/70], LR: [2.0]\n",
            "Loss: 0.17085742950439453\n",
            "Epoch [14/70], LR: [2.0]\n",
            "Loss: 0.2850116491317749\n",
            "Epoch [15/70], LR: [2.0]\n",
            "Loss: 0.15575085580348969\n",
            "Epoch [16/70], LR: [2.0]\n",
            "Loss: 0.17029595375061035\n",
            "Epoch [17/70], LR: [2.0]\n",
            "Loss: 0.15569818019866943\n",
            "Epoch [18/70], LR: [2.0]\n",
            "Loss: 0.1679246723651886\n",
            "Epoch [19/70], LR: [2.0]\n",
            "Loss: 0.15127979218959808\n",
            "Epoch [20/70], LR: [2.0]\n",
            "Loss: 0.1435447633266449\n",
            "Epoch [21/70], LR: [2.0]\n",
            "Loss: 0.1549893617630005\n",
            "Epoch [22/70], LR: [2.0]\n",
            "Loss: 0.21421115100383759\n",
            "Epoch [23/70], LR: [2.0]\n",
            "Loss: 0.2550312876701355\n",
            "Epoch [24/70], LR: [2.0]\n",
            "Loss: 0.22635045647621155\n",
            "Epoch [25/70], LR: [2.0]\n",
            "Loss: 0.164541095495224\n",
            "Epoch [26/70], LR: [2.0]\n",
            "Loss: 0.20599977672100067\n",
            "Epoch [27/70], LR: [2.0]\n",
            "Loss: 0.1645902395248413\n",
            "Epoch [28/70], LR: [2.0]\n",
            "Loss: 0.16758303344249725\n",
            "Epoch [29/70], LR: [2.0]\n",
            "Loss: 0.15936614573001862\n",
            "Epoch [30/70], LR: [2.0]\n",
            "Loss: 0.15970323979854584\n",
            "Epoch [31/70], LR: [2.0]\n",
            "Loss: 0.157409206032753\n",
            "Epoch [32/70], LR: [2.0]\n",
            "Loss: 0.1658555120229721\n",
            "Epoch [33/70], LR: [2.0]\n",
            "Loss: 0.1619427502155304\n",
            "Epoch [34/70], LR: [2.0]\n",
            "Loss: 0.1588425487279892\n",
            "Epoch [35/70], LR: [2.0]\n",
            "Loss: 0.1687231957912445\n",
            "Epoch [36/70], LR: [2.0]\n",
            "Loss: 0.15121448040008545\n",
            "Epoch [37/70], LR: [2.0]\n",
            "Loss: 0.14124400913715363\n",
            "Epoch [38/70], LR: [2.0]\n",
            "Loss: 0.15614847838878632\n",
            "Epoch [39/70], LR: [2.0]\n",
            "Loss: 0.14987920224666595\n",
            "Epoch [40/70], LR: [2.0]\n",
            "Loss: 0.14576663076877594\n",
            "Epoch [41/70], LR: [2.0]\n",
            "Loss: 0.15820328891277313\n",
            "Epoch [42/70], LR: [2.0]\n",
            "Loss: 0.16755391657352448\n",
            "Epoch [43/70], LR: [2.0]\n",
            "Loss: 0.14867188036441803\n",
            "Epoch [44/70], LR: [2.0]\n",
            "Loss: 0.13943904638290405\n",
            "Epoch [45/70], LR: [2.0]\n",
            "Loss: 0.16474857926368713\n",
            "Epoch [46/70], LR: [2.0]\n",
            "Loss: 0.19326956570148468\n",
            "Epoch [47/70], LR: [2.0]\n",
            "Loss: 0.1535450965166092\n",
            "Epoch [48/70], LR: [0.4]\n",
            "Loss: 0.16676592826843262\n",
            "Epoch [49/70], LR: [0.4]\n",
            "Loss: 0.1633194088935852\n",
            "Epoch [50/70], LR: [0.4]\n",
            "Loss: 0.16456839442253113\n",
            "Epoch [51/70], LR: [0.4]\n",
            "Loss: 0.16876107454299927\n",
            "Epoch [52/70], LR: [0.4]\n",
            "Loss: 0.1641658991575241\n",
            "Epoch [53/70], LR: [0.4]\n",
            "Loss: 0.17614032328128815\n",
            "Epoch [54/70], LR: [0.4]\n",
            "Loss: 0.16673000156879425\n",
            "Epoch [55/70], LR: [0.4]\n",
            "Loss: 0.14765293896198273\n",
            "Epoch [56/70], LR: [0.4]\n",
            "Loss: 0.1715608537197113\n",
            "Epoch [57/70], LR: [0.4]\n",
            "Loss: 0.152170330286026\n",
            "Epoch [58/70], LR: [0.4]\n",
            "Loss: 0.18592648208141327\n",
            "Epoch [59/70], LR: [0.4]\n",
            "Loss: 0.15350952744483948\n",
            "Epoch [60/70], LR: [0.4]\n",
            "Loss: 0.1438421607017517\n",
            "Epoch [61/70], LR: [0.4]\n",
            "Loss: 0.15062250196933746\n",
            "Epoch [62/70], LR: [0.4]\n",
            "Loss: 0.14517079293727875\n",
            "Epoch [63/70], LR: [0.4]\n",
            "Loss: 0.17713096737861633\n",
            "Epoch [64/70], LR: [0.08000000000000002]\n",
            "Loss: 0.16038917005062103\n",
            "Epoch [65/70], LR: [0.08000000000000002]\n",
            "Loss: 0.1693410575389862\n",
            "Epoch [66/70], LR: [0.08000000000000002]\n",
            "Loss: 0.13596472144126892\n",
            "Epoch [67/70], LR: [0.08000000000000002]\n",
            "Loss: 0.1475585401058197\n",
            "Epoch [68/70], LR: [0.08000000000000002]\n",
            "Loss: 0.16319316625595093\n",
            "Epoch [69/70], LR: [0.08000000000000002]\n",
            "Loss: 0.19341909885406494\n",
            "Epoch [70/70], LR: [0.08000000000000002]\n",
            "Loss: 0.17805643379688263\n",
            "Validating classes ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Validating classes ['bicycle', 'plain', 'couch', 'lobster', 'lion', 'chair', 'tulip', 'television', 'skunk', 'girl'] -> [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Validating classes ['bowl', 'tank', 'lawn_mower', 'snake', 'ray', 'oak_tree', 'poppy', 'castle', 'telephone', 'clock'] -> [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "Validating classes ['worm', 'rabbit', 'tractor', 'cockroach', 'house', 'lamp', 'sweet_pepper', 'crab', 'beetle', 'dolphin'] -> [30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
            "Validating classes ['mouse', 'flatfish', 'pear', 'lizard', 'shark', 'orchid', 'cup', 'bus', 'sunflower', 'dinosaur'] -> [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
            "Validating classes ['whale', 'wolf', 'woman', 'cloud', 'porcupine', 'road', 'plate', 'table', 'sea', 'seal'] -> [50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
            "Accuracy: 0.3115\n",
            "BATCH [6]\n",
            "Training on ['bear', 'apple', 'forest', 'streetcar', 'can', 'bed', 'crocodile', 'keyboard', 'boy', 'raccoon'] -> [60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
            "Known classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
            "Epoch [1/70], LR: [2.0]\n",
            "Loss: 0.2019950896501541\n",
            "Epoch [2/70], LR: [2.0]\n",
            "Loss: 0.18019823729991913\n",
            "Epoch [3/70], LR: [2.0]\n",
            "Loss: 0.16832923889160156\n",
            "Epoch [4/70], LR: [2.0]\n",
            "Loss: 0.1772531270980835\n",
            "Epoch [5/70], LR: [2.0]\n",
            "Loss: 0.18002387881278992\n",
            "Epoch [6/70], LR: [2.0]\n",
            "Loss: 0.16862301528453827\n",
            "Epoch [7/70], LR: [2.0]\n",
            "Loss: 0.16336871683597565\n",
            "Epoch [8/70], LR: [2.0]\n",
            "Loss: 0.19345007836818695\n",
            "Epoch [9/70], LR: [2.0]\n",
            "Loss: 0.17659297585487366\n",
            "Epoch [10/70], LR: [2.0]\n",
            "Loss: 0.19479313492774963\n",
            "Epoch [11/70], LR: [2.0]\n",
            "Loss: 0.19812998175621033\n",
            "Epoch [12/70], LR: [2.0]\n",
            "Loss: 0.2073075771331787\n",
            "Epoch [13/70], LR: [2.0]\n",
            "Loss: 0.15169879794120789\n",
            "Epoch [14/70], LR: [2.0]\n",
            "Loss: 0.19215022027492523\n",
            "Epoch [15/70], LR: [2.0]\n",
            "Loss: 0.1831226348876953\n",
            "Epoch [16/70], LR: [2.0]\n",
            "Loss: 0.18489067256450653\n",
            "Epoch [17/70], LR: [2.0]\n",
            "Loss: 0.15798549354076385\n",
            "Epoch [18/70], LR: [2.0]\n",
            "Loss: 0.1732240468263626\n",
            "Epoch [19/70], LR: [2.0]\n",
            "Loss: 0.1694200187921524\n",
            "Epoch [20/70], LR: [2.0]\n",
            "Loss: 0.17367392778396606\n",
            "Epoch [21/70], LR: [2.0]\n",
            "Loss: 0.19150571525096893\n",
            "Epoch [22/70], LR: [2.0]\n",
            "Loss: 0.16235435009002686\n",
            "Epoch [23/70], LR: [2.0]\n",
            "Loss: 0.1699213981628418\n",
            "Epoch [24/70], LR: [2.0]\n",
            "Loss: 0.16881534457206726\n",
            "Epoch [25/70], LR: [2.0]\n",
            "Loss: 0.1973477452993393\n",
            "Epoch [26/70], LR: [2.0]\n",
            "Loss: 0.1659756749868393\n",
            "Epoch [27/70], LR: [2.0]\n",
            "Loss: 0.18639366328716278\n",
            "Epoch [28/70], LR: [2.0]\n",
            "Loss: 0.1735588014125824\n",
            "Epoch [29/70], LR: [2.0]\n",
            "Loss: 0.16705019772052765\n",
            "Epoch [30/70], LR: [2.0]\n",
            "Loss: 0.15767093002796173\n",
            "Epoch [31/70], LR: [2.0]\n",
            "Loss: 0.16853003203868866\n",
            "Epoch [32/70], LR: [2.0]\n",
            "Loss: 0.18707060813903809\n",
            "Epoch [33/70], LR: [2.0]\n",
            "Loss: 0.16631881892681122\n",
            "Epoch [34/70], LR: [2.0]\n",
            "Loss: 0.21187976002693176\n",
            "Epoch [35/70], LR: [2.0]\n",
            "Loss: 0.18414461612701416\n",
            "Epoch [36/70], LR: [2.0]\n",
            "Loss: 0.2073325663805008\n",
            "Epoch [37/70], LR: [2.0]\n",
            "Loss: 0.17998404800891876\n",
            "Epoch [38/70], LR: [2.0]\n",
            "Loss: 0.22387641668319702\n",
            "Epoch [39/70], LR: [2.0]\n",
            "Loss: 0.20031340420246124\n",
            "Epoch [40/70], LR: [2.0]\n",
            "Loss: 0.17896991968154907\n",
            "Epoch [41/70], LR: [2.0]\n",
            "Loss: 0.1812208741903305\n",
            "Epoch [42/70], LR: [2.0]\n",
            "Loss: 0.18167634308338165\n",
            "Epoch [43/70], LR: [2.0]\n",
            "Loss: 0.16966786980628967\n",
            "Epoch [44/70], LR: [2.0]\n",
            "Loss: 0.15914179384708405\n",
            "Epoch [45/70], LR: [2.0]\n",
            "Loss: 0.1838735193014145\n",
            "Epoch [46/70], LR: [2.0]\n",
            "Loss: 0.15399296581745148\n",
            "Epoch [47/70], LR: [2.0]\n",
            "Loss: 0.19952137768268585\n",
            "Epoch [48/70], LR: [0.4]\n",
            "Loss: 0.16179585456848145\n",
            "Epoch [49/70], LR: [0.4]\n",
            "Loss: 0.1762949377298355\n",
            "Epoch [50/70], LR: [0.4]\n",
            "Loss: 0.17586731910705566\n",
            "Epoch [51/70], LR: [0.4]\n",
            "Loss: 0.1726839393377304\n",
            "Epoch [52/70], LR: [0.4]\n",
            "Loss: 0.18444162607192993\n",
            "Epoch [53/70], LR: [0.4]\n",
            "Loss: 0.15787817537784576\n",
            "Epoch [54/70], LR: [0.4]\n",
            "Loss: 0.18796727061271667\n",
            "Epoch [55/70], LR: [0.4]\n",
            "Loss: 0.16855166852474213\n",
            "Epoch [56/70], LR: [0.4]\n",
            "Loss: 0.1600182056427002\n",
            "Epoch [57/70], LR: [0.4]\n",
            "Loss: 0.2183130830526352\n",
            "Epoch [58/70], LR: [0.4]\n",
            "Loss: 0.16633519530296326\n",
            "Epoch [59/70], LR: [0.4]\n",
            "Loss: 0.16297046840190887\n",
            "Epoch [60/70], LR: [0.4]\n",
            "Loss: 0.16600467264652252\n",
            "Epoch [61/70], LR: [0.4]\n",
            "Loss: 0.17190614342689514\n",
            "Epoch [62/70], LR: [0.4]\n",
            "Loss: 0.19371213018894196\n",
            "Epoch [63/70], LR: [0.4]\n",
            "Loss: 0.14323878288269043\n",
            "Epoch [64/70], LR: [0.08000000000000002]\n",
            "Loss: 0.1869044154882431\n",
            "Epoch [65/70], LR: [0.08000000000000002]\n",
            "Loss: 0.19973477721214294\n",
            "Epoch [66/70], LR: [0.08000000000000002]\n",
            "Loss: 0.212276428937912\n",
            "Epoch [67/70], LR: [0.08000000000000002]\n",
            "Loss: 0.15110312402248383\n",
            "Epoch [68/70], LR: [0.08000000000000002]\n",
            "Loss: 0.16433529555797577\n",
            "Epoch [69/70], LR: [0.08000000000000002]\n",
            "Loss: 0.15539544820785522\n",
            "Epoch [70/70], LR: [0.08000000000000002]\n",
            "Loss: 0.20467987656593323\n",
            "Validating classes ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Validating classes ['bicycle', 'plain', 'couch', 'lobster', 'lion', 'chair', 'tulip', 'television', 'skunk', 'girl'] -> [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Validating classes ['bowl', 'tank', 'lawn_mower', 'snake', 'ray', 'oak_tree', 'poppy', 'castle', 'telephone', 'clock'] -> [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "Validating classes ['worm', 'rabbit', 'tractor', 'cockroach', 'house', 'lamp', 'sweet_pepper', 'crab', 'beetle', 'dolphin'] -> [30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
            "Validating classes ['mouse', 'flatfish', 'pear', 'lizard', 'shark', 'orchid', 'cup', 'bus', 'sunflower', 'dinosaur'] -> [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
            "Validating classes ['whale', 'wolf', 'woman', 'cloud', 'porcupine', 'road', 'plate', 'table', 'sea', 'seal'] -> [50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
            "Validating classes ['bear', 'apple', 'forest', 'streetcar', 'can', 'bed', 'crocodile', 'keyboard', 'boy', 'raccoon'] -> [60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
            "Accuracy: 0.2875714285714286\n",
            "BATCH [7]\n",
            "Training on ['willow_tree', 'maple_tree', 'orange', 'rocket', 'spider', 'chimpanzee', 'cattle', 'kangaroo', 'bridge', 'fox'] -> [70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
            "Known classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
            "Epoch [1/70], LR: [2.0]\n",
            "Loss: 0.18230652809143066\n",
            "Epoch [2/70], LR: [2.0]\n",
            "Loss: 0.20319309830665588\n",
            "Epoch [3/70], LR: [2.0]\n",
            "Loss: 0.18473993241786957\n",
            "Epoch [4/70], LR: [2.0]\n",
            "Loss: 0.18262262642383575\n",
            "Epoch [5/70], LR: [2.0]\n",
            "Loss: 0.17037785053253174\n",
            "Epoch [6/70], LR: [2.0]\n",
            "Loss: 0.17986729741096497\n",
            "Epoch [7/70], LR: [2.0]\n",
            "Loss: 0.15567730367183685\n",
            "Epoch [8/70], LR: [2.0]\n",
            "Loss: 0.1898934543132782\n",
            "Epoch [9/70], LR: [2.0]\n",
            "Loss: 0.1810828596353531\n",
            "Epoch [10/70], LR: [2.0]\n",
            "Loss: 0.16163189709186554\n",
            "Epoch [11/70], LR: [2.0]\n",
            "Loss: 0.18528221547603607\n",
            "Epoch [12/70], LR: [2.0]\n",
            "Loss: 0.19410714507102966\n",
            "Epoch [13/70], LR: [2.0]\n",
            "Loss: 0.18282990157604218\n",
            "Epoch [14/70], LR: [2.0]\n",
            "Loss: 0.2008194476366043\n",
            "Epoch [15/70], LR: [2.0]\n",
            "Loss: 0.17168156802654266\n",
            "Epoch [16/70], LR: [2.0]\n",
            "Loss: 0.18493865430355072\n",
            "Epoch [17/70], LR: [2.0]\n",
            "Loss: 0.19132836163043976\n",
            "Epoch [18/70], LR: [2.0]\n",
            "Loss: 0.1844794601202011\n",
            "Epoch [19/70], LR: [2.0]\n",
            "Loss: 0.1860068440437317\n",
            "Epoch [20/70], LR: [2.0]\n",
            "Loss: 0.2007128745317459\n",
            "Epoch [21/70], LR: [2.0]\n",
            "Loss: 0.18885330855846405\n",
            "Epoch [22/70], LR: [2.0]\n",
            "Loss: 0.1733422875404358\n",
            "Epoch [23/70], LR: [2.0]\n",
            "Loss: 0.17449873685836792\n",
            "Epoch [24/70], LR: [2.0]\n",
            "Loss: 0.1894311010837555\n",
            "Epoch [25/70], LR: [2.0]\n",
            "Loss: 0.17939122021198273\n",
            "Epoch [26/70], LR: [2.0]\n",
            "Loss: 0.176292285323143\n",
            "Epoch [27/70], LR: [2.0]\n",
            "Loss: 0.16367636620998383\n",
            "Epoch [28/70], LR: [2.0]\n",
            "Loss: 0.1715593785047531\n",
            "Epoch [29/70], LR: [2.0]\n",
            "Loss: 0.17598752677440643\n",
            "Epoch [30/70], LR: [2.0]\n",
            "Loss: 0.17901575565338135\n",
            "Epoch [31/70], LR: [2.0]\n",
            "Loss: 0.1814100295305252\n",
            "Epoch [32/70], LR: [2.0]\n",
            "Loss: 0.1868804544210434\n",
            "Epoch [33/70], LR: [2.0]\n",
            "Loss: 0.1908596009016037\n",
            "Epoch [34/70], LR: [2.0]\n",
            "Loss: 0.17883183062076569\n",
            "Epoch [35/70], LR: [2.0]\n",
            "Loss: 0.2006610482931137\n",
            "Epoch [36/70], LR: [2.0]\n",
            "Loss: 0.1670745313167572\n",
            "Epoch [37/70], LR: [2.0]\n",
            "Loss: 0.18434976041316986\n",
            "Epoch [38/70], LR: [2.0]\n",
            "Loss: 0.16253985464572906\n",
            "Epoch [39/70], LR: [2.0]\n",
            "Loss: 0.1681496948003769\n",
            "Epoch [40/70], LR: [2.0]\n",
            "Loss: 0.16030704975128174\n",
            "Epoch [41/70], LR: [2.0]\n",
            "Loss: 0.16478364169597626\n",
            "Epoch [42/70], LR: [2.0]\n",
            "Loss: 0.19006016850471497\n",
            "Epoch [43/70], LR: [2.0]\n",
            "Loss: 0.20494237542152405\n",
            "Epoch [44/70], LR: [2.0]\n",
            "Loss: 0.17309433221817017\n",
            "Epoch [45/70], LR: [2.0]\n",
            "Loss: 0.17452095448970795\n",
            "Epoch [46/70], LR: [2.0]\n",
            "Loss: 0.19194236397743225\n",
            "Epoch [47/70], LR: [2.0]\n",
            "Loss: 0.1754634976387024\n",
            "Epoch [48/70], LR: [0.4]\n",
            "Loss: 0.17813518643379211\n",
            "Epoch [49/70], LR: [0.4]\n",
            "Loss: 0.16806773841381073\n",
            "Epoch [50/70], LR: [0.4]\n",
            "Loss: 0.17361557483673096\n",
            "Epoch [51/70], LR: [0.4]\n",
            "Loss: 0.19237880408763885\n",
            "Epoch [52/70], LR: [0.4]\n",
            "Loss: 0.1681666225194931\n",
            "Epoch [53/70], LR: [0.4]\n",
            "Loss: 0.17105984687805176\n",
            "Epoch [54/70], LR: [0.4]\n",
            "Loss: 0.18575772643089294\n",
            "Epoch [55/70], LR: [0.4]\n",
            "Loss: 0.18266163766384125\n",
            "Epoch [56/70], LR: [0.4]\n",
            "Loss: 0.17143647372722626\n",
            "Epoch [57/70], LR: [0.4]\n",
            "Loss: 0.15382838249206543\n",
            "Epoch [58/70], LR: [0.4]\n",
            "Loss: 0.1600305438041687\n",
            "Epoch [59/70], LR: [0.4]\n",
            "Loss: 0.16473488509655\n",
            "Epoch [60/70], LR: [0.4]\n",
            "Loss: 0.14878487586975098\n",
            "Epoch [61/70], LR: [0.4]\n",
            "Loss: 0.1731308251619339\n",
            "Epoch [62/70], LR: [0.4]\n",
            "Loss: 0.16630449891090393\n",
            "Epoch [63/70], LR: [0.4]\n",
            "Loss: 0.17904134094715118\n",
            "Epoch [64/70], LR: [0.08000000000000002]\n",
            "Loss: 0.17056266963481903\n",
            "Epoch [65/70], LR: [0.08000000000000002]\n",
            "Loss: 0.1771463006734848\n",
            "Epoch [66/70], LR: [0.08000000000000002]\n",
            "Loss: 0.16802804172039032\n",
            "Epoch [67/70], LR: [0.08000000000000002]\n",
            "Loss: 0.18418145179748535\n",
            "Epoch [68/70], LR: [0.08000000000000002]\n",
            "Loss: 0.16044984757900238\n",
            "Epoch [69/70], LR: [0.08000000000000002]\n",
            "Loss: 0.16664904356002808\n",
            "Epoch [70/70], LR: [0.08000000000000002]\n",
            "Loss: 0.17542223632335663\n",
            "Validating classes ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Validating classes ['bicycle', 'plain', 'couch', 'lobster', 'lion', 'chair', 'tulip', 'television', 'skunk', 'girl'] -> [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Validating classes ['bowl', 'tank', 'lawn_mower', 'snake', 'ray', 'oak_tree', 'poppy', 'castle', 'telephone', 'clock'] -> [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "Validating classes ['worm', 'rabbit', 'tractor', 'cockroach', 'house', 'lamp', 'sweet_pepper', 'crab', 'beetle', 'dolphin'] -> [30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
            "Validating classes ['mouse', 'flatfish', 'pear', 'lizard', 'shark', 'orchid', 'cup', 'bus', 'sunflower', 'dinosaur'] -> [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
            "Validating classes ['whale', 'wolf', 'woman', 'cloud', 'porcupine', 'road', 'plate', 'table', 'sea', 'seal'] -> [50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
            "Validating classes ['bear', 'apple', 'forest', 'streetcar', 'can', 'bed', 'crocodile', 'keyboard', 'boy', 'raccoon'] -> [60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
            "Validating classes ['willow_tree', 'maple_tree', 'orange', 'rocket', 'spider', 'chimpanzee', 'cattle', 'kangaroo', 'bridge', 'fox'] -> [70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
            "Accuracy: 0.252625\n",
            "BATCH [8]\n",
            "Training on ['butterfly', 'baby', 'elephant', 'shrew', 'pine_tree', 'squirrel', 'mountain', 'caterpillar', 'bee', 'camel'] -> [80, 81, 82, 83, 84, 85, 86, 87, 88, 89]\n",
            "Known classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]\n",
            "Epoch [1/70], LR: [2.0]\n",
            "Loss: 0.2036147266626358\n",
            "Epoch [2/70], LR: [2.0]\n",
            "Loss: 0.20561303198337555\n",
            "Epoch [3/70], LR: [2.0]\n",
            "Loss: 0.18782083690166473\n",
            "Epoch [4/70], LR: [2.0]\n",
            "Loss: 0.2197861224412918\n",
            "Epoch [5/70], LR: [2.0]\n",
            "Loss: 0.18983307480812073\n",
            "Epoch [6/70], LR: [2.0]\n",
            "Loss: 0.18162310123443604\n",
            "Epoch [7/70], LR: [2.0]\n",
            "Loss: 0.18378323316574097\n",
            "Epoch [8/70], LR: [2.0]\n",
            "Loss: 0.17232416570186615\n",
            "Epoch [9/70], LR: [2.0]\n",
            "Loss: 0.17887119948863983\n",
            "Epoch [10/70], LR: [2.0]\n",
            "Loss: 0.17889609932899475\n",
            "Epoch [11/70], LR: [2.0]\n",
            "Loss: 0.17240922152996063\n",
            "Epoch [12/70], LR: [2.0]\n",
            "Loss: 0.1831018477678299\n",
            "Epoch [13/70], LR: [2.0]\n",
            "Loss: 0.2116757333278656\n",
            "Epoch [14/70], LR: [2.0]\n",
            "Loss: 0.18650567531585693\n",
            "Epoch [15/70], LR: [2.0]\n",
            "Loss: 0.19728730618953705\n",
            "Epoch [16/70], LR: [2.0]\n",
            "Loss: 0.19683775305747986\n",
            "Epoch [17/70], LR: [2.0]\n",
            "Loss: 0.19173671305179596\n",
            "Epoch [18/70], LR: [2.0]\n",
            "Loss: 0.18981800973415375\n",
            "Epoch [19/70], LR: [2.0]\n",
            "Loss: 0.1723344624042511\n",
            "Epoch [20/70], LR: [2.0]\n",
            "Loss: 0.17142394185066223\n",
            "Epoch [21/70], LR: [2.0]\n",
            "Loss: 0.20316509902477264\n",
            "Epoch [22/70], LR: [2.0]\n",
            "Loss: 0.19137129187583923\n",
            "Epoch [23/70], LR: [2.0]\n",
            "Loss: 0.16811560094356537\n",
            "Epoch [24/70], LR: [2.0]\n",
            "Loss: 0.17195342481136322\n",
            "Epoch [25/70], LR: [2.0]\n",
            "Loss: 0.18563856184482574\n",
            "Epoch [26/70], LR: [2.0]\n",
            "Loss: 0.16717229783535004\n",
            "Epoch [27/70], LR: [2.0]\n",
            "Loss: 0.1656138151884079\n",
            "Epoch [28/70], LR: [2.0]\n",
            "Loss: 0.17631064355373383\n",
            "Epoch [29/70], LR: [2.0]\n",
            "Loss: 0.15720653533935547\n",
            "Epoch [30/70], LR: [2.0]\n",
            "Loss: 0.1771160513162613\n",
            "Epoch [31/70], LR: [2.0]\n",
            "Loss: 0.18243375420570374\n",
            "Epoch [32/70], LR: [2.0]\n",
            "Loss: 0.1789698302745819\n",
            "Epoch [33/70], LR: [2.0]\n",
            "Loss: 0.199850931763649\n",
            "Epoch [34/70], LR: [2.0]\n",
            "Loss: 0.17434240877628326\n",
            "Epoch [35/70], LR: [2.0]\n",
            "Loss: 0.19791650772094727\n",
            "Epoch [36/70], LR: [2.0]\n",
            "Loss: 0.18764817714691162\n",
            "Epoch [37/70], LR: [2.0]\n",
            "Loss: 0.17358092963695526\n",
            "Epoch [38/70], LR: [2.0]\n",
            "Loss: 0.17237937450408936\n",
            "Epoch [39/70], LR: [2.0]\n",
            "Loss: 0.19498476386070251\n",
            "Epoch [40/70], LR: [2.0]\n",
            "Loss: 0.1729825735092163\n",
            "Epoch [41/70], LR: [2.0]\n",
            "Loss: 0.1820107400417328\n",
            "Epoch [42/70], LR: [2.0]\n",
            "Loss: 0.17189131677150726\n",
            "Epoch [43/70], LR: [2.0]\n",
            "Loss: 0.20194214582443237\n",
            "Epoch [44/70], LR: [2.0]\n",
            "Loss: 0.15986154973506927\n",
            "Epoch [45/70], LR: [2.0]\n",
            "Loss: 0.16563676297664642\n",
            "Epoch [46/70], LR: [2.0]\n",
            "Loss: 0.18581771850585938\n",
            "Epoch [47/70], LR: [2.0]\n",
            "Loss: 0.1777981072664261\n",
            "Epoch [48/70], LR: [0.4]\n",
            "Loss: 0.18647369742393494\n",
            "Epoch [49/70], LR: [0.4]\n",
            "Loss: 0.16535373032093048\n",
            "Epoch [50/70], LR: [0.4]\n",
            "Loss: 0.18695171177387238\n",
            "Epoch [51/70], LR: [0.4]\n",
            "Loss: 0.16852974891662598\n",
            "Epoch [52/70], LR: [0.4]\n",
            "Loss: 0.1975194811820984\n",
            "Epoch [53/70], LR: [0.4]\n",
            "Loss: 0.18016417324543\n",
            "Epoch [54/70], LR: [0.4]\n",
            "Loss: 0.18283693492412567\n",
            "Epoch [55/70], LR: [0.4]\n",
            "Loss: 0.17205041646957397\n",
            "Epoch [56/70], LR: [0.4]\n",
            "Loss: 0.17412368953227997\n",
            "Epoch [57/70], LR: [0.4]\n",
            "Loss: 0.17192208766937256\n",
            "Epoch [58/70], LR: [0.4]\n",
            "Loss: 0.17047861218452454\n",
            "Epoch [59/70], LR: [0.4]\n",
            "Loss: 0.17230212688446045\n",
            "Epoch [60/70], LR: [0.4]\n",
            "Loss: 0.18759651482105255\n",
            "Epoch [61/70], LR: [0.4]\n",
            "Loss: 0.1761380136013031\n",
            "Epoch [62/70], LR: [0.4]\n",
            "Loss: 0.16906876862049103\n",
            "Epoch [63/70], LR: [0.4]\n",
            "Loss: 0.1827009618282318\n",
            "Epoch [64/70], LR: [0.08000000000000002]\n",
            "Loss: 0.16531693935394287\n",
            "Epoch [65/70], LR: [0.08000000000000002]\n",
            "Loss: 0.17110306024551392\n",
            "Epoch [66/70], LR: [0.08000000000000002]\n",
            "Loss: 0.16768786311149597\n",
            "Epoch [67/70], LR: [0.08000000000000002]\n",
            "Loss: 0.16115078330039978\n",
            "Epoch [68/70], LR: [0.08000000000000002]\n",
            "Loss: 0.21794408559799194\n",
            "Epoch [69/70], LR: [0.08000000000000002]\n",
            "Loss: 0.1845884770154953\n",
            "Epoch [70/70], LR: [0.08000000000000002]\n",
            "Loss: 0.17843972146511078\n",
            "Validating classes ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Validating classes ['bicycle', 'plain', 'couch', 'lobster', 'lion', 'chair', 'tulip', 'television', 'skunk', 'girl'] -> [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Validating classes ['bowl', 'tank', 'lawn_mower', 'snake', 'ray', 'oak_tree', 'poppy', 'castle', 'telephone', 'clock'] -> [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "Validating classes ['worm', 'rabbit', 'tractor', 'cockroach', 'house', 'lamp', 'sweet_pepper', 'crab', 'beetle', 'dolphin'] -> [30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
            "Validating classes ['mouse', 'flatfish', 'pear', 'lizard', 'shark', 'orchid', 'cup', 'bus', 'sunflower', 'dinosaur'] -> [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
            "Validating classes ['whale', 'wolf', 'woman', 'cloud', 'porcupine', 'road', 'plate', 'table', 'sea', 'seal'] -> [50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
            "Validating classes ['bear', 'apple', 'forest', 'streetcar', 'can', 'bed', 'crocodile', 'keyboard', 'boy', 'raccoon'] -> [60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
            "Validating classes ['willow_tree', 'maple_tree', 'orange', 'rocket', 'spider', 'chimpanzee', 'cattle', 'kangaroo', 'bridge', 'fox'] -> [70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
            "Validating classes ['butterfly', 'baby', 'elephant', 'shrew', 'pine_tree', 'squirrel', 'mountain', 'caterpillar', 'bee', 'camel'] -> [80, 81, 82, 83, 84, 85, 86, 87, 88, 89]\n",
            "Accuracy: 0.22166666666666668\n",
            "BATCH [9]\n",
            "Training on ['leopard', 'trout', 'turtle', 'rose', 'aquarium_fish', 'possum', 'hamster', 'otter', 'motorcycle', 'pickup_truck'] -> [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
            "Known classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
            "Epoch [1/70], LR: [2.0]\n",
            "Loss: 0.18836402893066406\n",
            "Epoch [2/70], LR: [2.0]\n",
            "Loss: 0.19684386253356934\n",
            "Epoch [3/70], LR: [2.0]\n",
            "Loss: 0.19895116984844208\n",
            "Epoch [4/70], LR: [2.0]\n",
            "Loss: 0.19736476242542267\n",
            "Epoch [5/70], LR: [2.0]\n",
            "Loss: 0.20663124322891235\n",
            "Epoch [6/70], LR: [2.0]\n",
            "Loss: 0.19397661089897156\n",
            "Epoch [7/70], LR: [2.0]\n",
            "Loss: 0.18788035213947296\n",
            "Epoch [8/70], LR: [2.0]\n",
            "Loss: 0.1714559942483902\n",
            "Epoch [9/70], LR: [2.0]\n",
            "Loss: 0.20000417530536652\n",
            "Epoch [10/70], LR: [2.0]\n",
            "Loss: 0.1834794580936432\n",
            "Epoch [11/70], LR: [2.0]\n",
            "Loss: 0.1892656683921814\n",
            "Epoch [12/70], LR: [2.0]\n",
            "Loss: 0.17073822021484375\n",
            "Epoch [13/70], LR: [2.0]\n",
            "Loss: 0.17384250462055206\n",
            "Epoch [14/70], LR: [2.0]\n",
            "Loss: 0.18181884288787842\n",
            "Epoch [15/70], LR: [2.0]\n",
            "Loss: 0.18061812222003937\n",
            "Epoch [16/70], LR: [2.0]\n",
            "Loss: 0.18237437307834625\n",
            "Epoch [17/70], LR: [2.0]\n",
            "Loss: 0.18569713830947876\n",
            "Epoch [18/70], LR: [2.0]\n",
            "Loss: 0.19510020315647125\n",
            "Epoch [19/70], LR: [2.0]\n",
            "Loss: 0.17890524864196777\n",
            "Epoch [20/70], LR: [2.0]\n",
            "Loss: 0.1877799928188324\n",
            "Epoch [21/70], LR: [2.0]\n",
            "Loss: 0.18662643432617188\n",
            "Epoch [22/70], LR: [2.0]\n",
            "Loss: 0.1973985880613327\n",
            "Epoch [23/70], LR: [2.0]\n",
            "Loss: 0.17358435690402985\n",
            "Epoch [24/70], LR: [2.0]\n",
            "Loss: 0.17522062361240387\n",
            "Epoch [25/70], LR: [2.0]\n",
            "Loss: 0.1874464601278305\n",
            "Epoch [26/70], LR: [2.0]\n",
            "Loss: 0.18149028718471527\n",
            "Epoch [27/70], LR: [2.0]\n",
            "Loss: 0.1988094449043274\n",
            "Epoch [28/70], LR: [2.0]\n",
            "Loss: 0.18945550918579102\n",
            "Epoch [29/70], LR: [2.0]\n",
            "Loss: 0.1789161115884781\n",
            "Epoch [30/70], LR: [2.0]\n",
            "Loss: 0.1832490861415863\n",
            "Epoch [31/70], LR: [2.0]\n",
            "Loss: 0.17248252034187317\n",
            "Epoch [32/70], LR: [2.0]\n",
            "Loss: 0.18170638382434845\n",
            "Epoch [33/70], LR: [2.0]\n",
            "Loss: 0.18598823249340057\n",
            "Epoch [34/70], LR: [2.0]\n",
            "Loss: 0.1945328712463379\n",
            "Epoch [35/70], LR: [2.0]\n",
            "Loss: 0.19146209955215454\n",
            "Epoch [36/70], LR: [2.0]\n",
            "Loss: 0.1899900734424591\n",
            "Epoch [37/70], LR: [2.0]\n",
            "Loss: 0.18067100644111633\n",
            "Epoch [38/70], LR: [2.0]\n",
            "Loss: 0.16651026904582977\n",
            "Epoch [39/70], LR: [2.0]\n",
            "Loss: 0.18275199830532074\n",
            "Epoch [40/70], LR: [2.0]\n",
            "Loss: 0.1783020794391632\n",
            "Epoch [41/70], LR: [2.0]\n",
            "Loss: 0.18896719813346863\n",
            "Epoch [42/70], LR: [2.0]\n",
            "Loss: 0.18933875858783722\n",
            "Epoch [43/70], LR: [2.0]\n",
            "Loss: 0.16980688273906708\n",
            "Epoch [44/70], LR: [2.0]\n",
            "Loss: 0.1831931322813034\n",
            "Epoch [45/70], LR: [2.0]\n",
            "Loss: 0.16643600165843964\n",
            "Epoch [46/70], LR: [2.0]\n",
            "Loss: 0.17477309703826904\n",
            "Epoch [47/70], LR: [2.0]\n",
            "Loss: 0.17548473179340363\n",
            "Epoch [48/70], LR: [0.4]\n",
            "Loss: 0.17066039144992828\n",
            "Epoch [49/70], LR: [0.4]\n",
            "Loss: 0.1745552271604538\n",
            "Epoch [50/70], LR: [0.4]\n",
            "Loss: 0.1805361956357956\n",
            "Epoch [51/70], LR: [0.4]\n",
            "Loss: 0.17643393576145172\n",
            "Epoch [52/70], LR: [0.4]\n",
            "Loss: 0.16996236145496368\n",
            "Epoch [53/70], LR: [0.4]\n",
            "Loss: 0.16565684974193573\n",
            "Epoch [54/70], LR: [0.4]\n",
            "Loss: 0.17045167088508606\n",
            "Epoch [55/70], LR: [0.4]\n",
            "Loss: 0.15976457297801971\n",
            "Epoch [56/70], LR: [0.4]\n",
            "Loss: 0.18599070608615875\n",
            "Epoch [57/70], LR: [0.4]\n",
            "Loss: 0.17759548127651215\n",
            "Epoch [58/70], LR: [0.4]\n",
            "Loss: 0.17494133114814758\n",
            "Epoch [59/70], LR: [0.4]\n",
            "Loss: 0.18677718937397003\n",
            "Epoch [60/70], LR: [0.4]\n",
            "Loss: 0.17127074301242828\n",
            "Epoch [61/70], LR: [0.4]\n",
            "Loss: 0.1790270209312439\n",
            "Epoch [62/70], LR: [0.4]\n",
            "Loss: 0.18083555996418\n",
            "Epoch [63/70], LR: [0.4]\n",
            "Loss: 0.1783088892698288\n",
            "Epoch [64/70], LR: [0.08000000000000002]\n",
            "Loss: 0.18503428995609283\n",
            "Epoch [65/70], LR: [0.08000000000000002]\n",
            "Loss: 0.17464232444763184\n",
            "Epoch [66/70], LR: [0.08000000000000002]\n",
            "Loss: 0.17861659824848175\n",
            "Epoch [67/70], LR: [0.08000000000000002]\n",
            "Loss: 0.1860123723745346\n",
            "Epoch [68/70], LR: [0.08000000000000002]\n",
            "Loss: 0.17196406424045563\n",
            "Epoch [69/70], LR: [0.08000000000000002]\n",
            "Loss: 0.16627782583236694\n",
            "Epoch [70/70], LR: [0.08000000000000002]\n",
            "Loss: 0.18815124034881592\n",
            "Validating classes ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Validating classes ['bicycle', 'plain', 'couch', 'lobster', 'lion', 'chair', 'tulip', 'television', 'skunk', 'girl'] -> [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Validating classes ['bowl', 'tank', 'lawn_mower', 'snake', 'ray', 'oak_tree', 'poppy', 'castle', 'telephone', 'clock'] -> [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "Validating classes ['worm', 'rabbit', 'tractor', 'cockroach', 'house', 'lamp', 'sweet_pepper', 'crab', 'beetle', 'dolphin'] -> [30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
            "Validating classes ['mouse', 'flatfish', 'pear', 'lizard', 'shark', 'orchid', 'cup', 'bus', 'sunflower', 'dinosaur'] -> [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
            "Validating classes ['whale', 'wolf', 'woman', 'cloud', 'porcupine', 'road', 'plate', 'table', 'sea', 'seal'] -> [50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
            "Validating classes ['bear', 'apple', 'forest', 'streetcar', 'can', 'bed', 'crocodile', 'keyboard', 'boy', 'raccoon'] -> [60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
            "Validating classes ['willow_tree', 'maple_tree', 'orange', 'rocket', 'spider', 'chimpanzee', 'cattle', 'kangaroo', 'bridge', 'fox'] -> [70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
            "Validating classes ['butterfly', 'baby', 'elephant', 'shrew', 'pine_tree', 'squirrel', 'mountain', 'caterpillar', 'bee', 'camel'] -> [80, 81, 82, 83, 84, 85, 86, 87, 88, 89]\n",
            "Validating classes ['leopard', 'trout', 'turtle', 'rose', 'aquarium_fish', 'possum', 'hamster', 'otter', 'motorcycle', 'pickup_truck'] -> [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
            "Accuracy: 0.2128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhabEzELXMFR",
        "colab_type": "text"
      },
      "source": [
        "# **Catastrophic forgetting model (finetuning on iCaRL paper)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DgMzUscI0asL",
        "colab_type": "text"
      },
      "source": [
        "CATASTROPHIC FORGETTING NET\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ph1HdRD40f5C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CatastrophicForgetting(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        self.total_num_classes = n_classes\n",
        "        self.known_classes = 0\n",
        "        self.list_known_classes=[]\n",
        "        self.exemplar_sets = []\n",
        "        self.flag_mean = True\n",
        "        self.exemplar_means = []\n",
        "\n",
        "        # We take a standard ResNet and Extend it\n",
        "        super(CatastrophicForgetting, self).__init__()\n",
        "        self.extractor = resnet32()\n",
        "        self.fully_connected = nn.Linear(self.extractor.out_dim, 0, bias=True)\n",
        "        torch.nn.init.xavier_uniform_(self.fully_connected.weight)\n",
        "\n",
        "        self.fully_connected.bias.data.fill_(0.01)\n",
        "        self.loss=nn.BCEWithLogitsLoss()\n",
        "        self.optimizer = optim.SGD(self.parameters(), lr=2.0,weight_decay=0.00001)\n",
        "        self.scheduler=MultiStepLR(self.optimizer,[47,63],gamma=GAMMA)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # X: input data\n",
        "        self.extractor.to(DEVICE)\n",
        "        self.fully_connected.to(DEVICE)\n",
        "        x = self.extractor(x)\n",
        "        x = self.fully_connected(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "    def increment_classes(self, classes_to_add):          # increments the number of classes we are using\n",
        "      n_classes_to_add = len(classes_to_add)\n",
        "      self.list_known_classes+=classes_to_add             #add the new classes\n",
        "      print(f\"Known classes {self.list_known_classes}\")\n",
        "      weight = self.fully_connected.weight.data\n",
        "      feature_size = self.fully_connected.in_features\n",
        "      old_num_classes = self.fully_connected.out_features\n",
        "      self.fully_connected = nn.Linear(\n",
        "          feature_size, old_num_classes+n_classes_to_add, bias = True)\n",
        "      self.fully_connected.weight.data[:old_num_classes] = weight\n",
        "\n",
        "\n",
        "    def update_representation(self, dataset):             \n",
        "        classes_to_idx = dataset.labels_to_int\n",
        "        new_classes = [cls for cls in classes_to_idx if cls not in self.list_known_classes] #gets indexes of new classes only\n",
        "\n",
        "        train_data_loader = DataLoader(\n",
        "            dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
        "\n",
        "        self.increment_classes(new_classes)\n",
        "        \n",
        "        optimizer = optim.SGD(self.parameters(), lr=2.0,weight_decay=0.00001, momentum = 0.9)\n",
        "        scheduler = MultiStepLR(optimizer,[47,63],gamma=GAMMA)\n",
        "        self.train()\n",
        "\n",
        "        eye = torch.eye(self.known_classes+len(new_classes))\n",
        "\n",
        "        for epoch in range(NUM_EPOCHS):\n",
        "          print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], LR: {scheduler.get_last_lr()}\")\n",
        "          i=0\n",
        "          for indices, images, labels in train_data_loader:\n",
        "            images = images.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "            indices = indices.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            output = self.forward(images)\n",
        "            \n",
        "            labels_one_hot_new_classes = []\n",
        "            for label in labels:\n",
        "              labels_one_hot_new_classes.append(eye[label])\n",
        "            labels_one_hot_new_classes = torch.stack(labels_one_hot_new_classes).cuda()\n",
        "            loss=self.loss(output,labels_one_hot_new_classes)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "          torch.cuda.empty_cache()\n",
        "          print(f\"Loss: {loss.item()}\")\n",
        "          scheduler.step() \n",
        "        self.known_classes += len(new_classes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnIebV0E2A9j",
        "colab_type": "text"
      },
      "source": [
        "TRAINING CATASTROPHIC FORGETTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASMmv3tc2E7X",
        "colab_type": "code",
        "outputId": "a3782ea8-20ec-42cd-ec66-7db7aeca3669",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "net = CatastrophicForgetting(n_classes=100)\n",
        "net.to(DEVICE)\n",
        "\n",
        "for i, train_dataset in enumerate(train_datasets):\n",
        "  torch.cuda.empty_cache()\n",
        "  print(f\"BATCH [{i}]\")\n",
        "  print(f\"Training on {train_dataset.labels} -> {train_dataset.labels_to_int}\")\n",
        "  net.train()\n",
        "  net.update_representation(copy.deepcopy(train_dataset))\n",
        "\n",
        "\n",
        "  net.eval()\n",
        "  corrects = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "    for num in range(i+1):\n",
        "      print(f\"Validating classes {test_datasets[num].labels} -> {test_datasets[num].labels_to_int}\")\n",
        "      test_dataloader = DataLoader(test_datasets[num], batch_size=BATCH_SIZE, num_workers=4, shuffle = True)\n",
        "      for _, images, labels in test_dataloader:\n",
        "        images = images.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        outputs = net(images)\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        corrects += torch.sum(preds == labels.data).data.item()\n",
        "        total += len(images)\n",
        "    torch.cuda.empty_cache()\n",
        "    # Calculate Accuracy\n",
        "    accuracy = corrects / float(total)\n",
        "    print(f\"Accuracy: {accuracy}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BATCH [0]\n",
            "Training on ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Known classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Epoch [1/70], LR: [2.0]\n",
            "Loss: 0.3104826807975769\n",
            "Epoch [2/70], LR: [2.0]\n",
            "Loss: 0.3267080783843994\n",
            "Epoch [3/70], LR: [2.0]\n",
            "Loss: 0.33472558856010437\n",
            "Epoch [4/70], LR: [2.0]\n",
            "Loss: 0.3417496681213379\n",
            "Epoch [5/70], LR: [2.0]\n",
            "Loss: 0.33384209871292114\n",
            "Epoch [6/70], LR: [2.0]\n",
            "Loss: 0.3297256827354431\n",
            "Epoch [7/70], LR: [2.0]\n",
            "Loss: 0.3167625367641449\n",
            "Epoch [8/70], LR: [2.0]\n",
            "Loss: 0.3126598298549652\n",
            "Epoch [9/70], LR: [2.0]\n",
            "Loss: 0.3189757764339447\n",
            "Epoch [10/70], LR: [2.0]\n",
            "Loss: 0.3290937542915344\n",
            "Epoch [11/70], LR: [2.0]\n",
            "Loss: 0.348395437002182\n",
            "Epoch [12/70], LR: [2.0]\n",
            "Loss: 0.2938593327999115\n",
            "Epoch [13/70], LR: [2.0]\n",
            "Loss: 0.2551422715187073\n",
            "Epoch [14/70], LR: [2.0]\n",
            "Loss: 0.26085516810417175\n",
            "Epoch [15/70], LR: [2.0]\n",
            "Loss: 0.2518620491027832\n",
            "Epoch [16/70], LR: [2.0]\n",
            "Loss: 0.24940279126167297\n",
            "Epoch [17/70], LR: [2.0]\n",
            "Loss: 0.30478420853614807\n",
            "Epoch [18/70], LR: [2.0]\n",
            "Loss: 0.21414315700531006\n",
            "Epoch [19/70], LR: [2.0]\n",
            "Loss: 0.27095362544059753\n",
            "Epoch [20/70], LR: [2.0]\n",
            "Loss: 0.3507908880710602\n",
            "Epoch [21/70], LR: [2.0]\n",
            "Loss: 0.18382051587104797\n",
            "Epoch [22/70], LR: [2.0]\n",
            "Loss: 0.26377010345458984\n",
            "Epoch [23/70], LR: [2.0]\n",
            "Loss: 0.2051779329776764\n",
            "Epoch [24/70], LR: [2.0]\n",
            "Loss: 0.26800432801246643\n",
            "Epoch [25/70], LR: [2.0]\n",
            "Loss: 0.1748492568731308\n",
            "Epoch [26/70], LR: [2.0]\n",
            "Loss: 0.25920554995536804\n",
            "Epoch [27/70], LR: [2.0]\n",
            "Loss: 0.3628006875514984\n",
            "Epoch [28/70], LR: [2.0]\n",
            "Loss: 0.18577955663204193\n",
            "Epoch [29/70], LR: [2.0]\n",
            "Loss: 0.20145586133003235\n",
            "Epoch [30/70], LR: [2.0]\n",
            "Loss: 0.3265957832336426\n",
            "Epoch [31/70], LR: [2.0]\n",
            "Loss: 0.3112252652645111\n",
            "Epoch [32/70], LR: [2.0]\n",
            "Loss: 0.11138217896223068\n",
            "Epoch [33/70], LR: [2.0]\n",
            "Loss: 0.24745707213878632\n",
            "Epoch [34/70], LR: [2.0]\n",
            "Loss: 0.16351734101772308\n",
            "Epoch [35/70], LR: [2.0]\n",
            "Loss: 0.16989710927009583\n",
            "Epoch [36/70], LR: [2.0]\n",
            "Loss: 0.17642977833747864\n",
            "Epoch [37/70], LR: [2.0]\n",
            "Loss: 0.22780953347682953\n",
            "Epoch [38/70], LR: [2.0]\n",
            "Loss: 0.1679697334766388\n",
            "Epoch [39/70], LR: [2.0]\n",
            "Loss: 0.11868260055780411\n",
            "Epoch [40/70], LR: [2.0]\n",
            "Loss: 0.1587459146976471\n",
            "Epoch [41/70], LR: [2.0]\n",
            "Loss: 0.17554913461208344\n",
            "Epoch [42/70], LR: [2.0]\n",
            "Loss: 0.13554494082927704\n",
            "Epoch [43/70], LR: [2.0]\n",
            "Loss: 0.14546780288219452\n",
            "Epoch [44/70], LR: [2.0]\n",
            "Loss: 0.2199789583683014\n",
            "Epoch [45/70], LR: [2.0]\n",
            "Loss: 0.17961309850215912\n",
            "Epoch [46/70], LR: [2.0]\n",
            "Loss: 0.1419772356748581\n",
            "Epoch [47/70], LR: [2.0]\n",
            "Loss: 0.19252848625183105\n",
            "Epoch [48/70], LR: [0.4]\n",
            "Loss: 0.2133898288011551\n",
            "Epoch [49/70], LR: [0.4]\n",
            "Loss: 0.08707312494516373\n",
            "Epoch [50/70], LR: [0.4]\n",
            "Loss: 0.04749245196580887\n",
            "Epoch [51/70], LR: [0.4]\n",
            "Loss: 0.270357221364975\n",
            "Epoch [52/70], LR: [0.4]\n",
            "Loss: 0.19975093007087708\n",
            "Epoch [53/70], LR: [0.4]\n",
            "Loss: 0.13890278339385986\n",
            "Epoch [54/70], LR: [0.4]\n",
            "Loss: 0.1131981834769249\n",
            "Epoch [55/70], LR: [0.4]\n",
            "Loss: 0.1830447018146515\n",
            "Epoch [56/70], LR: [0.4]\n",
            "Loss: 0.19464018940925598\n",
            "Epoch [57/70], LR: [0.4]\n",
            "Loss: 0.11386940628290176\n",
            "Epoch [58/70], LR: [0.4]\n",
            "Loss: 0.06556321680545807\n",
            "Epoch [59/70], LR: [0.4]\n",
            "Loss: 0.2719399631023407\n",
            "Epoch [60/70], LR: [0.4]\n",
            "Loss: 0.26832446455955505\n",
            "Epoch [61/70], LR: [0.4]\n",
            "Loss: 0.10428154468536377\n",
            "Epoch [62/70], LR: [0.4]\n",
            "Loss: 0.15534089505672455\n",
            "Epoch [63/70], LR: [0.4]\n",
            "Loss: 0.5657047629356384\n",
            "Epoch [64/70], LR: [0.08000000000000002]\n",
            "Loss: 0.2382369488477707\n",
            "Epoch [65/70], LR: [0.08000000000000002]\n",
            "Loss: 0.12411897629499435\n",
            "Epoch [66/70], LR: [0.08000000000000002]\n",
            "Loss: 0.13312892615795135\n",
            "Epoch [67/70], LR: [0.08000000000000002]\n",
            "Loss: 0.09450196474790573\n",
            "Epoch [68/70], LR: [0.08000000000000002]\n",
            "Loss: 0.03737938031554222\n",
            "Epoch [69/70], LR: [0.08000000000000002]\n",
            "Loss: 0.12621665000915527\n",
            "Epoch [70/70], LR: [0.08000000000000002]\n",
            "Loss: 0.1671089380979538\n",
            "Validating classes ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Accuracy: 0.833\n",
            "BATCH [1]\n",
            "Training on ['bicycle', 'plain', 'couch', 'lobster', 'lion', 'chair', 'tulip', 'television', 'skunk', 'girl'] -> [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Known classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Epoch [1/70], LR: [2.0]\n",
            "Loss: 0.10509710758924484\n",
            "Epoch [2/70], LR: [2.0]\n",
            "Loss: 0.10007704794406891\n",
            "Epoch [3/70], LR: [2.0]\n",
            "Loss: 0.07862712442874908\n",
            "Epoch [4/70], LR: [2.0]\n",
            "Loss: 0.15478134155273438\n",
            "Epoch [5/70], LR: [2.0]\n",
            "Loss: 0.10774125903844833\n",
            "Epoch [6/70], LR: [2.0]\n",
            "Loss: 0.11824303865432739\n",
            "Epoch [7/70], LR: [2.0]\n",
            "Loss: 0.0834747701883316\n",
            "Epoch [8/70], LR: [2.0]\n",
            "Loss: 0.04215212166309357\n",
            "Epoch [9/70], LR: [2.0]\n",
            "Loss: 0.038881104439496994\n",
            "Epoch [10/70], LR: [2.0]\n",
            "Loss: 0.06593087315559387\n",
            "Epoch [11/70], LR: [2.0]\n",
            "Loss: 0.07720281928777695\n",
            "Epoch [12/70], LR: [2.0]\n",
            "Loss: 0.014921528287231922\n",
            "Epoch [13/70], LR: [2.0]\n",
            "Loss: 0.041563428938388824\n",
            "Epoch [14/70], LR: [2.0]\n",
            "Loss: 0.06953214854001999\n",
            "Epoch [15/70], LR: [2.0]\n",
            "Loss: 0.06068028137087822\n",
            "Epoch [16/70], LR: [2.0]\n",
            "Loss: 0.06635011732578278\n",
            "Epoch [17/70], LR: [2.0]\n",
            "Loss: 0.11022534221410751\n",
            "Epoch [18/70], LR: [2.0]\n",
            "Loss: 0.06705152988433838\n",
            "Epoch [19/70], LR: [2.0]\n",
            "Loss: 0.05094799026846886\n",
            "Epoch [20/70], LR: [2.0]\n",
            "Loss: 0.0436813049018383\n",
            "Epoch [21/70], LR: [2.0]\n",
            "Loss: 0.02768251672387123\n",
            "Epoch [22/70], LR: [2.0]\n",
            "Loss: 0.045922111719846725\n",
            "Epoch [23/70], LR: [2.0]\n",
            "Loss: 0.06214147433638573\n",
            "Epoch [24/70], LR: [2.0]\n",
            "Loss: 0.012791420333087444\n",
            "Epoch [25/70], LR: [2.0]\n",
            "Loss: 0.09844402223825455\n",
            "Epoch [26/70], LR: [2.0]\n",
            "Loss: 0.05374760553240776\n",
            "Epoch [27/70], LR: [2.0]\n",
            "Loss: 0.02740250527858734\n",
            "Epoch [28/70], LR: [2.0]\n",
            "Loss: 0.028836771845817566\n",
            "Epoch [29/70], LR: [2.0]\n",
            "Loss: 0.027432719245553017\n",
            "Epoch [30/70], LR: [2.0]\n",
            "Loss: 0.05057620629668236\n",
            "Epoch [31/70], LR: [2.0]\n",
            "Loss: 0.1515689343214035\n",
            "Epoch [32/70], LR: [2.0]\n",
            "Loss: 0.014894119463860989\n",
            "Epoch [33/70], LR: [2.0]\n",
            "Loss: 0.05464175343513489\n",
            "Epoch [34/70], LR: [2.0]\n",
            "Loss: 0.03449585661292076\n",
            "Epoch [35/70], LR: [2.0]\n",
            "Loss: 0.030302906408905983\n",
            "Epoch [36/70], LR: [2.0]\n",
            "Loss: 0.022500237450003624\n",
            "Epoch [37/70], LR: [2.0]\n",
            "Loss: 0.048858869820833206\n",
            "Epoch [38/70], LR: [2.0]\n",
            "Loss: 0.12037444114685059\n",
            "Epoch [39/70], LR: [2.0]\n",
            "Loss: 0.034584298729896545\n",
            "Epoch [40/70], LR: [2.0]\n",
            "Loss: 0.09683486074209213\n",
            "Epoch [41/70], LR: [2.0]\n",
            "Loss: 0.03087695874273777\n",
            "Epoch [42/70], LR: [2.0]\n",
            "Loss: 0.05506333336234093\n",
            "Epoch [43/70], LR: [2.0]\n",
            "Loss: 0.02928604558110237\n",
            "Epoch [44/70], LR: [2.0]\n",
            "Loss: 0.05207471176981926\n",
            "Epoch [45/70], LR: [2.0]\n",
            "Loss: 0.1574753373861313\n",
            "Epoch [46/70], LR: [2.0]\n",
            "Loss: 0.04304773733019829\n",
            "Epoch [47/70], LR: [2.0]\n",
            "Loss: 0.019713368266820908\n",
            "Epoch [48/70], LR: [0.4]\n",
            "Loss: 0.007745278067886829\n",
            "Epoch [49/70], LR: [0.4]\n",
            "Loss: 0.058963753283023834\n",
            "Epoch [50/70], LR: [0.4]\n",
            "Loss: 0.06695588678121567\n",
            "Epoch [51/70], LR: [0.4]\n",
            "Loss: 0.04042304679751396\n",
            "Epoch [52/70], LR: [0.4]\n",
            "Loss: 0.0074095227755606174\n",
            "Epoch [53/70], LR: [0.4]\n",
            "Loss: 0.032881882041692734\n",
            "Epoch [54/70], LR: [0.4]\n",
            "Loss: 0.012584676034748554\n",
            "Epoch [55/70], LR: [0.4]\n",
            "Loss: 0.023052334785461426\n",
            "Epoch [56/70], LR: [0.4]\n",
            "Loss: 0.039402104914188385\n",
            "Epoch [57/70], LR: [0.4]\n",
            "Loss: 0.03215348720550537\n",
            "Epoch [58/70], LR: [0.4]\n",
            "Loss: 0.05375681445002556\n",
            "Epoch [59/70], LR: [0.4]\n",
            "Loss: 0.0172681026160717\n",
            "Epoch [60/70], LR: [0.4]\n",
            "Loss: 0.009732875041663647\n",
            "Epoch [61/70], LR: [0.4]\n",
            "Loss: 0.046517644077539444\n",
            "Epoch [62/70], LR: [0.4]\n",
            "Loss: 0.04981713369488716\n",
            "Epoch [63/70], LR: [0.4]\n",
            "Loss: 0.01441957801580429\n",
            "Epoch [64/70], LR: [0.08000000000000002]\n",
            "Loss: 0.004487478639930487\n",
            "Epoch [65/70], LR: [0.08000000000000002]\n",
            "Loss: 0.010010774247348309\n",
            "Epoch [66/70], LR: [0.08000000000000002]\n",
            "Loss: 0.010179376229643822\n",
            "Epoch [67/70], LR: [0.08000000000000002]\n",
            "Loss: 0.02192455530166626\n",
            "Epoch [68/70], LR: [0.08000000000000002]\n",
            "Loss: 0.10212855786085129\n",
            "Epoch [69/70], LR: [0.08000000000000002]\n",
            "Loss: 0.01725655607879162\n",
            "Epoch [70/70], LR: [0.08000000000000002]\n",
            "Loss: 0.0026538409292697906\n",
            "Validating classes ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Validating classes ['bicycle', 'plain', 'couch', 'lobster', 'lion', 'chair', 'tulip', 'television', 'skunk', 'girl'] -> [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Accuracy: 0.4455\n",
            "BATCH [2]\n",
            "Training on ['bowl', 'tank', 'lawn_mower', 'snake', 'ray', 'oak_tree', 'poppy', 'castle', 'telephone', 'clock'] -> [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "Known classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "Epoch [1/70], LR: [2.0]\n",
            "Loss: 0.10107006132602692\n",
            "Epoch [2/70], LR: [2.0]\n",
            "Loss: 0.03655528277158737\n",
            "Epoch [3/70], LR: [2.0]\n",
            "Loss: 0.06424818187952042\n",
            "Epoch [4/70], LR: [2.0]\n",
            "Loss: 0.0719551146030426\n",
            "Epoch [5/70], LR: [2.0]\n",
            "Loss: 0.05189476162195206\n",
            "Epoch [6/70], LR: [2.0]\n",
            "Loss: 0.0596911795437336\n",
            "Epoch [7/70], LR: [2.0]\n",
            "Loss: 0.037618692964315414\n",
            "Epoch [8/70], LR: [2.0]\n",
            "Loss: 0.04280366376042366\n",
            "Epoch [9/70], LR: [2.0]\n",
            "Loss: 0.04029546678066254\n",
            "Epoch [10/70], LR: [2.0]\n",
            "Loss: 0.0869092047214508\n",
            "Epoch [11/70], LR: [2.0]\n",
            "Loss: 0.014199920929968357\n",
            "Epoch [12/70], LR: [2.0]\n",
            "Loss: 0.0804034098982811\n",
            "Epoch [13/70], LR: [2.0]\n",
            "Loss: 0.015945805236697197\n",
            "Epoch [14/70], LR: [2.0]\n",
            "Loss: 0.14205288887023926\n",
            "Epoch [15/70], LR: [2.0]\n",
            "Loss: 0.11613582819700241\n",
            "Epoch [16/70], LR: [2.0]\n",
            "Loss: 0.03548537939786911\n",
            "Epoch [17/70], LR: [2.0]\n",
            "Loss: 0.06434663385152817\n",
            "Epoch [18/70], LR: [2.0]\n",
            "Loss: 0.050940971821546555\n",
            "Epoch [19/70], LR: [2.0]\n",
            "Loss: 0.034285418689250946\n",
            "Epoch [20/70], LR: [2.0]\n",
            "Loss: 0.030923152342438698\n",
            "Epoch [21/70], LR: [2.0]\n",
            "Loss: 0.02251761592924595\n",
            "Epoch [22/70], LR: [2.0]\n",
            "Loss: 0.027343839406967163\n",
            "Epoch [23/70], LR: [2.0]\n",
            "Loss: 0.0390007346868515\n",
            "Epoch [24/70], LR: [2.0]\n",
            "Loss: 0.027905650436878204\n",
            "Epoch [25/70], LR: [2.0]\n",
            "Loss: 0.04272814095020294\n",
            "Epoch [26/70], LR: [2.0]\n",
            "Loss: 0.04431818053126335\n",
            "Epoch [27/70], LR: [2.0]\n",
            "Loss: 0.05453546345233917\n",
            "Epoch [28/70], LR: [2.0]\n",
            "Loss: 0.07002383470535278\n",
            "Epoch [29/70], LR: [2.0]\n",
            "Loss: 0.023306885734200478\n",
            "Epoch [30/70], LR: [2.0]\n",
            "Loss: 0.026016054674983025\n",
            "Epoch [31/70], LR: [2.0]\n",
            "Loss: 0.019333278760313988\n",
            "Epoch [32/70], LR: [2.0]\n",
            "Loss: 0.12524573504924774\n",
            "Epoch [33/70], LR: [2.0]\n",
            "Loss: 0.047916412353515625\n",
            "Epoch [34/70], LR: [2.0]\n",
            "Loss: 0.040138889104127884\n",
            "Epoch [35/70], LR: [2.0]\n",
            "Loss: 0.016185380518436432\n",
            "Epoch [36/70], LR: [2.0]\n",
            "Loss: 0.05557696893811226\n",
            "Epoch [37/70], LR: [2.0]\n",
            "Loss: 0.02740974724292755\n",
            "Epoch [38/70], LR: [2.0]\n",
            "Loss: 0.05836210399866104\n",
            "Epoch [39/70], LR: [2.0]\n",
            "Loss: 0.03567816689610481\n",
            "Epoch [40/70], LR: [2.0]\n",
            "Loss: 0.026568159461021423\n",
            "Epoch [41/70], LR: [2.0]\n",
            "Loss: 0.025230174884200096\n",
            "Epoch [42/70], LR: [2.0]\n",
            "Loss: 0.05318845063447952\n",
            "Epoch [43/70], LR: [2.0]\n",
            "Loss: 0.044225338846445084\n",
            "Epoch [44/70], LR: [2.0]\n",
            "Loss: 0.018246807157993317\n",
            "Epoch [45/70], LR: [2.0]\n",
            "Loss: 0.06402295082807541\n",
            "Epoch [46/70], LR: [2.0]\n",
            "Loss: 0.009167961776256561\n",
            "Epoch [47/70], LR: [2.0]\n",
            "Loss: 0.026727966964244843\n",
            "Epoch [48/70], LR: [0.4]\n",
            "Loss: 0.0378071554005146\n",
            "Epoch [49/70], LR: [0.4]\n",
            "Loss: 0.013399126939475536\n",
            "Epoch [50/70], LR: [0.4]\n",
            "Loss: 0.08421666920185089\n",
            "Epoch [51/70], LR: [0.4]\n",
            "Loss: 0.01585208997130394\n",
            "Epoch [52/70], LR: [0.4]\n",
            "Loss: 0.01871592365205288\n",
            "Epoch [53/70], LR: [0.4]\n",
            "Loss: 0.043901216238737106\n",
            "Epoch [54/70], LR: [0.4]\n",
            "Loss: 0.005198732018470764\n",
            "Epoch [55/70], LR: [0.4]\n",
            "Loss: 0.011529515497386456\n",
            "Epoch [56/70], LR: [0.4]\n",
            "Loss: 0.06318934261798859\n",
            "Epoch [57/70], LR: [0.4]\n",
            "Loss: 0.022746386006474495\n",
            "Epoch [58/70], LR: [0.4]\n",
            "Loss: 0.036328475922346115\n",
            "Epoch [59/70], LR: [0.4]\n",
            "Loss: 0.007133398205041885\n",
            "Epoch [60/70], LR: [0.4]\n",
            "Loss: 0.017551174387335777\n",
            "Epoch [61/70], LR: [0.4]\n",
            "Loss: 0.08220867812633514\n",
            "Epoch [62/70], LR: [0.4]\n",
            "Loss: 0.04385852813720703\n",
            "Epoch [63/70], LR: [0.4]\n",
            "Loss: 0.055374957621097565\n",
            "Epoch [64/70], LR: [0.08000000000000002]\n",
            "Loss: 0.03798428550362587\n",
            "Epoch [65/70], LR: [0.08000000000000002]\n",
            "Loss: 0.016796860843896866\n",
            "Epoch [66/70], LR: [0.08000000000000002]\n",
            "Loss: 0.02424035407602787\n",
            "Epoch [67/70], LR: [0.08000000000000002]\n",
            "Loss: 0.022444074973464012\n",
            "Epoch [68/70], LR: [0.08000000000000002]\n",
            "Loss: 0.0025330118369311094\n",
            "Epoch [69/70], LR: [0.08000000000000002]\n",
            "Loss: 0.010125447064638138\n",
            "Epoch [70/70], LR: [0.08000000000000002]\n",
            "Loss: 0.023944402113556862\n",
            "Validating classes ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Validating classes ['bicycle', 'plain', 'couch', 'lobster', 'lion', 'chair', 'tulip', 'television', 'skunk', 'girl'] -> [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Validating classes ['bowl', 'tank', 'lawn_mower', 'snake', 'ray', 'oak_tree', 'poppy', 'castle', 'telephone', 'clock'] -> [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "Accuracy: 0.3\n",
            "BATCH [3]\n",
            "Training on ['worm', 'rabbit', 'tractor', 'cockroach', 'house', 'lamp', 'sweet_pepper', 'crab', 'beetle', 'dolphin'] -> [30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
            "Known classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
            "Epoch [1/70], LR: [2.0]\n",
            "Loss: 0.08513464778661728\n",
            "Epoch [2/70], LR: [2.0]\n",
            "Loss: 0.04792959988117218\n",
            "Epoch [3/70], LR: [2.0]\n",
            "Loss: 0.044558074325323105\n",
            "Epoch [4/70], LR: [2.0]\n",
            "Loss: 0.048203762620687485\n",
            "Epoch [5/70], LR: [2.0]\n",
            "Loss: 0.08234748989343643\n",
            "Epoch [6/70], LR: [2.0]\n",
            "Loss: 0.057185329496860504\n",
            "Epoch [7/70], LR: [2.0]\n",
            "Loss: 0.03519681841135025\n",
            "Epoch [8/70], LR: [2.0]\n",
            "Loss: 0.03137127682566643\n",
            "Epoch [9/70], LR: [2.0]\n",
            "Loss: 0.03978629782795906\n",
            "Epoch [10/70], LR: [2.0]\n",
            "Loss: 0.07247836142778397\n",
            "Epoch [11/70], LR: [2.0]\n",
            "Loss: 0.03219384700059891\n",
            "Epoch [12/70], LR: [2.0]\n",
            "Loss: 0.04192361980676651\n",
            "Epoch [13/70], LR: [2.0]\n",
            "Loss: 0.04174993559718132\n",
            "Epoch [14/70], LR: [2.0]\n",
            "Loss: 0.02962217666208744\n",
            "Epoch [15/70], LR: [2.0]\n",
            "Loss: 0.036934588104486465\n",
            "Epoch [16/70], LR: [2.0]\n",
            "Loss: 0.04867643117904663\n",
            "Epoch [17/70], LR: [2.0]\n",
            "Loss: 0.007310885936021805\n",
            "Epoch [18/70], LR: [2.0]\n",
            "Loss: 0.044513821601867676\n",
            "Epoch [19/70], LR: [2.0]\n",
            "Loss: 0.03945473954081535\n",
            "Epoch [20/70], LR: [2.0]\n",
            "Loss: 0.07192984968423843\n",
            "Epoch [21/70], LR: [2.0]\n",
            "Loss: 0.03948326036334038\n",
            "Epoch [22/70], LR: [2.0]\n",
            "Loss: 0.01947006769478321\n",
            "Epoch [23/70], LR: [2.0]\n",
            "Loss: 0.024079814553260803\n",
            "Epoch [24/70], LR: [2.0]\n",
            "Loss: 0.029210656881332397\n",
            "Epoch [25/70], LR: [2.0]\n",
            "Loss: 0.05264747142791748\n",
            "Epoch [26/70], LR: [2.0]\n",
            "Loss: 0.0075753228738904\n",
            "Epoch [27/70], LR: [2.0]\n",
            "Loss: 0.02761547826230526\n",
            "Epoch [28/70], LR: [2.0]\n",
            "Loss: 0.007518339902162552\n",
            "Epoch [29/70], LR: [2.0]\n",
            "Loss: 0.034640729427337646\n",
            "Epoch [30/70], LR: [2.0]\n",
            "Loss: 0.022596342489123344\n",
            "Epoch [31/70], LR: [2.0]\n",
            "Loss: 0.008820625953376293\n",
            "Epoch [32/70], LR: [2.0]\n",
            "Loss: 0.0390724241733551\n",
            "Epoch [33/70], LR: [2.0]\n",
            "Loss: 0.02475026249885559\n",
            "Epoch [34/70], LR: [2.0]\n",
            "Loss: 0.022708658128976822\n",
            "Epoch [35/70], LR: [2.0]\n",
            "Loss: 0.03609943762421608\n",
            "Epoch [36/70], LR: [2.0]\n",
            "Loss: 0.03803667053580284\n",
            "Epoch [37/70], LR: [2.0]\n",
            "Loss: 0.017626577988266945\n",
            "Epoch [38/70], LR: [2.0]\n",
            "Loss: 0.04366884380578995\n",
            "Epoch [39/70], LR: [2.0]\n",
            "Loss: 0.011327340267598629\n",
            "Epoch [40/70], LR: [2.0]\n",
            "Loss: 0.059526849538087845\n",
            "Epoch [41/70], LR: [2.0]\n",
            "Loss: 0.026850981637835503\n",
            "Epoch [42/70], LR: [2.0]\n",
            "Loss: 0.032408442348241806\n",
            "Epoch [43/70], LR: [2.0]\n",
            "Loss: 0.009948636405169964\n",
            "Epoch [44/70], LR: [2.0]\n",
            "Loss: 0.05491715297102928\n",
            "Epoch [45/70], LR: [2.0]\n",
            "Loss: 0.0168119166046381\n",
            "Epoch [46/70], LR: [2.0]\n",
            "Loss: 0.03967327997088432\n",
            "Epoch [47/70], LR: [2.0]\n",
            "Loss: 0.04568886756896973\n",
            "Epoch [48/70], LR: [0.4]\n",
            "Loss: 0.011284264735877514\n",
            "Epoch [49/70], LR: [0.4]\n",
            "Loss: 0.01888922229409218\n",
            "Epoch [50/70], LR: [0.4]\n",
            "Loss: 0.02267194539308548\n",
            "Epoch [51/70], LR: [0.4]\n",
            "Loss: 0.032238543033599854\n",
            "Epoch [52/70], LR: [0.4]\n",
            "Loss: 0.019539205357432365\n",
            "Epoch [53/70], LR: [0.4]\n",
            "Loss: 0.044895853847265244\n",
            "Epoch [54/70], LR: [0.4]\n",
            "Loss: 0.02105119824409485\n",
            "Epoch [55/70], LR: [0.4]\n",
            "Loss: 0.03955260291695595\n",
            "Epoch [56/70], LR: [0.4]\n",
            "Loss: 0.012394382618367672\n",
            "Epoch [57/70], LR: [0.4]\n",
            "Loss: 0.011296353302896023\n",
            "Epoch [58/70], LR: [0.4]\n",
            "Loss: 0.023711388930678368\n",
            "Epoch [59/70], LR: [0.4]\n",
            "Loss: 0.007209397852420807\n",
            "Epoch [60/70], LR: [0.4]\n",
            "Loss: 0.04566190019249916\n",
            "Epoch [61/70], LR: [0.4]\n",
            "Loss: 0.03900451958179474\n",
            "Epoch [62/70], LR: [0.4]\n",
            "Loss: 0.03322723135352135\n",
            "Epoch [63/70], LR: [0.4]\n",
            "Loss: 0.03992645815014839\n",
            "Epoch [64/70], LR: [0.08000000000000002]\n",
            "Loss: 0.005543270148336887\n",
            "Epoch [65/70], LR: [0.08000000000000002]\n",
            "Loss: 0.04723840579390526\n",
            "Epoch [66/70], LR: [0.08000000000000002]\n",
            "Loss: 0.026765981689095497\n",
            "Epoch [67/70], LR: [0.08000000000000002]\n",
            "Loss: 0.0027901651337742805\n",
            "Epoch [68/70], LR: [0.08000000000000002]\n",
            "Loss: 0.0324094258248806\n",
            "Epoch [69/70], LR: [0.08000000000000002]\n",
            "Loss: 0.03965546190738678\n",
            "Epoch [70/70], LR: [0.08000000000000002]\n",
            "Loss: 0.023350588977336884\n",
            "Validating classes ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Validating classes ['bicycle', 'plain', 'couch', 'lobster', 'lion', 'chair', 'tulip', 'television', 'skunk', 'girl'] -> [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Validating classes ['bowl', 'tank', 'lawn_mower', 'snake', 'ray', 'oak_tree', 'poppy', 'castle', 'telephone', 'clock'] -> [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "Validating classes ['worm', 'rabbit', 'tractor', 'cockroach', 'house', 'lamp', 'sweet_pepper', 'crab', 'beetle', 'dolphin'] -> [30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
            "Accuracy: 0.2205\n",
            "BATCH [4]\n",
            "Training on ['mouse', 'flatfish', 'pear', 'lizard', 'shark', 'orchid', 'cup', 'bus', 'sunflower', 'dinosaur'] -> [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
            "Known classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
            "Epoch [1/70], LR: [2.0]\n",
            "Loss: 0.049924835562705994\n",
            "Epoch [2/70], LR: [2.0]\n",
            "Loss: 0.05133132264018059\n",
            "Epoch [3/70], LR: [2.0]\n",
            "Loss: 0.015526875853538513\n",
            "Epoch [4/70], LR: [2.0]\n",
            "Loss: 0.016543615609407425\n",
            "Epoch [5/70], LR: [2.0]\n",
            "Loss: 0.04898793250322342\n",
            "Epoch [6/70], LR: [2.0]\n",
            "Loss: 0.02475823275744915\n",
            "Epoch [7/70], LR: [2.0]\n",
            "Loss: 0.01906701549887657\n",
            "Epoch [8/70], LR: [2.0]\n",
            "Loss: 0.05457080155611038\n",
            "Epoch [9/70], LR: [2.0]\n",
            "Loss: 0.0198687557131052\n",
            "Epoch [10/70], LR: [2.0]\n",
            "Loss: 0.009077204391360283\n",
            "Epoch [11/70], LR: [2.0]\n",
            "Loss: 0.03129410743713379\n",
            "Epoch [12/70], LR: [2.0]\n",
            "Loss: 0.012406117282807827\n",
            "Epoch [13/70], LR: [2.0]\n",
            "Loss: 0.055311962962150574\n",
            "Epoch [14/70], LR: [2.0]\n",
            "Loss: 0.061586588621139526\n",
            "Epoch [15/70], LR: [2.0]\n",
            "Loss: 0.06097457930445671\n",
            "Epoch [16/70], LR: [2.0]\n",
            "Loss: 0.04519135504961014\n",
            "Epoch [17/70], LR: [2.0]\n",
            "Loss: 0.04094496741890907\n",
            "Epoch [18/70], LR: [2.0]\n",
            "Loss: 0.04342830553650856\n",
            "Epoch [19/70], LR: [2.0]\n",
            "Loss: 0.018684428185224533\n",
            "Epoch [20/70], LR: [2.0]\n",
            "Loss: 0.007741858717054129\n",
            "Epoch [21/70], LR: [2.0]\n",
            "Loss: 0.015327121131122112\n",
            "Epoch [22/70], LR: [2.0]\n",
            "Loss: 0.009515895508229733\n",
            "Epoch [23/70], LR: [2.0]\n",
            "Loss: 0.02389778010547161\n",
            "Epoch [24/70], LR: [2.0]\n",
            "Loss: 0.011418403126299381\n",
            "Epoch [25/70], LR: [2.0]\n",
            "Loss: 0.017992105334997177\n",
            "Epoch [26/70], LR: [2.0]\n",
            "Loss: 0.012879237532615662\n",
            "Epoch [27/70], LR: [2.0]\n",
            "Loss: 0.003897545626387\n",
            "Epoch [28/70], LR: [2.0]\n",
            "Loss: 0.05513322353363037\n",
            "Epoch [29/70], LR: [2.0]\n",
            "Loss: 0.037981972098350525\n",
            "Epoch [30/70], LR: [2.0]\n",
            "Loss: 0.04111308604478836\n",
            "Epoch [31/70], LR: [2.0]\n",
            "Loss: 0.008892733603715897\n",
            "Epoch [32/70], LR: [2.0]\n",
            "Loss: 0.0396384559571743\n",
            "Epoch [33/70], LR: [2.0]\n",
            "Loss: 0.058582525700330734\n",
            "Epoch [34/70], LR: [2.0]\n",
            "Loss: 0.016198566183447838\n",
            "Epoch [35/70], LR: [2.0]\n",
            "Loss: 0.0392363965511322\n",
            "Epoch [36/70], LR: [2.0]\n",
            "Loss: 0.06397321820259094\n",
            "Epoch [37/70], LR: [2.0]\n",
            "Loss: 0.018406689167022705\n",
            "Epoch [38/70], LR: [2.0]\n",
            "Loss: 0.019838852807879448\n",
            "Epoch [39/70], LR: [2.0]\n",
            "Loss: 0.013332245871424675\n",
            "Epoch [40/70], LR: [2.0]\n",
            "Loss: 0.03143490478396416\n",
            "Epoch [41/70], LR: [2.0]\n",
            "Loss: 0.0548882931470871\n",
            "Epoch [42/70], LR: [2.0]\n",
            "Loss: 0.036690764129161835\n",
            "Epoch [43/70], LR: [2.0]\n",
            "Loss: 0.010992927476763725\n",
            "Epoch [44/70], LR: [2.0]\n",
            "Loss: 0.023914674296975136\n",
            "Epoch [45/70], LR: [2.0]\n",
            "Loss: 0.03833920508623123\n",
            "Epoch [46/70], LR: [2.0]\n",
            "Loss: 0.022755421698093414\n",
            "Epoch [47/70], LR: [2.0]\n",
            "Loss: 0.017401911318302155\n",
            "Epoch [48/70], LR: [0.4]\n",
            "Loss: 0.0010644792346283793\n",
            "Epoch [49/70], LR: [0.4]\n",
            "Loss: 0.017017649486660957\n",
            "Epoch [50/70], LR: [0.4]\n",
            "Loss: 0.005682787392288446\n",
            "Epoch [51/70], LR: [0.4]\n",
            "Loss: 0.0007452996796928346\n",
            "Epoch [52/70], LR: [0.4]\n",
            "Loss: 0.005359357688575983\n",
            "Epoch [53/70], LR: [0.4]\n",
            "Loss: 0.002764065284281969\n",
            "Epoch [54/70], LR: [0.4]\n",
            "Loss: 0.005138403736054897\n",
            "Epoch [55/70], LR: [0.4]\n",
            "Loss: 0.01787119358778\n",
            "Epoch [56/70], LR: [0.4]\n",
            "Loss: 0.0019793345127254725\n",
            "Epoch [57/70], LR: [0.4]\n",
            "Loss: 0.003617409151047468\n",
            "Epoch [58/70], LR: [0.4]\n",
            "Loss: 0.0074125006794929504\n",
            "Epoch [59/70], LR: [0.4]\n",
            "Loss: 0.009174167178571224\n",
            "Epoch [60/70], LR: [0.4]\n",
            "Loss: 0.003463772824034095\n",
            "Epoch [61/70], LR: [0.4]\n",
            "Loss: 0.019897490739822388\n",
            "Epoch [62/70], LR: [0.4]\n",
            "Loss: 0.008640478365123272\n",
            "Epoch [63/70], LR: [0.4]\n",
            "Loss: 0.02730959840118885\n",
            "Epoch [64/70], LR: [0.08000000000000002]\n",
            "Loss: 0.012344274669885635\n",
            "Epoch [65/70], LR: [0.08000000000000002]\n",
            "Loss: 0.004044921137392521\n",
            "Epoch [66/70], LR: [0.08000000000000002]\n",
            "Loss: 0.0016260546399280429\n",
            "Epoch [67/70], LR: [0.08000000000000002]\n",
            "Loss: 0.023858193308115005\n",
            "Epoch [68/70], LR: [0.08000000000000002]\n",
            "Loss: 0.010050153359770775\n",
            "Epoch [69/70], LR: [0.08000000000000002]\n",
            "Loss: 0.017596673220396042\n",
            "Epoch [70/70], LR: [0.08000000000000002]\n",
            "Loss: 0.008047893643379211\n",
            "Validating classes ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Validating classes ['bicycle', 'plain', 'couch', 'lobster', 'lion', 'chair', 'tulip', 'television', 'skunk', 'girl'] -> [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Validating classes ['bowl', 'tank', 'lawn_mower', 'snake', 'ray', 'oak_tree', 'poppy', 'castle', 'telephone', 'clock'] -> [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "Validating classes ['worm', 'rabbit', 'tractor', 'cockroach', 'house', 'lamp', 'sweet_pepper', 'crab', 'beetle', 'dolphin'] -> [30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
            "Validating classes ['mouse', 'flatfish', 'pear', 'lizard', 'shark', 'orchid', 'cup', 'bus', 'sunflower', 'dinosaur'] -> [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
            "Accuracy: 0.1726\n",
            "BATCH [5]\n",
            "Training on ['whale', 'wolf', 'woman', 'cloud', 'porcupine', 'road', 'plate', 'table', 'sea', 'seal'] -> [50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
            "Known classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
            "Epoch [1/70], LR: [2.0]\n",
            "Loss: 0.03363940119743347\n",
            "Epoch [2/70], LR: [2.0]\n",
            "Loss: 0.016224097460508347\n",
            "Epoch [3/70], LR: [2.0]\n",
            "Loss: 0.028098799288272858\n",
            "Epoch [4/70], LR: [2.0]\n",
            "Loss: 0.03565404191613197\n",
            "Epoch [5/70], LR: [2.0]\n",
            "Loss: 0.03124244511127472\n",
            "Epoch [6/70], LR: [2.0]\n",
            "Loss: 0.0080572459846735\n",
            "Epoch [7/70], LR: [2.0]\n",
            "Loss: 0.02657809853553772\n",
            "Epoch [8/70], LR: [2.0]\n",
            "Loss: 0.03051520697772503\n",
            "Epoch [9/70], LR: [2.0]\n",
            "Loss: 0.014761754311621189\n",
            "Epoch [10/70], LR: [2.0]\n",
            "Loss: 0.014716221019625664\n",
            "Epoch [11/70], LR: [2.0]\n",
            "Loss: 0.021615877747535706\n",
            "Epoch [12/70], LR: [2.0]\n",
            "Loss: 0.012623770162463188\n",
            "Epoch [13/70], LR: [2.0]\n",
            "Loss: 0.02167028747498989\n",
            "Epoch [14/70], LR: [2.0]\n",
            "Loss: 0.03278110921382904\n",
            "Epoch [15/70], LR: [2.0]\n",
            "Loss: 0.030136385932564735\n",
            "Epoch [16/70], LR: [2.0]\n",
            "Loss: 0.020671041682362556\n",
            "Epoch [17/70], LR: [2.0]\n",
            "Loss: 0.016571257263422012\n",
            "Epoch [18/70], LR: [2.0]\n",
            "Loss: 0.007186702452600002\n",
            "Epoch [19/70], LR: [2.0]\n",
            "Loss: 0.027560867369174957\n",
            "Epoch [20/70], LR: [2.0]\n",
            "Loss: 0.01817014440894127\n",
            "Epoch [21/70], LR: [2.0]\n",
            "Loss: 0.017436955124139786\n",
            "Epoch [22/70], LR: [2.0]\n",
            "Loss: 0.0029784259386360645\n",
            "Epoch [23/70], LR: [2.0]\n",
            "Loss: 0.02547764964401722\n",
            "Epoch [24/70], LR: [2.0]\n",
            "Loss: 0.0341574065387249\n",
            "Epoch [25/70], LR: [2.0]\n",
            "Loss: 0.03140496462583542\n",
            "Epoch [26/70], LR: [2.0]\n",
            "Loss: 0.008463759906589985\n",
            "Epoch [27/70], LR: [2.0]\n",
            "Loss: 0.010210788808763027\n",
            "Epoch [28/70], LR: [2.0]\n",
            "Loss: 0.031301725655794144\n",
            "Epoch [29/70], LR: [2.0]\n",
            "Loss: 0.023986341431736946\n",
            "Epoch [30/70], LR: [2.0]\n",
            "Loss: 0.007892492227256298\n",
            "Epoch [31/70], LR: [2.0]\n",
            "Loss: 0.02138788066804409\n",
            "Epoch [32/70], LR: [2.0]\n",
            "Loss: 0.02045147866010666\n",
            "Epoch [33/70], LR: [2.0]\n",
            "Loss: 0.05213337391614914\n",
            "Epoch [34/70], LR: [2.0]\n",
            "Loss: 0.0172136053442955\n",
            "Epoch [35/70], LR: [2.0]\n",
            "Loss: 0.010989935137331486\n",
            "Epoch [36/70], LR: [2.0]\n",
            "Loss: 0.023873159661889076\n",
            "Epoch [37/70], LR: [2.0]\n",
            "Loss: 0.027216129004955292\n",
            "Epoch [38/70], LR: [2.0]\n",
            "Loss: 0.022916309535503387\n",
            "Epoch [39/70], LR: [2.0]\n",
            "Loss: 0.004445002879947424\n",
            "Epoch [40/70], LR: [2.0]\n",
            "Loss: 0.06257263571023941\n",
            "Epoch [41/70], LR: [2.0]\n",
            "Loss: 0.043743353337049484\n",
            "Epoch [42/70], LR: [2.0]\n",
            "Loss: 0.024469837546348572\n",
            "Epoch [43/70], LR: [2.0]\n",
            "Loss: 0.019819097593426704\n",
            "Epoch [44/70], LR: [2.0]\n",
            "Loss: 0.009971530176699162\n",
            "Epoch [45/70], LR: [2.0]\n",
            "Loss: 0.026705846190452576\n",
            "Epoch [46/70], LR: [2.0]\n",
            "Loss: 0.015430023893713951\n",
            "Epoch [47/70], LR: [2.0]\n",
            "Loss: 0.02275569550693035\n",
            "Epoch [48/70], LR: [0.4]\n",
            "Loss: 0.01185714639723301\n",
            "Epoch [49/70], LR: [0.4]\n",
            "Loss: 0.01884963922202587\n",
            "Epoch [50/70], LR: [0.4]\n",
            "Loss: 0.013155335560441017\n",
            "Epoch [51/70], LR: [0.4]\n",
            "Loss: 0.020042797550559044\n",
            "Epoch [52/70], LR: [0.4]\n",
            "Loss: 0.0012218985939398408\n",
            "Epoch [53/70], LR: [0.4]\n",
            "Loss: 0.05256418138742447\n",
            "Epoch [54/70], LR: [0.4]\n",
            "Loss: 0.007895691320300102\n",
            "Epoch [55/70], LR: [0.4]\n",
            "Loss: 0.02900252863764763\n",
            "Epoch [56/70], LR: [0.4]\n",
            "Loss: 0.009203607216477394\n",
            "Epoch [57/70], LR: [0.4]\n",
            "Loss: 0.009728747420012951\n",
            "Epoch [58/70], LR: [0.4]\n",
            "Loss: 0.021244686096906662\n",
            "Epoch [59/70], LR: [0.4]\n",
            "Loss: 0.02327185682952404\n",
            "Epoch [60/70], LR: [0.4]\n",
            "Loss: 0.01959633268415928\n",
            "Epoch [61/70], LR: [0.4]\n",
            "Loss: 0.017548339441418648\n",
            "Epoch [62/70], LR: [0.4]\n",
            "Loss: 0.025192677974700928\n",
            "Epoch [63/70], LR: [0.4]\n",
            "Loss: 0.005509273614734411\n",
            "Epoch [64/70], LR: [0.08000000000000002]\n",
            "Loss: 0.04663380980491638\n",
            "Epoch [65/70], LR: [0.08000000000000002]\n",
            "Loss: 0.013737702742218971\n",
            "Epoch [66/70], LR: [0.08000000000000002]\n",
            "Loss: 0.003569435328245163\n",
            "Epoch [67/70], LR: [0.08000000000000002]\n",
            "Loss: 0.007535295095294714\n",
            "Epoch [68/70], LR: [0.08000000000000002]\n",
            "Loss: 0.03789757937192917\n",
            "Epoch [69/70], LR: [0.08000000000000002]\n",
            "Loss: 0.012068336829543114\n",
            "Epoch [70/70], LR: [0.08000000000000002]\n",
            "Loss: 0.0037288940511643887\n",
            "Validating classes ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Validating classes ['bicycle', 'plain', 'couch', 'lobster', 'lion', 'chair', 'tulip', 'television', 'skunk', 'girl'] -> [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Validating classes ['bowl', 'tank', 'lawn_mower', 'snake', 'ray', 'oak_tree', 'poppy', 'castle', 'telephone', 'clock'] -> [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "Validating classes ['worm', 'rabbit', 'tractor', 'cockroach', 'house', 'lamp', 'sweet_pepper', 'crab', 'beetle', 'dolphin'] -> [30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
            "Validating classes ['mouse', 'flatfish', 'pear', 'lizard', 'shark', 'orchid', 'cup', 'bus', 'sunflower', 'dinosaur'] -> [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
            "Validating classes ['whale', 'wolf', 'woman', 'cloud', 'porcupine', 'road', 'plate', 'table', 'sea', 'seal'] -> [50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
            "Accuracy: 0.14733333333333334\n",
            "BATCH [6]\n",
            "Training on ['bear', 'apple', 'forest', 'streetcar', 'can', 'bed', 'crocodile', 'keyboard', 'boy', 'raccoon'] -> [60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
            "Known classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
            "Epoch [1/70], LR: [2.0]\n",
            "Loss: 0.03957352787256241\n",
            "Epoch [2/70], LR: [2.0]\n",
            "Loss: 0.02690017968416214\n",
            "Epoch [3/70], LR: [2.0]\n",
            "Loss: 0.028303049504756927\n",
            "Epoch [4/70], LR: [2.0]\n",
            "Loss: 0.01976018212735653\n",
            "Epoch [5/70], LR: [2.0]\n",
            "Loss: 0.022521622478961945\n",
            "Epoch [6/70], LR: [2.0]\n",
            "Loss: 0.028685815632343292\n",
            "Epoch [7/70], LR: [2.0]\n",
            "Loss: 0.016677802428603172\n",
            "Epoch [8/70], LR: [2.0]\n",
            "Loss: 0.04942001774907112\n",
            "Epoch [9/70], LR: [2.0]\n",
            "Loss: 0.03237827867269516\n",
            "Epoch [10/70], LR: [2.0]\n",
            "Loss: 0.018481958657503128\n",
            "Epoch [11/70], LR: [2.0]\n",
            "Loss: 0.023455172777175903\n",
            "Epoch [12/70], LR: [2.0]\n",
            "Loss: 0.02544832043349743\n",
            "Epoch [13/70], LR: [2.0]\n",
            "Loss: 0.019908646121621132\n",
            "Epoch [14/70], LR: [2.0]\n",
            "Loss: 0.03053966909646988\n",
            "Epoch [15/70], LR: [2.0]\n",
            "Loss: 0.0014814917230978608\n",
            "Epoch [16/70], LR: [2.0]\n",
            "Loss: 0.05255567654967308\n",
            "Epoch [17/70], LR: [2.0]\n",
            "Loss: 0.02998516894876957\n",
            "Epoch [18/70], LR: [2.0]\n",
            "Loss: 0.011710093356668949\n",
            "Epoch [19/70], LR: [2.0]\n",
            "Loss: 0.01408264972269535\n",
            "Epoch [20/70], LR: [2.0]\n",
            "Loss: 0.01002079714089632\n",
            "Epoch [21/70], LR: [2.0]\n",
            "Loss: 0.0014540472766384482\n",
            "Epoch [22/70], LR: [2.0]\n",
            "Loss: 0.0356832779943943\n",
            "Epoch [23/70], LR: [2.0]\n",
            "Loss: 0.011078338138759136\n",
            "Epoch [24/70], LR: [2.0]\n",
            "Loss: 0.009912610985338688\n",
            "Epoch [25/70], LR: [2.0]\n",
            "Loss: 0.00843863282352686\n",
            "Epoch [26/70], LR: [2.0]\n",
            "Loss: 0.009541160427033901\n",
            "Epoch [27/70], LR: [2.0]\n",
            "Loss: 0.014429129660129547\n",
            "Epoch [28/70], LR: [2.0]\n",
            "Loss: 0.015431956388056278\n",
            "Epoch [29/70], LR: [2.0]\n",
            "Loss: 0.023813879117369652\n",
            "Epoch [30/70], LR: [2.0]\n",
            "Loss: 0.019356723874807358\n",
            "Epoch [31/70], LR: [2.0]\n",
            "Loss: 0.009269597008824348\n",
            "Epoch [32/70], LR: [2.0]\n",
            "Loss: 0.032024893909692764\n",
            "Epoch [33/70], LR: [2.0]\n",
            "Loss: 0.006701693404465914\n",
            "Epoch [34/70], LR: [2.0]\n",
            "Loss: 0.0066243563778698444\n",
            "Epoch [35/70], LR: [2.0]\n",
            "Loss: 0.030142832547426224\n",
            "Epoch [36/70], LR: [2.0]\n",
            "Loss: 0.014152666553854942\n",
            "Epoch [37/70], LR: [2.0]\n",
            "Loss: 0.021224677562713623\n",
            "Epoch [38/70], LR: [2.0]\n",
            "Loss: 0.011213107034564018\n",
            "Epoch [39/70], LR: [2.0]\n",
            "Loss: 0.006364379543811083\n",
            "Epoch [40/70], LR: [2.0]\n",
            "Loss: 0.018471291288733482\n",
            "Epoch [41/70], LR: [2.0]\n",
            "Loss: 0.0031339589040726423\n",
            "Epoch [42/70], LR: [2.0]\n",
            "Loss: 0.016170017421245575\n",
            "Epoch [43/70], LR: [2.0]\n",
            "Loss: 0.0013423262862488627\n",
            "Epoch [44/70], LR: [2.0]\n",
            "Loss: 0.020808478817343712\n",
            "Epoch [45/70], LR: [2.0]\n",
            "Loss: 0.017469624057412148\n",
            "Epoch [46/70], LR: [2.0]\n",
            "Loss: 0.023094870150089264\n",
            "Epoch [47/70], LR: [2.0]\n",
            "Loss: 0.018739601597189903\n",
            "Epoch [48/70], LR: [0.4]\n",
            "Loss: 0.0058066765777766705\n",
            "Epoch [49/70], LR: [0.4]\n",
            "Loss: 0.012880798429250717\n",
            "Epoch [50/70], LR: [0.4]\n",
            "Loss: 0.00848858430981636\n",
            "Epoch [51/70], LR: [0.4]\n",
            "Loss: 0.0020184111781418324\n",
            "Epoch [52/70], LR: [0.4]\n",
            "Loss: 0.03447990119457245\n",
            "Epoch [53/70], LR: [0.4]\n",
            "Loss: 0.027987034991383553\n",
            "Epoch [54/70], LR: [0.4]\n",
            "Loss: 0.00996827706694603\n",
            "Epoch [55/70], LR: [0.4]\n",
            "Loss: 0.020464302971959114\n",
            "Epoch [56/70], LR: [0.4]\n",
            "Loss: 0.02418399602174759\n",
            "Epoch [57/70], LR: [0.4]\n",
            "Loss: 0.031170498579740524\n",
            "Epoch [58/70], LR: [0.4]\n",
            "Loss: 0.030573738738894463\n",
            "Epoch [59/70], LR: [0.4]\n",
            "Loss: 0.03400421515107155\n",
            "Epoch [60/70], LR: [0.4]\n",
            "Loss: 0.016601599752902985\n",
            "Epoch [61/70], LR: [0.4]\n",
            "Loss: 0.0013342060847207904\n",
            "Epoch [62/70], LR: [0.4]\n",
            "Loss: 0.012323413044214249\n",
            "Epoch [63/70], LR: [0.4]\n",
            "Loss: 0.044532906264066696\n",
            "Epoch [64/70], LR: [0.08000000000000002]\n",
            "Loss: 0.016785304993391037\n",
            "Epoch [65/70], LR: [0.08000000000000002]\n",
            "Loss: 0.0299869105219841\n",
            "Epoch [66/70], LR: [0.08000000000000002]\n",
            "Loss: 0.04653240367770195\n",
            "Epoch [67/70], LR: [0.08000000000000002]\n",
            "Loss: 0.009178661741316319\n",
            "Epoch [68/70], LR: [0.08000000000000002]\n",
            "Loss: 0.0018127820221707225\n",
            "Epoch [69/70], LR: [0.08000000000000002]\n",
            "Loss: 0.004125951323658228\n",
            "Epoch [70/70], LR: [0.08000000000000002]\n",
            "Loss: 0.0009203573572449386\n",
            "Validating classes ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Validating classes ['bicycle', 'plain', 'couch', 'lobster', 'lion', 'chair', 'tulip', 'television', 'skunk', 'girl'] -> [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Validating classes ['bowl', 'tank', 'lawn_mower', 'snake', 'ray', 'oak_tree', 'poppy', 'castle', 'telephone', 'clock'] -> [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "Validating classes ['worm', 'rabbit', 'tractor', 'cockroach', 'house', 'lamp', 'sweet_pepper', 'crab', 'beetle', 'dolphin'] -> [30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
            "Validating classes ['mouse', 'flatfish', 'pear', 'lizard', 'shark', 'orchid', 'cup', 'bus', 'sunflower', 'dinosaur'] -> [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
            "Validating classes ['whale', 'wolf', 'woman', 'cloud', 'porcupine', 'road', 'plate', 'table', 'sea', 'seal'] -> [50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
            "Validating classes ['bear', 'apple', 'forest', 'streetcar', 'can', 'bed', 'crocodile', 'keyboard', 'boy', 'raccoon'] -> [60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
            "Accuracy: 0.13014285714285714\n",
            "BATCH [7]\n",
            "Training on ['willow_tree', 'maple_tree', 'orange', 'rocket', 'spider', 'chimpanzee', 'cattle', 'kangaroo', 'bridge', 'fox'] -> [70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
            "Known classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
            "Epoch [1/70], LR: [2.0]\n",
            "Loss: 0.017864709720015526\n",
            "Epoch [2/70], LR: [2.0]\n",
            "Loss: 0.021730130538344383\n",
            "Epoch [3/70], LR: [2.0]\n",
            "Loss: 0.023961670696735382\n",
            "Epoch [4/70], LR: [2.0]\n",
            "Loss: 0.0256402138620615\n",
            "Epoch [5/70], LR: [2.0]\n",
            "Loss: 0.020571595057845116\n",
            "Epoch [6/70], LR: [2.0]\n",
            "Loss: 0.013584338128566742\n",
            "Epoch [7/70], LR: [2.0]\n",
            "Loss: 0.010684202425181866\n",
            "Epoch [8/70], LR: [2.0]\n",
            "Loss: 0.018980247899889946\n",
            "Epoch [9/70], LR: [2.0]\n",
            "Loss: 0.0053497557528316975\n",
            "Epoch [10/70], LR: [2.0]\n",
            "Loss: 0.03320210054516792\n",
            "Epoch [11/70], LR: [2.0]\n",
            "Loss: 0.016116727143526077\n",
            "Epoch [12/70], LR: [2.0]\n",
            "Loss: 0.017894038930535316\n",
            "Epoch [13/70], LR: [2.0]\n",
            "Loss: 0.007603643927723169\n",
            "Epoch [14/70], LR: [2.0]\n",
            "Loss: 0.021887367591261864\n",
            "Epoch [15/70], LR: [2.0]\n",
            "Loss: 0.015349826775491238\n",
            "Epoch [16/70], LR: [2.0]\n",
            "Loss: 0.016080757603049278\n",
            "Epoch [17/70], LR: [2.0]\n",
            "Loss: 0.011409069411456585\n",
            "Epoch [18/70], LR: [2.0]\n",
            "Loss: 0.021552419289946556\n",
            "Epoch [19/70], LR: [2.0]\n",
            "Loss: 0.00896051712334156\n",
            "Epoch [20/70], LR: [2.0]\n",
            "Loss: 0.024761274456977844\n",
            "Epoch [21/70], LR: [2.0]\n",
            "Loss: 0.04225211217999458\n",
            "Epoch [22/70], LR: [2.0]\n",
            "Loss: 0.013242560438811779\n",
            "Epoch [23/70], LR: [2.0]\n",
            "Loss: 0.027785344049334526\n",
            "Epoch [24/70], LR: [2.0]\n",
            "Loss: 0.012782315723598003\n",
            "Epoch [25/70], LR: [2.0]\n",
            "Loss: 0.005244577769190073\n",
            "Epoch [26/70], LR: [2.0]\n",
            "Loss: 0.022710157558321953\n",
            "Epoch [27/70], LR: [2.0]\n",
            "Loss: 0.011262670159339905\n",
            "Epoch [28/70], LR: [2.0]\n",
            "Loss: 0.011546628549695015\n",
            "Epoch [29/70], LR: [2.0]\n",
            "Loss: 0.013335386291146278\n",
            "Epoch [30/70], LR: [2.0]\n",
            "Loss: 0.037724755704402924\n",
            "Epoch [31/70], LR: [2.0]\n",
            "Loss: 0.035421814769506454\n",
            "Epoch [32/70], LR: [2.0]\n",
            "Loss: 0.019200095906853676\n",
            "Epoch [33/70], LR: [2.0]\n",
            "Loss: 0.001852935180068016\n",
            "Epoch [34/70], LR: [2.0]\n",
            "Loss: 0.01939137652516365\n",
            "Epoch [35/70], LR: [2.0]\n",
            "Loss: 0.040746722370386124\n",
            "Epoch [36/70], LR: [2.0]\n",
            "Loss: 0.007775104139000177\n",
            "Epoch [37/70], LR: [2.0]\n",
            "Loss: 0.02585737779736519\n",
            "Epoch [38/70], LR: [2.0]\n",
            "Loss: 0.00800282508134842\n",
            "Epoch [39/70], LR: [2.0]\n",
            "Loss: 0.006298185791820288\n",
            "Epoch [40/70], LR: [2.0]\n",
            "Loss: 0.018044909462332726\n",
            "Epoch [41/70], LR: [2.0]\n",
            "Loss: 0.020359141752123833\n",
            "Epoch [42/70], LR: [2.0]\n",
            "Loss: 0.024928642436861992\n",
            "Epoch [43/70], LR: [2.0]\n",
            "Loss: 0.011157597415149212\n",
            "Epoch [44/70], LR: [2.0]\n",
            "Loss: 0.012941000051796436\n",
            "Epoch [45/70], LR: [2.0]\n",
            "Loss: 0.015355822630226612\n",
            "Epoch [46/70], LR: [2.0]\n",
            "Loss: 0.017896505072712898\n",
            "Epoch [47/70], LR: [2.0]\n",
            "Loss: 0.014099666848778725\n",
            "Epoch [48/70], LR: [0.4]\n",
            "Loss: 0.00953871849924326\n",
            "Epoch [49/70], LR: [0.4]\n",
            "Loss: 0.0017011904856190085\n",
            "Epoch [50/70], LR: [0.4]\n",
            "Loss: 0.0015182001516222954\n",
            "Epoch [51/70], LR: [0.4]\n",
            "Loss: 0.008767501451075077\n",
            "Epoch [52/70], LR: [0.4]\n",
            "Loss: 0.03809290751814842\n",
            "Epoch [53/70], LR: [0.4]\n",
            "Loss: 0.002820189343765378\n",
            "Epoch [54/70], LR: [0.4]\n",
            "Loss: 0.011329280212521553\n",
            "Epoch [55/70], LR: [0.4]\n",
            "Loss: 0.009728002361953259\n",
            "Epoch [56/70], LR: [0.4]\n",
            "Loss: 0.019978368654847145\n",
            "Epoch [57/70], LR: [0.4]\n",
            "Loss: 0.011295774951577187\n",
            "Epoch [58/70], LR: [0.4]\n",
            "Loss: 0.0017129341140389442\n",
            "Epoch [59/70], LR: [0.4]\n",
            "Loss: 0.014107820577919483\n",
            "Epoch [60/70], LR: [0.4]\n",
            "Loss: 0.0010703676380217075\n",
            "Epoch [61/70], LR: [0.4]\n",
            "Loss: 0.002390759764239192\n",
            "Epoch [62/70], LR: [0.4]\n",
            "Loss: 0.0015221842331811786\n",
            "Epoch [63/70], LR: [0.4]\n",
            "Loss: 0.0003694311308208853\n",
            "Epoch [64/70], LR: [0.08000000000000002]\n",
            "Loss: 0.014081067405641079\n",
            "Epoch [65/70], LR: [0.08000000000000002]\n",
            "Loss: 0.009728141129016876\n",
            "Epoch [66/70], LR: [0.08000000000000002]\n",
            "Loss: 0.014766893349587917\n",
            "Epoch [67/70], LR: [0.08000000000000002]\n",
            "Loss: 0.01698928140103817\n",
            "Epoch [68/70], LR: [0.08000000000000002]\n",
            "Loss: 0.025393787771463394\n",
            "Epoch [69/70], LR: [0.08000000000000002]\n",
            "Loss: 0.022352397441864014\n",
            "Epoch [70/70], LR: [0.08000000000000002]\n",
            "Loss: 0.013859531842172146\n",
            "Validating classes ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Validating classes ['bicycle', 'plain', 'couch', 'lobster', 'lion', 'chair', 'tulip', 'television', 'skunk', 'girl'] -> [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Validating classes ['bowl', 'tank', 'lawn_mower', 'snake', 'ray', 'oak_tree', 'poppy', 'castle', 'telephone', 'clock'] -> [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "Validating classes ['worm', 'rabbit', 'tractor', 'cockroach', 'house', 'lamp', 'sweet_pepper', 'crab', 'beetle', 'dolphin'] -> [30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
            "Validating classes ['mouse', 'flatfish', 'pear', 'lizard', 'shark', 'orchid', 'cup', 'bus', 'sunflower', 'dinosaur'] -> [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
            "Validating classes ['whale', 'wolf', 'woman', 'cloud', 'porcupine', 'road', 'plate', 'table', 'sea', 'seal'] -> [50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
            "Validating classes ['bear', 'apple', 'forest', 'streetcar', 'can', 'bed', 'crocodile', 'keyboard', 'boy', 'raccoon'] -> [60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
            "Validating classes ['willow_tree', 'maple_tree', 'orange', 'rocket', 'spider', 'chimpanzee', 'cattle', 'kangaroo', 'bridge', 'fox'] -> [70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
            "Accuracy: 0.109375\n",
            "BATCH [8]\n",
            "Training on ['butterfly', 'baby', 'elephant', 'shrew', 'pine_tree', 'squirrel', 'mountain', 'caterpillar', 'bee', 'camel'] -> [80, 81, 82, 83, 84, 85, 86, 87, 88, 89]\n",
            "Known classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89]\n",
            "Epoch [1/70], LR: [2.0]\n",
            "Loss: 0.02668963558971882\n",
            "Epoch [2/70], LR: [2.0]\n",
            "Loss: 0.019336029887199402\n",
            "Epoch [3/70], LR: [2.0]\n",
            "Loss: 0.019248133525252342\n",
            "Epoch [4/70], LR: [2.0]\n",
            "Loss: 0.011058839038014412\n",
            "Epoch [5/70], LR: [2.0]\n",
            "Loss: 0.007725391536951065\n",
            "Epoch [6/70], LR: [2.0]\n",
            "Loss: 0.027329223230481148\n",
            "Epoch [7/70], LR: [2.0]\n",
            "Loss: 0.01480876374989748\n",
            "Epoch [8/70], LR: [2.0]\n",
            "Loss: 0.01405745092779398\n",
            "Epoch [9/70], LR: [2.0]\n",
            "Loss: 0.013835222460329533\n",
            "Epoch [10/70], LR: [2.0]\n",
            "Loss: 0.003119673114269972\n",
            "Epoch [11/70], LR: [2.0]\n",
            "Loss: 0.028561381623148918\n",
            "Epoch [12/70], LR: [2.0]\n",
            "Loss: 0.010002227500081062\n",
            "Epoch [13/70], LR: [2.0]\n",
            "Loss: 0.004875172395259142\n",
            "Epoch [14/70], LR: [2.0]\n",
            "Loss: 0.02950052171945572\n",
            "Epoch [15/70], LR: [2.0]\n",
            "Loss: 0.007691139820963144\n",
            "Epoch [16/70], LR: [2.0]\n",
            "Loss: 0.0049033998511731625\n",
            "Epoch [17/70], LR: [2.0]\n",
            "Loss: 0.012487741187214851\n",
            "Epoch [18/70], LR: [2.0]\n",
            "Loss: 0.019176432862877846\n",
            "Epoch [19/70], LR: [2.0]\n",
            "Loss: 0.021110787987709045\n",
            "Epoch [20/70], LR: [2.0]\n",
            "Loss: 0.012620759196579456\n",
            "Epoch [21/70], LR: [2.0]\n",
            "Loss: 0.008478157222270966\n",
            "Epoch [22/70], LR: [2.0]\n",
            "Loss: 0.00947860348969698\n",
            "Epoch [23/70], LR: [2.0]\n",
            "Loss: 0.014534820802509785\n",
            "Epoch [24/70], LR: [2.0]\n",
            "Loss: 0.014109915122389793\n",
            "Epoch [25/70], LR: [2.0]\n",
            "Loss: 0.01051727682352066\n",
            "Epoch [26/70], LR: [2.0]\n",
            "Loss: 0.03280195593833923\n",
            "Epoch [27/70], LR: [2.0]\n",
            "Loss: 0.00900406762957573\n",
            "Epoch [28/70], LR: [2.0]\n",
            "Loss: 0.0071258144453167915\n",
            "Epoch [29/70], LR: [2.0]\n",
            "Loss: 0.0029582204297184944\n",
            "Epoch [30/70], LR: [2.0]\n",
            "Loss: 0.020345637574791908\n",
            "Epoch [31/70], LR: [2.0]\n",
            "Loss: 0.010702474974095821\n",
            "Epoch [32/70], LR: [2.0]\n",
            "Loss: 0.015687480568885803\n",
            "Epoch [33/70], LR: [2.0]\n",
            "Loss: 0.002905764617025852\n",
            "Epoch [34/70], LR: [2.0]\n",
            "Loss: 0.02130693942308426\n",
            "Epoch [35/70], LR: [2.0]\n",
            "Loss: 0.02807001583278179\n",
            "Epoch [36/70], LR: [2.0]\n",
            "Loss: 0.008604563772678375\n",
            "Epoch [37/70], LR: [2.0]\n",
            "Loss: 0.01977861486375332\n",
            "Epoch [38/70], LR: [2.0]\n",
            "Loss: 0.0014771826099604368\n",
            "Epoch [39/70], LR: [2.0]\n",
            "Loss: 0.021212276071310043\n",
            "Epoch [40/70], LR: [2.0]\n",
            "Loss: 0.014081458561122417\n",
            "Epoch [41/70], LR: [2.0]\n",
            "Loss: 0.018323365598917007\n",
            "Epoch [42/70], LR: [2.0]\n",
            "Loss: 0.014550857245922089\n",
            "Epoch [43/70], LR: [2.0]\n",
            "Loss: 0.004511503502726555\n",
            "Epoch [44/70], LR: [2.0]\n",
            "Loss: 0.013249446637928486\n",
            "Epoch [45/70], LR: [2.0]\n",
            "Loss: 0.03179837390780449\n",
            "Epoch [46/70], LR: [2.0]\n",
            "Loss: 0.017903443425893784\n",
            "Epoch [47/70], LR: [2.0]\n",
            "Loss: 0.009658650495111942\n",
            "Epoch [48/70], LR: [0.4]\n",
            "Loss: 0.02255161851644516\n",
            "Epoch [49/70], LR: [0.4]\n",
            "Loss: 0.024651696905493736\n",
            "Epoch [50/70], LR: [0.4]\n",
            "Loss: 0.0047754887491464615\n",
            "Epoch [51/70], LR: [0.4]\n",
            "Loss: 0.005125247407704592\n",
            "Epoch [52/70], LR: [0.4]\n",
            "Loss: 0.009451218880712986\n",
            "Epoch [53/70], LR: [0.4]\n",
            "Loss: 0.0050109997391700745\n",
            "Epoch [54/70], LR: [0.4]\n",
            "Loss: 0.016540220007300377\n",
            "Epoch [55/70], LR: [0.4]\n",
            "Loss: 0.008676583878695965\n",
            "Epoch [56/70], LR: [0.4]\n",
            "Loss: 0.0012615625746548176\n",
            "Epoch [57/70], LR: [0.4]\n",
            "Loss: 0.007517585530877113\n",
            "Epoch [58/70], LR: [0.4]\n",
            "Loss: 0.0013931631110608578\n",
            "Epoch [59/70], LR: [0.4]\n",
            "Loss: 0.0009067244245670736\n",
            "Epoch [60/70], LR: [0.4]\n",
            "Loss: 0.009086417965590954\n",
            "Epoch [61/70], LR: [0.4]\n",
            "Loss: 0.0017940144753083587\n",
            "Epoch [62/70], LR: [0.4]\n",
            "Loss: 0.0038354794960469007\n",
            "Epoch [63/70], LR: [0.4]\n",
            "Loss: 0.0042826347053050995\n",
            "Epoch [64/70], LR: [0.08000000000000002]\n",
            "Loss: 0.010725181549787521\n",
            "Epoch [65/70], LR: [0.08000000000000002]\n",
            "Loss: 0.003758456092327833\n",
            "Epoch [66/70], LR: [0.08000000000000002]\n",
            "Loss: 0.002817378146573901\n",
            "Epoch [67/70], LR: [0.08000000000000002]\n",
            "Loss: 0.009683075360953808\n",
            "Epoch [68/70], LR: [0.08000000000000002]\n",
            "Loss: 0.016909750178456306\n",
            "Epoch [69/70], LR: [0.08000000000000002]\n",
            "Loss: 0.008225609548389912\n",
            "Epoch [70/70], LR: [0.08000000000000002]\n",
            "Loss: 0.0011105609592050314\n",
            "Validating classes ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Validating classes ['bicycle', 'plain', 'couch', 'lobster', 'lion', 'chair', 'tulip', 'television', 'skunk', 'girl'] -> [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Validating classes ['bowl', 'tank', 'lawn_mower', 'snake', 'ray', 'oak_tree', 'poppy', 'castle', 'telephone', 'clock'] -> [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "Validating classes ['worm', 'rabbit', 'tractor', 'cockroach', 'house', 'lamp', 'sweet_pepper', 'crab', 'beetle', 'dolphin'] -> [30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
            "Validating classes ['mouse', 'flatfish', 'pear', 'lizard', 'shark', 'orchid', 'cup', 'bus', 'sunflower', 'dinosaur'] -> [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
            "Validating classes ['whale', 'wolf', 'woman', 'cloud', 'porcupine', 'road', 'plate', 'table', 'sea', 'seal'] -> [50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
            "Validating classes ['bear', 'apple', 'forest', 'streetcar', 'can', 'bed', 'crocodile', 'keyboard', 'boy', 'raccoon'] -> [60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
            "Validating classes ['willow_tree', 'maple_tree', 'orange', 'rocket', 'spider', 'chimpanzee', 'cattle', 'kangaroo', 'bridge', 'fox'] -> [70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
            "Validating classes ['butterfly', 'baby', 'elephant', 'shrew', 'pine_tree', 'squirrel', 'mountain', 'caterpillar', 'bee', 'camel'] -> [80, 81, 82, 83, 84, 85, 86, 87, 88, 89]\n",
            "Accuracy: 0.09388888888888888\n",
            "BATCH [9]\n",
            "Training on ['leopard', 'trout', 'turtle', 'rose', 'aquarium_fish', 'possum', 'hamster', 'otter', 'motorcycle', 'pickup_truck'] -> [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
            "Known classes [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
            "Epoch [1/70], LR: [2.0]\n",
            "Loss: 0.020006822422146797\n",
            "Epoch [2/70], LR: [2.0]\n",
            "Loss: 0.02156064659357071\n",
            "Epoch [3/70], LR: [2.0]\n",
            "Loss: 0.015636328607797623\n",
            "Epoch [4/70], LR: [2.0]\n",
            "Loss: 0.015690606087446213\n",
            "Epoch [5/70], LR: [2.0]\n",
            "Loss: 0.008555257692933083\n",
            "Epoch [6/70], LR: [2.0]\n",
            "Loss: 0.01986466348171234\n",
            "Epoch [7/70], LR: [2.0]\n",
            "Loss: 0.011823543347418308\n",
            "Epoch [8/70], LR: [2.0]\n",
            "Loss: 0.007345212623476982\n",
            "Epoch [9/70], LR: [2.0]\n",
            "Loss: 0.01227907557040453\n",
            "Epoch [10/70], LR: [2.0]\n",
            "Loss: 0.017522044479846954\n",
            "Epoch [11/70], LR: [2.0]\n",
            "Loss: 0.0118833277374506\n",
            "Epoch [12/70], LR: [2.0]\n",
            "Loss: 0.028604639694094658\n",
            "Epoch [13/70], LR: [2.0]\n",
            "Loss: 0.003189201932400465\n",
            "Epoch [14/70], LR: [2.0]\n",
            "Loss: 0.020527906715869904\n",
            "Epoch [15/70], LR: [2.0]\n",
            "Loss: 0.01588539592921734\n",
            "Epoch [16/70], LR: [2.0]\n",
            "Loss: 0.02204039692878723\n",
            "Epoch [17/70], LR: [2.0]\n",
            "Loss: 0.015812216326594353\n",
            "Epoch [18/70], LR: [2.0]\n",
            "Loss: 0.011799357831478119\n",
            "Epoch [19/70], LR: [2.0]\n",
            "Loss: 0.026241425424814224\n",
            "Epoch [20/70], LR: [2.0]\n",
            "Loss: 0.00691796513274312\n",
            "Epoch [21/70], LR: [2.0]\n",
            "Loss: 0.012570198625326157\n",
            "Epoch [22/70], LR: [2.0]\n",
            "Loss: 0.007046084385365248\n",
            "Epoch [23/70], LR: [2.0]\n",
            "Loss: 0.00333934323862195\n",
            "Epoch [24/70], LR: [2.0]\n",
            "Loss: 0.02109399065375328\n",
            "Epoch [25/70], LR: [2.0]\n",
            "Loss: 0.013986371457576752\n",
            "Epoch [26/70], LR: [2.0]\n",
            "Loss: 0.019315961748361588\n",
            "Epoch [27/70], LR: [2.0]\n",
            "Loss: 0.02492152899503708\n",
            "Epoch [28/70], LR: [2.0]\n",
            "Loss: 0.020809762179851532\n",
            "Epoch [29/70], LR: [2.0]\n",
            "Loss: 0.022081756964325905\n",
            "Epoch [30/70], LR: [2.0]\n",
            "Loss: 0.011084373109042645\n",
            "Epoch [31/70], LR: [2.0]\n",
            "Loss: 0.004454775247722864\n",
            "Epoch [32/70], LR: [2.0]\n",
            "Loss: 0.004195827059447765\n",
            "Epoch [33/70], LR: [2.0]\n",
            "Loss: 0.006805106066167355\n",
            "Epoch [34/70], LR: [2.0]\n",
            "Loss: 0.009475135244429111\n",
            "Epoch [35/70], LR: [2.0]\n",
            "Loss: 0.0059677897952497005\n",
            "Epoch [36/70], LR: [2.0]\n",
            "Loss: 0.0032774212304502726\n",
            "Epoch [37/70], LR: [2.0]\n",
            "Loss: 0.01462236326187849\n",
            "Epoch [38/70], LR: [2.0]\n",
            "Loss: 0.005555726587772369\n",
            "Epoch [39/70], LR: [2.0]\n",
            "Loss: 0.0020890464074909687\n",
            "Epoch [40/70], LR: [2.0]\n",
            "Loss: 0.002360874554142356\n",
            "Epoch [41/70], LR: [2.0]\n",
            "Loss: 0.011691626161336899\n",
            "Epoch [42/70], LR: [2.0]\n",
            "Loss: 0.0053903101943433285\n",
            "Epoch [43/70], LR: [2.0]\n",
            "Loss: 0.030509857460856438\n",
            "Epoch [44/70], LR: [2.0]\n",
            "Loss: 0.010654423385858536\n",
            "Epoch [45/70], LR: [2.0]\n",
            "Loss: 0.01469399780035019\n",
            "Epoch [46/70], LR: [2.0]\n",
            "Loss: 0.01352783665060997\n",
            "Epoch [47/70], LR: [2.0]\n",
            "Loss: 0.018469206988811493\n",
            "Epoch [48/70], LR: [0.4]\n",
            "Loss: 0.0019298878032714128\n",
            "Epoch [49/70], LR: [0.4]\n",
            "Loss: 0.025796763598918915\n",
            "Epoch [50/70], LR: [0.4]\n",
            "Loss: 0.0027579718735069036\n",
            "Epoch [51/70], LR: [0.4]\n",
            "Loss: 0.032720182090997696\n",
            "Epoch [52/70], LR: [0.4]\n",
            "Loss: 0.003445456037297845\n",
            "Epoch [53/70], LR: [0.4]\n",
            "Loss: 0.004698607139289379\n",
            "Epoch [54/70], LR: [0.4]\n",
            "Loss: 0.0203829575330019\n",
            "Epoch [55/70], LR: [0.4]\n",
            "Loss: 0.012466291896998882\n",
            "Epoch [56/70], LR: [0.4]\n",
            "Loss: 0.005791948176920414\n",
            "Epoch [57/70], LR: [0.4]\n",
            "Loss: 0.006201163399964571\n",
            "Epoch [58/70], LR: [0.4]\n",
            "Loss: 0.004963234066963196\n",
            "Epoch [59/70], LR: [0.4]\n",
            "Loss: 0.009081993252038956\n",
            "Epoch [60/70], LR: [0.4]\n",
            "Loss: 0.02355405129492283\n",
            "Epoch [61/70], LR: [0.4]\n",
            "Loss: 0.00047603066195733845\n",
            "Epoch [62/70], LR: [0.4]\n",
            "Loss: 0.0006840224377810955\n",
            "Epoch [63/70], LR: [0.4]\n",
            "Loss: 0.009206194430589676\n",
            "Epoch [64/70], LR: [0.08000000000000002]\n",
            "Loss: 0.009445358999073505\n",
            "Epoch [65/70], LR: [0.08000000000000002]\n",
            "Loss: 0.006015900056809187\n",
            "Epoch [66/70], LR: [0.08000000000000002]\n",
            "Loss: 0.014160946011543274\n",
            "Epoch [67/70], LR: [0.08000000000000002]\n",
            "Loss: 0.007837084122002125\n",
            "Epoch [68/70], LR: [0.08000000000000002]\n",
            "Loss: 0.002559467451646924\n",
            "Epoch [69/70], LR: [0.08000000000000002]\n",
            "Loss: 0.00884395930916071\n",
            "Epoch [70/70], LR: [0.08000000000000002]\n",
            "Loss: 0.0010126311099156737\n",
            "Validating classes ['palm_tree', 'bottle', 'man', 'mushroom', 'snail', 'tiger', 'beaver', 'skyscraper', 'wardrobe', 'train'] -> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
            "Validating classes ['bicycle', 'plain', 'couch', 'lobster', 'lion', 'chair', 'tulip', 'television', 'skunk', 'girl'] -> [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Validating classes ['bowl', 'tank', 'lawn_mower', 'snake', 'ray', 'oak_tree', 'poppy', 'castle', 'telephone', 'clock'] -> [20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
            "Validating classes ['worm', 'rabbit', 'tractor', 'cockroach', 'house', 'lamp', 'sweet_pepper', 'crab', 'beetle', 'dolphin'] -> [30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
            "Validating classes ['mouse', 'flatfish', 'pear', 'lizard', 'shark', 'orchid', 'cup', 'bus', 'sunflower', 'dinosaur'] -> [40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
            "Validating classes ['whale', 'wolf', 'woman', 'cloud', 'porcupine', 'road', 'plate', 'table', 'sea', 'seal'] -> [50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
            "Validating classes ['bear', 'apple', 'forest', 'streetcar', 'can', 'bed', 'crocodile', 'keyboard', 'boy', 'raccoon'] -> [60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
            "Validating classes ['willow_tree', 'maple_tree', 'orange', 'rocket', 'spider', 'chimpanzee', 'cattle', 'kangaroo', 'bridge', 'fox'] -> [70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
            "Validating classes ['butterfly', 'baby', 'elephant', 'shrew', 'pine_tree', 'squirrel', 'mountain', 'caterpillar', 'bee', 'camel'] -> [80, 81, 82, 83, 84, 85, 86, 87, 88, 89]\n",
            "Validating classes ['leopard', 'trout', 'turtle', 'rose', 'aquarium_fish', 'possum', 'hamster', 'otter', 'motorcycle', 'pickup_truck'] -> [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
            "Accuracy: 0.0867\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
